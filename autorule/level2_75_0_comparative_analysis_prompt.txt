You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 8.16 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for optimized fused min reduction and bias addition
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <limits>

__global__ void fused_min_bias_add_kernel(const float* input, const float* bias, float* output, int batch_size, int features) {
    int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;

    extern __shared__ float shared_min[];

    const float* current_input = input + batch_idx * features;

    // Calculate feature chunk for this thread
    const int chunk_size = (features + blockDim.x - 1) / blockDim.x;
    const int start = threadIdx.x * chunk_size;
    const int end = min(start + chunk_size, features);

    // 1. Compute thread-local min over feature chunk
    float thread_min = INFINITY;
    for(int i = start; i < end; ++i) {
        thread_min = fminf(thread_min, current_input[i]);
    }

    // 2. Parallel reduction to find global min
    shared_min[threadIdx.x] = thread_min;
    __syncthreads();

    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(threadIdx.x < stride) {
            shared_min[threadIdx.x] = fminf(shared_min[threadIdx.x], shared_min[threadIdx.x + stride]);
        }
        __syncthreads();
    }

    const float global_min = shared_min[0];

    // 3. Write results with coalesced memory access pattern
    for(int i = start; i < end; ++i) {
        const int output_idx = i * batch_size + batch_idx;
        output[output_idx] = global_min + bias[i];
    }
}

torch::Tensor fused_min_bias_add_cuda(torch::Tensor input, torch::Tensor bias) {
    const int batch_size = input.size(0);
    const int features = input.size(1);

    // Create output tensor with original model's expected output shape [1, features, batch_size, 1]
    auto output = torch::empty({1, features, batch_size, 1}, input.options());

    // Ensure bias is contiguous and properly shaped
    torch::Tensor bias_contiguous = bias.contiguous().view({features});

    // Optimal block size based on feature dimension
    const int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);
    const int shared_mem_size = block_size * sizeof(float);

    fused_min_bias_add_kernel<<<grid, block, shared_mem_size>>>(
        input.data_ptr<float>(),
        bias_contiguous.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        features
    );

    return output;
}
"""

fused_kernel_cpp_source = "torch::Tensor fused_min_bias_add_cuda(torch::Tensor input, torch::Tensor bias);"

# Load the optimized CUDA kernel
fused_min_bias_add = load_inline(
    name='fused_min_bias_add',
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_min_bias_add_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_min_bias_add = fused_min_bias_add

    def forward(self, x):
        x = self.gemm(x)
        x = self.group_norm(x)
        x = self.fused_min_bias_add.fused_min_bias_add_cuda(x, self.bias)
        return x
```

Kernel 2 (runtime: 8.15 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for optimized fused min reduction and bias addition
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <limits>

__global__ void fused_min_bias_add_kernel(const float* input, const float* bias, float* output, int batch_size, int features) {
    int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;

    extern __shared__ float shared_min[];

    const float* current_input = input + batch_idx * features;

    // Calculate feature chunk for this thread
    const int chunk_size = features / blockDim.x;
    const int start = threadIdx.x * chunk_size;
    const int end = (threadIdx.x == blockDim.x-1) ? features : (threadIdx.x+1)*chunk_size;

    // 1. Compute thread-local min over feature chunk
    float thread_min = INFINITY;
    for(int i=start; i<end; ++i) {
        thread_min = fminf(thread_min, current_input[i]);
    }

    // 2. Parallel reduction to find global min
    shared_min[threadIdx.x] = thread_min;
    __syncthreads();

    for(int stride=blockDim.x/2; stride>0; stride>>=1) {
        if(threadIdx.x < stride) {
            shared_min[threadIdx.x] = fminf(shared_min[threadIdx.x], shared_min[threadIdx.x+stride]);
        }
        __syncthreads();
    }

    const float global_min = shared_min[0];

    // 3. Write results with coalesced memory access
    for(int i=start; i<end; ++i) {
        const int output_idx = i * batch_size + batch_idx;
        output[output_idx] = global_min + bias[i];
    }
}

torch::Tensor fused_min_bias_add_cuda(torch::Tensor input, torch::Tensor bias) {
    const int batch_size = input.size(0);
    const int features = input.size(1);

    // Create output tensor with original model's output shape [1, features, batch_size, 1]
    auto output = torch::empty({1, features, batch_size, 1}, input.options());

    // Ensure bias is contiguous and properly shaped
    torch::Tensor bias_contiguous = bias.contiguous().view({features});

    // Use 256 threads per block (optimal for feature chunks of 8192/256=32)
    const int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);
    const int shared_mem_size = block_size * sizeof(float);

    fused_min_bias_add_kernel<<<grid, block, shared_mem_size>>>(
        input.data_ptr<float>(),
        bias_contiguous.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        features
    );

    return output;
}
"""

fused_kernel_cpp_source = "torch::Tensor fused_min_bias_add_cuda(torch::Tensor input, torch::Tensor bias);"

# Load the optimized CUDA kernel
fused_min_bias_add = load_inline(
    name='fused_min_bias_add',
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_min_bias_add_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_min_bias_add = fused_min_bias_add

    def forward(self, x):
        x = self.gemm(x)
        x = self.group_norm(x)
        x = self.fused_min_bias_add.fused_min_bias_add_cuda(x, self.bias)
        return x
```
