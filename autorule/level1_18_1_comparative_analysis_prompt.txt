You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 63.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused transpose and matrix multiplication
fused_transpose_matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_transpose_matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;

    if (i < M && j < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += A[k * M + i] * B[j * K + k];
        }
        C[i * N + j] = sum;
    }
}

torch::Tensor fused_transpose_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    // A is (K, M), B is (N, K)
    int K = A.size(0);
    int M = A.size(1);
    int N = B.size(0);

    auto C = torch::zeros({M, N}, A.options());

    dim3 block(16, 16);
    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);

    fused_transpose_matmul_kernel<<<grid, block>>>(
        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N
    );

    return C;
}
"""

fused_transpose_matmul_cpp_source = """
torch::Tensor fused_transpose_matmul_cuda(torch::Tensor A, torch::Tensor B);
"""

# Compile the inline CUDA code
fused_transpose_matmul = load_inline(
    name="fused_transpose_matmul",
    cpp_sources=fused_transpose_matmul_cpp_source,
    cuda_sources=fused_transpose_matmul_source,
    functions=["fused_transpose_matmul_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.fused_transpose_matmul = fused_transpose_matmul

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.fused_transpose_matmul.fused_transpose_matmul_cuda(A, B)
```

Kernel 2 (runtime: 64.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for transposed matrix multiplication
matmul_transposed_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void matmul_transposed_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    // Block index
    int bx = blockIdx.x;
    int by = blockIdx.y;

    // Thread index
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    const int TILE_SIZE = 16;

    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;

    float sum = 0.0f;

    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {
        // Load A^T tile (A is KxM)
        int a_col_in_A = t * TILE_SIZE + tx;
        int a_row_in_A = row;
        if (a_col_in_A < K && a_row_in_A < M) {
            As[ty][tx] = A[a_col_in_A * M + a_row_in_A];
        } else {
            As[ty][tx] = 0.0f;
        }

        // Load B^T tile (B is NxK)
        int b_row_in_B = col;
        int b_col_in_B = t * TILE_SIZE + ty;
        if (b_col_in_B < K && b_row_in_B < N) {
            Bs[ty][tx] = B[b_row_in_B * K + b_col_in_B];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}

torch::Tensor matmul_transposed_cuda(torch::Tensor A, torch::Tensor B) {
    int K = A.size(0);
    int M = A.size(1);
    int N = B.size(0);

    auto C = torch::zeros({M, N}, A.options());

    dim3 block(16, 16);
    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);

    matmul_transposed_kernel<<<grid, block>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, K, N
    );

    return C;
}
"""

matmul_transposed_cpp_source = "torch::Tensor matmul_transposed_cuda(torch::Tensor A, torch::Tensor B);"

# Compile the inline CUDA code
matmul_transposed = load_inline(
    name="matmul_transposed",
    cpp_sources=matmul_transposed_cpp_source,
    cuda_sources=matmul_transposed_source,
    functions=["matmul_transposed_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.matmul_transposed = matmul_transposed

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matmul_transposed.matmul_transposed_cuda(A, B)
```
