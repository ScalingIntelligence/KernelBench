You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 8.12 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused ReLU + GroupNorm
fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void compute_group_stats_kernel(
    const float* input,
    float* sum,
    float* sum_sq,
    int batch_size,
    int channels,
    int groups,
    int D, int H, int W
) {
    int group_id = blockIdx.x % groups;
    int batch_id = blockIdx.x / groups;

    if (batch_id >= batch_size) return;

    int channels_per_group = channels / groups;
    int elements_per_group = channels_per_group * D * H * W;

    float thread_sum = 0.0f;
    float thread_sum_sq = 0.0f;

    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        int c = i / (D * H * W);
        int spatial = i % (D * H * W);
        int d = spatial / (H * W);
        int h = (spatial % (H * W)) / W;
        int w = spatial % W;

        int input_idx = batch_id * channels * D * H * W +
                        (group_id * channels_per_group + c) * D * H * W +
                        d * H * W + h * W + w;

        float val = fmaxf(input[input_idx], 0.0f);
        thread_sum += val;
        thread_sum_sq += val * val;
    }

    __shared__ float s_sum[256];
    __shared__ float s_sum_sq[256];
    s_sum[threadIdx.x] = thread_sum;
    s_sum_sq[threadIdx.x] = thread_sum_sq;
    __syncthreads();

    for (int s = blockDim.x/2; s>0; s>>=1) {
        if (threadIdx.x < s) {
            s_sum[threadIdx.x] += s_sum[threadIdx.x + s];
            s_sum_sq[threadIdx.x] += s_sum_sq[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        atomicAdd(&sum[batch_id*groups + group_id], s_sum[0]);
        atomicAdd(&sum_sq[batch_id*groups + group_id], s_sum_sq[0]);
    }
}

__global__ void apply_group_norm_kernel(
    const float* input,
    const float* mean,
    const float* var,
    const float* gamma,
    const float* beta,
    float* output,
    int batch_size,
    int channels,
    int groups,
    int D, int H, int W,
    float eps
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_elements = batch_size * channels * D * H * W;
    if (idx >= total_elements) return;

    int c = (idx / (D * H * W)) % channels;
    int batch = idx / (channels * D * H * W);
    int spatial_idx = idx % (D * H * W);
    int d = spatial_idx / (H * W);
    int h = (spatial_idx % (H * W)) / W;
    int w = spatial_idx % W;

    int group_id = c / (channels / groups);
    float m = mean[batch*groups + group_id];
    float v = var[batch*groups + group_id];
    float inv_std = rsqrtf(v + eps);

    float val = fmaxf(input[idx], 0.0f);
    output[idx] = (val - m) * inv_std * gamma[c] + beta[c];
}

torch::Tensor fused_relu_gn_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int groups,
    float eps
) {
    auto batch_size = input.size(0);
    auto channels = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);

    auto options = torch::TensorOptions().dtype(input.dtype()).device(input.device());
    auto sum = torch::zeros({batch_size, groups}, options);
    auto sum_sq = torch::zeros({batch_size, groups}, options);

    // Compute group statistics
    int num_blocks = batch_size * groups;
    int block_size = 256;
    compute_group_stats_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        sum.data_ptr<float>(),
        sum_sq.data_ptr<float>(),
        batch_size,
        channels,
        groups,
        D, H, W
    );

    // Compute mean and variance
    auto num_elements_per_group = (channels/groups) * D * H * W;
    auto mean = sum / num_elements_per_group;
    auto var = (sum_sq / num_elements_per_group) - mean.pow(2);

    // Apply normalization
    auto output = torch::empty_like(input);
    int total_elements = batch_size * channels * D * H * W;
    block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    apply_group_norm_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        mean.data_ptr<float>(),
        var.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        groups,
        D, H, W,
        eps
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_relu_gn_forward(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int groups, float eps);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=["fused_relu_gn_forward"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, groups, bias=False):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, bias=bias
        )
        self.groups = groups
        self.eps = 1e-5
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_relu_gn_forward(
            x, self.gamma, self.beta, self.groups, self.eps
        )
        return x
```

Kernel 2 (runtime: 8.18 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused ReLU + GroupNorm CUDA kernel
fused_relu_gn_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename scalar_t>
__global__ void fused_relu_group_norm_kernel(
    const scalar_t* input,
    const scalar_t* weight,
    const scalar_t* bias,
    scalar_t* output,
    int num_samples,
    int num_channels,
    int D, int H, int W,
    int groups,
    float eps) {

    int n = blockIdx.z;  // Sample index
    int g = blockIdx.y;  // Group index
    int tid = threadIdx.x;

    const int C_per_group = num_channels / groups;
    const int elements_per_group = C_per_group * D * H * W;

    scalar_t sum = 0;
    scalar_t sum_sq = 0;

    // First pass: compute sum and sum_sq with ReLU
    for (int i = tid; i < elements_per_group; i += blockDim.x) {
        const int c = i / (D * H * W);
        const int spatial_idx = i % (D * H * W);
        const int d = spatial_idx / (H * W);
        const int hw = spatial_idx % (H * W);
        const int h = hw / W;
        const int w = hw % W;

        const int input_idx = n * num_channels * D * H * W +
                            (g * C_per_group + c) * D * H * W +
                            d * H * W +
                            h * W +
                            w;

        scalar_t val = input[input_idx];
        val = max(val, scalar_t(0));  // ReLU
        sum += val;
        sum_sq += val * val;
    }

    // Block reduction for sum and sum_sq
    __shared__ scalar_t shared_sum[256];
    __shared__ scalar_t shared_sum_sq[256];
    shared_sum[tid] = sum;
    shared_sum_sq[tid] = sum_sq;
    __syncthreads();

    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            shared_sum[tid] += shared_sum[tid + stride];
            shared_sum_sq[tid] += shared_sum_sq[tid + stride];
        }
        __syncthreads();
    }

    const scalar_t mean = shared_sum[0] / elements_per_group;
    const scalar_t var = shared_sum_sq[0] / elements_per_group - mean * mean + eps;
    const scalar_t inv_std = rsqrt(var);

    // Second pass: apply normalization
    for (int i = tid; i < elements_per_group; i += blockDim.x) {
        const int c = i / (D * H * W);
        const int spatial_idx = i % (D * H * W);
        const int d = spatial_idx / (H * W);
        const int hw = spatial_idx % (H * W);
        const int h = hw / W;
        const int w = hw % W;

        const int input_idx = n * num_channels * D * H * W +
                            (g * C_per_group + c) * D * H * W +
                            d * H * W +
                            h * W +
                            w;

        scalar_t val = input[input_idx];
        val = max(val, scalar_t(0));  // ReLU again

        // Normalize and apply affine transformation
        const int channel_idx = g * C_per_group + c;
        val = (val - mean) * inv_std * weight[channel_idx] + bias[channel_idx];
        output[input_idx] = val;
    }
}

torch::Tensor fused_relu_group_norm_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int groups,
    float eps) {

    auto output = torch::empty_like(input);
    const int num_samples = input.size(0);
    const int num_channels = input.size(1);
    const int D = input.size(2);
    const int H = input.size(3);
    const int W = input.size(4);

    const int block_size = 256;
    dim3 grid(1, groups, num_samples);
    dim3 block(block_size);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "fused_relu_group_norm", ([&] {
        fused_relu_group_norm_kernel<scalar_t><<<grid, block>>>(
            input.contiguous().data_ptr<scalar_t>(),
            weight.contiguous().data_ptr<scalar_t>(),
            bias.contiguous().data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            num_samples,
            num_channels,
            D, H, W,
            groups,
            eps);
    }));

    return output;
}
"""

fused_relu_gn_cpp = """
torch::Tensor fused_relu_group_norm_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int groups,
    float eps);
"""

# Load the custom CUDA extension
fused_gn = load_inline(
    name='fused_gn',
    cpp_sources=fused_relu_gn_cpp,
    cuda_sources=fused_relu_gn_source,
    functions=['fused_relu_group_norm_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, groups, bias=False):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, bias=bias
        )
        self.groups = groups
        self.eps = 1e-5

        # Initialize GroupNorm parameters
        self.group_norm_weight = nn.Parameter(torch.ones(out_channels))
        self.group_norm_bias = nn.Parameter(torch.zeros(out_channels))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_gn.fused_relu_group_norm_cuda(
            x, 
            self.group_norm_weight,
            self.group_norm_bias,
            self.groups,
            self.eps
        )
        return x
```
