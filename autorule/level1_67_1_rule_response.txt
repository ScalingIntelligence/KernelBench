REASONING TRACE:
[
    "The kernel uses the read-only data cache (`__ldg`) for accessing input and bias tensors.",
    "The kernel loads weights into shared memory with a coalesced global memory access pattern.",
    "The kernel organizes thread blocks to ensure consecutive threads write to contiguous output memory locations for coalesced writes.",
    "The kernel uses a grid configuration that groups computations by output channel to optimize shared memory usage and reduce global memory access latency."
]


ANSWER:


The JSON array above captures the key rule-like statements derived from the reasoning. These rules are: 

1. **Read-Only Cache Usage**: Explicit use of `__ldg` for input/bias access improves memory efficiency.  
2. **Coalesced Weight Loading**: Threads load contiguous weight elements into shared memory, maximizing global memory throughput.  
3. **Output-Coherent Thread Mapping**: Thread/work organization ensures coalesced writes to output memory.  
4. **Output-Channel Grouping**: Grid structure prioritizes output-channel parallelism, improving shared memory locality and reducing global memory access overhead.  

These rules are objective, deterministic, and applicable to CUDA kernel optimization in general.

Usage:
{'inputTokens': 4089, 'outputTokens': 234, 'totalTokens': 4323}