You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 54.6 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused kernel for HardSwish + GroupNorm + Spatial Mean
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <c10/cuda/CUDAGuard.h>

template<typename scalar_t>
__device__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

template<typename scalar_t>
__device__ float blockReduceSum(float val) {
    static __shared__ float shared[32];
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    val = warpReduceSum<scalar_t>(val);

    if (lane == 0) shared[wid] = val;
    __syncthreads();

    val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : 0.0f;
    if (wid == 0) val = warpReduceSum<scalar_t>(val);
    return val;
}

template<typename scalar_t>
__global__ void fused_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int B, int C, int D, int H, int W,
    int G, float eps) {
    
    const int b = blockIdx.z;
    const int g = blockIdx.y;
    const int c_start = g * (C / G);
    const int c_end = (g + 1) * (C / G);
    const int spatial_size = D * H * W;
    
    extern __shared__ float smem[];
    float* group_sum = smem;
    float* group_sum_sq = smem + 1;

    // Phase 1: Compute group statistics
    float thread_sum = 0.0f;
    float thread_sum_sq = 0.0f;
    
    for (int c = c_start; c < c_end; ++c) {
        for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
            int d = i / (H * W);
            int h = (i % (H * W)) / W;
            int w = i % W;
            
            scalar_t val = input[((b * C + c) * D + d) * H * W + h * W + w];
            val = val * fminf(fmaxf(val + 3.0f, 0.0f), 6.0f) / 6.0f;
            
            thread_sum += val;
            thread_sum_sq += val * val;
        }
    }
    
    // Block reduction for statistics
    float block_sum = blockReduceSum<scalar_t>(thread_sum);
    float block_sum_sq = blockReduceSum<scalar_t>(thread_sum_sq);
    
    if (threadIdx.x == 0) {
        *group_sum = block_sum;
        *group_sum_sq = block_sum_sq;
    }
    __syncthreads();
    
    // Compute group statistics
    const float mean = *group_sum / ((C/G) * spatial_size);
    const float var = *group_sum_sq/((C/G) * spatial_size) - mean*mean;
    const float inv_std = rsqrtf(var + eps);
    
    // Phase 2: Normalize and compute means
    for (int c = c_start; c < c_end; ++c) {
        float channel_sum = 0.0f;
        
        for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
            int d = i / (H * W);
            int h = (i % (H * W)) / W;
            int w = i % W;
            
            scalar_t val = input[((b * C + c) * D + d) * H * W + h * W + w];
            val = val * fminf(fmaxf(val + 3.0f, 0.0f), 6.0f) / 6.0f;
            val = (val - mean) * inv_std * weight[c] + bias[c];
            
            channel_sum += val;
        }
        
        // Reduce channel sum
        float total = blockReduceSum<scalar_t>(channel_sum);
        if (threadIdx.x == 0) {
            output[b * C + c] = total / spatial_size;
        }
        __syncthreads();
    }
}

torch::Tensor fused_op_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int num_groups,
    float eps) {
    
    at::cuda::CUDAGuard guard(input.device());
    auto B = input.size(0);
    auto C = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);
    auto output = torch::zeros({B, C}, input.options());
    
    dim3 blocks(1, num_groups, B);
    int threads = 256;
    size_t smem = 2 * sizeof(float);
    
    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "fused_op", ([&] {
        fused_kernel<scalar_t><<<blocks, threads, smem>>>(
            input.data_ptr<scalar_t>(),
            weight.data_ptr<scalar_t>(),
            bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            B, C, D, H, W,
            num_groups, eps);
    }));
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_op_cuda(torch::Tensor, torch::Tensor, torch::Tensor, int, float);"

fused_op = load_inline(
    name="fused_op",
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=["fused_op_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)
        self.num_groups = num_groups
        self.weight = nn.Parameter(torch.ones(out_channels))
        self.bias = nn.Parameter(torch.zeros(out_channels)) if bias else None
        self.eps = 1e-5

    def forward(self, x):
        x = self.conv(x)
        bias = self.bias if self.bias is not None else torch.zeros_like(self.weight)
        return fused_op.fused_op_cuda(x, self.weight, bias, self.num_groups, self.eps)
```

Kernel 2 (runtime: 54.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused kernel for HardSwish + GroupNorm + Spatial Mean
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <c10/cuda/CUDAGuard.h>

template<typename scalar_t>
__device__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

template<typename scalar_t>
__device__ float blockReduceSum(float val) {
    static __shared__ float shared[32];
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    val = warpReduceSum<scalar_t>(val);

    if (lane == 0) shared[wid] = val;
    __syncthreads();

    val = (threadIdx.x < (blockDim.x + 31) / 32) ? shared[lane] : 0.0f;
    if (wid == 0) val = warpReduceSum<scalar_t>(val);
    return val;
}

template<typename scalar_t>
__global__ void fused_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ weight,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int B, int C, int D, int H, int W,
    int G, float eps) {
    
    const int b = blockIdx.z;
    const int g = blockIdx.y;
    const int c_start = g * (C / G);
    const int c_end = (g + 1) * (C / G);
    const int spatial_size = D * H * W;
    
    extern __shared__ float smem[];
    float* group_sum = smem;
    float* group_sum_sq = smem + 1;

    // Phase 1: Compute group statistics
    float thread_sum = 0.0f;
    float thread_sum_sq = 0.0f;
    
    for (int c = c_start; c < c_end; ++c) {
        for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
            int d = i / (H * W);
            int h = (i % (H * W)) / W;
            int w = i % W;
            
            scalar_t val = input[((b * C + c) * D + d) * H * W + h * W + w];
            val = val * fminf(fmaxf(val + 3.0f, 0.0f), 6.0f) / 6.0f;
            
            thread_sum += val;
            thread_sum_sq += val * val;
        }
    }
    
    // Block reduction for statistics
    float block_sum = blockReduceSum<scalar_t>(thread_sum);
    float block_sum_sq = blockReduceSum<scalar_t>(thread_sum_sq);
    
    if (threadIdx.x == 0) {
        *group_sum = block_sum;
        *group_sum_sq = block_sum_sq;
    }
    __syncthreads();
    
    // Compute group statistics
    const float mean = *group_sum / (C/G * spatial_size);
    const float var = *group_sum_sq/(C/G * spatial_size) - mean*mean;
    const float inv_std = rsqrtf(var + eps);
    
    // Phase 2: Normalize and compute means
    for (int c = c_start; c < c_end; ++c) {
        float channel_sum = 0.0f;
        
        for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
            int d = i / (H * W);
            int h = (i % (H * W)) / W;
            int w = i % W;
            
            scalar_t val = input[((b * C + c) * D + d) * H * W + h * W + w];
            val = val * fminf(fmaxf(val + 3.0f, 0.0f), 6.0f) / 6.0f;
            val = (val - mean) * inv_std * weight[c] + bias[c];
            
            channel_sum += val;
        }
        
        // Reduce channel sum
        float total = blockReduceSum<scalar_t>(channel_sum);
        if (threadIdx.x == 0) {
            output[b * C + c] = total / spatial_size;
        }
        __syncthreads();
    }
}

torch::Tensor fused_op_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int num_groups,
    float eps) {
    
    at::cuda::CUDAGuard guard(input.device());
    auto B = input.size(0);
    auto C = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);
    auto output = torch::zeros({B, C}, input.options());
    
    dim3 blocks(1, num_groups, B);
    int threads = 256;
    size_t smem = 2 * sizeof(float);  // For group_sum and group_sum_sq
    
    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "fused_op", ([&] {
        fused_kernel<scalar_t><<<blocks, threads, smem>>>(
            input.data_ptr<scalar_t>(),
            weight.data_ptr<scalar_t>(),
            bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            B, C, D, H, W,
            num_groups, eps);
    }));
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_op_cuda(torch::Tensor, torch::Tensor, torch::Tensor, int, float);"

fused_op = load_inline(
    name="fused_op",
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=["fused_op_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)
        self.num_groups = num_groups
        self.weight = nn.Parameter(torch.ones(out_channels))
        self.bias = nn.Parameter(torch.zeros(out_channels)) if bias else None
        self.eps = 1e-5

    def forward(self, x):
        x = self.conv(x)
        bias = self.bias if self.bias is not None else torch.zeros_like(self.weight)
        return fused_op.fused_op_cuda(x, self.weight, bias, self.num_groups, self.eps)
```
