You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 28.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel with mathematical simplification and correct remainder handling
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_ops_kernel(
    const float* __restrict__ conv_output,
    const float* __restrict__ bias,
    float* __restrict__ output,
    float inv_scale,
    int N, int C, int H, int W
) {
    __shared__ float s_bias;
    const int tid = threadIdx.x;
    const int n = blockIdx.x;
    const int c = blockIdx.y;
    
    // Load bias once per block using shared memory
    if (tid == 0) s_bias = __ldg(bias + c);
    __syncthreads();
    
    const int hw = H * W;
    const int vec_size = 4;
    const int base = n * C * H * W + c * H * W;
    const float upper_limit = fminf(1.0f, inv_scale);
    
    const float4* base_ptr = reinterpret_cast<const float4*>(conv_output + base);
    float4* out_ptr = reinterpret_cast<float4*>(output + base);
    
    const int vec_count = hw / vec_size;
    const int vec_processed = vec_count * vec_size;
    
    // Vectorized processing of full 4-element chunks
    for (int i = tid; i < vec_count; i += blockDim.x) {
        float4 conv_val = __ldg(base_ptr + i);
        float4 out_val;
        
        #pragma unroll
        for (int v = 0; v < vec_size; ++v) {
            float val = (&conv_val.x)[v] + s_bias;
            val = fmaxf(0.0f, fminf(val, upper_limit));
            (&out_val.x)[v] = val;
        }
        
        out_ptr[i] = out_val;
    }
    
    // Process remaining elements using scalar operations
    const int remaining = hw - vec_processed;
    for (int i = tid; i < remaining; i += blockDim.x) {
        const int linear_idx = base + vec_processed + i;
        float val = __ldg(conv_output + linear_idx) + s_bias;
        val = fmaxf(0.0f, fminf(val, upper_limit));
        output[linear_idx] = val;
    }
}

torch::Tensor fused_ops_cuda(
    torch::Tensor conv_output,
    torch::Tensor bias,
    float scaling_factor
) {
    auto output = torch::empty_like(conv_output);
    const int N = conv_output.size(0), C = conv_output.size(1);
    const float inv_scale = 1.0f / scaling_factor;
    
    dim3 grid(N, C);
    const int block_size = 256;
    
    fused_ops_kernel<<<grid, block_size>>>(
        conv_output.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        inv_scale,
        N, C, conv_output.size(2), conv_output.size(3)
    );
    
    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_ops_cuda(torch::Tensor conv_output, torch::Tensor bias, float scaling_factor);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=["fused_ops_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_ops_cuda(x, self.bias, self.scaling_factor)
        return x
```

Kernel 2 (runtime: 28.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_operations_kernel(float* input, const float* bias, int N, int C, int H, int W, float scaling_factor) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int total_elements = N * C * H * W;
    if (idx >= total_elements) return;

    const int HW = H * W;
    const int hw_idx = idx % HW;
    const int h = hw_idx / W;
    const int w = hw_idx % W;
    const int c = (idx / HW) % C;

    float val = input[idx];
    val += bias[c]; // Add bias per channel
    val = fmaxf(fminf(val, 1.0f), 0.0f); // Clamp between 0 and 1
    val *= scaling_factor;
    val = fmaxf(fminf(val, 1.0f), 0.0f);
    val /= scaling_factor; // Divide by scaling factor
    input[idx] = val;
}

torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor) {
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");
    TORCH_CHECK(bias.dim() == 3 && bias.size(1) == 1 && bias.size(2) == 1, "Bias must be [C,1,1]");
    TORCH_CHECK(scaling_factor > 0, "Scaling factor must be positive");

    auto input_flat = input.view({-1});
    const int numel = input_flat.numel();

    dim3 blocks((numel + 255)/256);
    dim3 threads(256);

    fused_operations_kernel<<<blocks, threads>>>(
        input_flat.data_ptr<float>(),
        bias.contiguous().data_ptr<float>(),
        input.size(0),
        input.size(1),
        input.size(2),
        input.size(3),
        scaling_factor
    );

    return input;
}
"""

fused_ops_cpp = "torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_operations_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_operations_cuda(x, self.bias, self.scaling_factor)
        return x
```
