You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 9.94 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused GroupNorm+Tanh+HardSwish+Residual kernel
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_group_norm_act_residual_kernel(
    const float* x_conv,
    const float* gamma,
    const float* beta,
    float* output,
    int N, int C, int H, int W,
    int groups,
    float eps
) {
    int n = blockIdx.y;
    int g = blockIdx.x;
    int channels_per_group = C / groups;
    int start_c = g * channels_per_group;
    int elements_per_group = channels_per_group * H * W;

    extern __shared__ float sdata[];
    float* sum_ptr = sdata;
    float* sum_sq_ptr = &sdata[blockDim.x];

    float sum = 0.0f;
    float sum_sq = 0.0f;

    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        int c = start_c + (i / (H * W));
        int hw = i % (H * W);
        int h = hw / W;
        int w = hw % W;
        float val = x_conv[((n * C + c) * H + h) * W + w];
        sum += val;
        sum_sq += val * val;
    }

    sum_ptr[threadIdx.x] = sum;
    sum_sq_ptr[threadIdx.x] = sum_sq;
    __syncthreads();

    // Parallel reduction
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sum_ptr[threadIdx.x] += sum_ptr[threadIdx.x + s];
            sum_sq_ptr[threadIdx.x] += sum_sq_ptr[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        float mean = sum_ptr[0] / elements_per_group;
        float var = (sum_sq_ptr[0]/elements_per_group) - (mean*mean) + eps;
        sum_ptr[0] = mean;
        sum_sq_ptr[0] = 1.0f / sqrtf(var);
    }
    __syncthreads();

    float mean = sum_ptr[0];
    float inv_std = sum_sq_ptr[0];

    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        int c = start_c + (i / (H * W));
        int hw = i % (H * W);
        int h = hw / W;
        int w = hw % W;
        float x = x_conv[((n * C + c) * H + h) * W + w];
        
        // GroupNorm
        float normalized = (x - mean) * inv_std;
        float scaled = normalized * gamma[c] + beta[c];
        
        // Tanh + HardSwish
        float tanh_val = tanhf(scaled);
        float hard_swish_val = tanh_val * __fdividef(fminf(fmaxf(tanh_val + 3.0f, 0.0f), 6.0f), 6.0f);
        
        // Residual addition
        output[((n * C + c) * H + h) * W + w] = x_conv[((n * C + c) * H + h) * W + w] + hard_swish_val;
    }
}

torch::Tensor fused_group_norm_act_residual_cuda(
    const torch::Tensor& x_conv,
    const torch::Tensor& gamma,
    const torch::Tensor& beta,
    int groups,
    float eps
) {
    auto output = torch::empty_like(x_conv);
    int N = x_conv.size(0);
    int C = x_conv.size(1);
    int H = x_conv.size(2);
    int W = x_conv.size(3);

    TORCH_CHECK(C % groups == 0, "Number of channels must be divisible by groups");

    dim3 block(256);
    dim3 grid(groups, N);
    size_t shared_mem_size = 2 * block.x * sizeof(float);

    fused_group_norm_act_residual_kernel<<<grid, block, shared_mem_size>>>(
        x_conv.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        groups,
        eps
    );

    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_group_norm_act_residual_cuda(const torch::Tensor& x_conv, const torch::Tensor& gamma, const torch::Tensor& beta, int groups, float eps);
"""

fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_group_norm_act_residual_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, groups, eps=1e-5):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.group_norm = nn.GroupNorm(groups, out_channels, eps=eps)

    def forward(self, x):
        x_conv = self.conv(x)
        x_res = fused_op.fused_group_norm_act_residual_cuda(
            x_conv,
            self.group_norm.weight,
            self.group_norm.bias,
            self.group_norm.num_groups,
            self.group_norm.eps
        )
        return torch.logsumexp(x_res, dim=1, keepdim=True)
```

Kernel 2 (runtime: 9.52 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused activation + residual kernel
fused_activations_residual_cpp = """
torch::Tensor fused_activations_residual_cuda(torch::Tensor x_conv, torch::Tensor x_norm);
"""
fused_activations_residual_cuda = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_activations_residual_kernel(const float* x_conv, const float* x_norm, float* out, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float norm_val = x_norm[idx];
        float tanh_val = tanhf(norm_val);
        float hs = tanh_val * fminf(fmaxf(tanh_val + 3.0f, 0.0f), 6.0f) / 6.0f;
        out[idx] = x_conv[idx] + hs;
    }
}

torch::Tensor fused_activations_residual_cuda(torch::Tensor x_conv, torch::Tensor x_norm) {
    TORCH_CHECK(x_conv.sizes() == x_norm.sizes(), "Input tensors must have same shape");
    auto out = torch::zeros_like(x_conv);
    int num_elements = x_conv.numel();
    
    const int block_size = 256;
    int grid_size = (num_elements + block_size - 1) / block_size;
    
    fused_activations_residual_kernel<<<grid_size, block_size>>>(
        x_conv.data_ptr<float>(),
        x_norm.data_ptr<float>(),
        out.data_ptr<float>(),
        num_elements
    );
    
    return out;
}
"""

fused_activations_residual = load_inline(
    name='fused_activations_residual',
    cpp_sources=fused_activations_residual_cpp,
    cuda_sources=fused_activations_residual_cuda,
    functions=['fused_activations_residual_cuda'],
    verbose=False
)

# Optimized LogSumExp kernel
logsumexp_cpp = """
torch::Tensor logsumexp_cuda(torch::Tensor input);
"""
logsumexp_cuda = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void logsumexp_kernel(const float* input, float* output, int batch_size, int channels, int height, int width) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int total_positions = batch_size * height * width;
    if (tid >= total_positions) return;

    int n = tid / (height * width);
    int hw = tid % (height * width);
    int h = hw / width;
    int w = hw % width;

    int input_base = n * channels * height * width + h * width + w;
    float max_val = -INFINITY;

    for (int c = 0; c < channels; ++c) {
        float val = input[input_base + c * height * width];
        if (val > max_val) max_val = val;
    }

    float sum_exp = 0.0f;
    for (int c = 0; c < channels; ++c) {
        sum_exp += expf(input[input_base + c * height * width] - max_val);
    }

    output[n * height * width + h * width + w] = logf(sum_exp) + max_val;
}

torch::Tensor logsumexp_cuda(torch::Tensor input) {
    TORCH_CHECK(input.dim() == 4, "Input must be 4D tensor");
    int batch_size = input.size(0);
    int channels = input.size(1);
    int height = input.size(2);
    int width = input.size(3);

    auto output = torch::empty({batch_size, 1, height, width}, input.options());
    int total_positions = batch_size * height * width;
    
    const int block_size = 256;
    int grid_size = (total_positions + block_size - 1) / block_size;
    
    logsumexp_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        height,
        width
    );
    
    return output;
}
"""

logsumexp = load_inline(
    name='logsumexp',
    cpp_sources=logsumexp_cpp,
    cuda_sources=logsumexp_cuda,
    functions=['logsumexp_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, groups, eps=1e-5):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.group_norm = nn.GroupNorm(groups, out_channels, eps=eps)
        self.fused_activations_residual = fused_activations_residual
        self.logsumexp = logsumexp

    def forward(self, x):
        x_conv = self.conv(x)
        x_norm = self.group_norm(x_conv)
        x_res = self.fused_activations_residual.fused_activations_residual_cuda(x_conv, x_norm)
        return self.logsumexp.logsumexp_cuda(x_res)
```
