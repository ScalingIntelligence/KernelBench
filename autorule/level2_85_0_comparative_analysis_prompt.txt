You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 10.4 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused GroupNorm + Scale kernel with shared memory reduction
group_norm_scale_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void group_norm_scale_kernel(
    const float* __restrict__ input,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    const float* __restrict__ scale,
    float* __restrict__ output,
    int N, int C, int H, int W,
    int G,
    float eps) {

    extern __shared__ float smem[];
    const int n = blockIdx.y;
    const int g = blockIdx.x;
    const int tid = threadIdx.x;
    const int channels_per_group = C / G;
    const int group_start = g * channels_per_group;
    const int group_end = group_start + channels_per_group;

    // Parallel reduction for group statistics
    float sum = 0.0f, sum_sq = 0.0f;
    for(int c = group_start + tid; c < group_end; c += blockDim.x) {
        for(int hw = 0; hw < H*W; ++hw) {
            const float val = input[((n * C + c) * H * W) + hw];
            sum += val;
            sum_sq += val * val;
        }
    }

    // Block reduction
    float* shared_sum = smem;
    float* shared_sum_sq = smem + blockDim.x;
    shared_sum[tid] = sum;
    shared_sum_sq[tid] = sum_sq;
    __syncthreads();

    for(int s = blockDim.x/2; s > 0; s >>= 1) {
        if(tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
            shared_sum_sq[tid] += shared_sum_sq[tid + s];
        }
        __syncthreads();
    }

    // Compute group statistics
    if(tid == 0) {
        const float mean = shared_sum[0] / (channels_per_group * H * W);
        const float var = shared_sum_sq[0]/(channels_per_group * H * W) - mean*mean;
        smem[0] = mean;
        smem[1] = rsqrtf(var + eps);
    }
    __syncthreads();

    const float mean = smem[0];
    const float inv_std = smem[1];

    // Apply normalization and scaling
    for(int c = group_start + tid; c < group_end; c += blockDim.x) {
        const float combined_gamma = gamma[c] * scale[c];
        const float combined_beta = beta[c] * scale[c];
        for(int hw = 0; hw < H*W; ++hw) {
            const int idx = ((n * C + c) * H * W) + hw;
            output[idx] = (input[idx] - mean) * inv_std * combined_gamma + combined_beta;
        }
    }
}

torch::Tensor group_norm_scale_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor scale,
    int num_groups,
    float eps) {
    
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    
    auto output = torch::empty_like(input);
    const int block_size = 256;
    const dim3 blocks(num_groups, N);
    const size_t smem_size = 2 * block_size * sizeof(float);

    group_norm_scale_kernel<<<blocks, block_size, smem_size>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        scale.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        num_groups,
        eps);
        
    return output;
}
"""

# Optimized fused MaxPool + Clamp kernel with 2D block structure
max_pool_clamp_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_pool_clamp_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int N, int C, int H, int W,
    int kernel_size,
    float clamp_min,
    float clamp_max) {
    
    const int h_out = blockIdx.y * blockDim.y + threadIdx.y;
    const int w_out = blockIdx.x * blockDim.x + threadIdx.x;
    const int nc = blockIdx.z;
    const int n = nc / C;
    const int c = nc % C;
    
    const int H_out = H / kernel_size;
    const int W_out = W / kernel_size;
    
    if(n >= N || c >= C || h_out >= H_out || w_out >= W_out) return;
    
    float max_val = -INFINITY;
    const int h_start = h_out * kernel_size;
    const int w_start = w_out * kernel_size;
    const int h_end = min(h_start + kernel_size, H);
    const int w_end = min(w_start + kernel_size, W);
    
    for(int h = h_start; h < h_end; ++h) {
        for(int w = w_start; w < w_end; ++w) {
            const float val = input[((n * C + c) * H + h) * W + w];
            if(val > max_val) max_val = val;
        }
    }
    
    output[((n * C + c) * H_out + h_out) * W_out + w_out] = 
        fminf(fmaxf(max_val, clamp_min), clamp_max);
}

torch::Tensor max_pool_clamp_cuda(
    torch::Tensor input,
    int kernel_size,
    float clamp_min,
    float clamp_max) {
    
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    
    const int H_out = H / kernel_size;
    const int W_out = W / kernel_size;
    auto output = torch::empty({N, C, H_out, W_out}, input.options());
    
    const dim3 block(16, 16);
    const dim3 grid(
        (W_out + block.x - 1) / block.x,
        (H_out + block.y - 1) / block.y,
        N * C
    );
    
    max_pool_clamp_kernel<<<grid, block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        kernel_size,
        clamp_min,
        clamp_max);
        
    return output;
}
"""

# Compile kernels
group_norm_scale = load_inline(
    name='group_norm_scale',
    cpp_sources="torch::Tensor group_norm_scale_cuda(torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, int, float);",
    cuda_sources=group_norm_scale_source,
    functions=['group_norm_scale_cuda'],
    verbose=True
)

max_pool_clamp = load_inline(
    name='max_pool_clamp',
    cpp_sources="torch::Tensor max_pool_clamp_cuda(torch::Tensor, int, float, float);",
    cuda_sources=max_pool_clamp_source,
    functions=['max_pool_clamp_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        self.scale = nn.Parameter(torch.ones(scale_shape))
        self.num_groups = num_groups
        self.eps = 1e-5
        self.maxpool_kernel_size = maxpool_kernel_size
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        x = self.conv(x)
        x = group_norm_scale.group_norm_scale_cuda(
            x, 
            self.gamma, 
            self.beta,
            self.scale.squeeze(),
            self.num_groups,
            self.eps
        )
        x = max_pool_clamp.max_pool_clamp_cuda(
            x,
            self.maxpool_kernel_size,
            self.clamp_min,
            self.clamp_max
        )
        return x
```

Kernel 2 (runtime: 14.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused GroupNorm + Scale CUDA kernel with shared memory optimization
group_norm_scale_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void group_norm_scale_kernel(
    const float* input,
    const float* gamma,
    const float* beta,
    const float* scale,
    float* output,
    int N, int C, int H, int W,
    int G,
    float eps) {

    extern __shared__ float smem[];
    float* sum_smem = smem;
    float* sum_sq_smem = smem + blockDim.x;

    const int n = blockIdx.y;
    const int g = blockIdx.x;
    const int tid = threadIdx.x;

    const int channels_per_group = C / G;
    const int group_start = g * channels_per_group;
    const int group_end = group_start + channels_per_group;

    float sum = 0.0f;
    float sum_sq = 0.0f;

    // Each thread processes multiple channels in the group
    for (int c = group_start + tid; c < group_end; c += blockDim.x) {
        for (int h = 0; h < H; ++h) {
            for (int w = 0; w < W; ++w) {
                const int idx = ((n * C + c) * H + h) * W + w;
                const float val = input[idx];
                sum += val;
                sum_sq += val * val;
            }
        }
    }

    sum_smem[tid] = sum;
    sum_sq_smem[tid] = sum_sq;
    __syncthreads();

    // Block reduction for sum and sum_sq
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sum_smem[tid] += sum_smem[tid + s];
            sum_sq_smem[tid] += sum_sq_smem[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        const float total_sum = sum_smem[0];
        const float total_sum_sq = sum_sq_smem[0];
        const int num_elements = channels_per_group * H * W;
        const float mean = total_sum / num_elements;
        const float var = (total_sum_sq / num_elements) - (mean * mean);
        const float inv_std = rsqrtf(var + eps);

        // Store mean and inv_std in shared memory
        smem[0] = mean;
        smem[1] = inv_std;
    }
    __syncthreads();

    const float mean = smem[0];
    const float inv_std = smem[1];

    // Apply normalization and scaling
    for (int c = group_start + tid; c < group_end; c += blockDim.x) {
        const float combined_gamma = gamma[c] * scale[c];
        const float combined_beta = beta[c] * scale[c];
        for (int h = 0; h < H; ++h) {
            for (int w = 0; w < W; ++w) {
                const int idx = ((n * C + c) * H + h) * W + w;
                const float val = input[idx];
                const float normalized = (val - mean) * inv_std;
                output[idx] = normalized * combined_gamma + combined_beta;
            }
        }
    }
}

torch::Tensor group_norm_scale_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor scale,
    int num_groups,
    float eps) {
    
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    const int G = num_groups;

    auto output = torch::empty_like(input);

    const int block_size = 256;
    const dim3 blocks(G, N); // Each block handles a group and sample
    const size_t smem_size = 2 * block_size * sizeof(float);

    group_norm_scale_kernel<<<blocks, block_size, smem_size>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        scale.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        G,
        eps);
        
    return output;
}
"""

# Fused MaxPool + Clamp CUDA kernel with boundary checks
max_pool_clamp_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_pool_clamp_kernel(
    const float* input,
    float* output,
    int N, int C, int H, int W,
    int kernel_size,
    float clamp_min,
    float clamp_max) {
    
    const int h_out = blockIdx.y * blockDim.y + threadIdx.y;
    const int w_out = blockIdx.x * blockDim.x + threadIdx.x;
    const int nc = blockIdx.z;
    const int n = nc / C;
    const int c = nc % C;
    
    const int H_out = H / kernel_size;
    const int W_out = W / kernel_size;
    
    if(n >= N || c >= C || h_out >= H_out || w_out >= W_out) return;
    
    float max_val = -INFINITY;
    const int h_start = h_out * kernel_size;
    const int w_start = w_out * kernel_size;
    const int h_end = min(h_start + kernel_size, H);
    const int w_end = min(w_start + kernel_size, W);
    
    for(int h = h_start; h < h_end; ++h) {
        for(int w = w_start; w < w_end; ++w) {
            const float val = input[((n * C + c) * H + h) * W + w];
            max_val = fmaxf(max_val, val);
        }
    }
    
    output[((n * C + c) * H_out + h_out) * W_out + w_out] = 
        fminf(fmaxf(max_val, clamp_min), clamp_max);
}

torch::Tensor max_pool_clamp_cuda(
    torch::Tensor input,
    int kernel_size,
    float clamp_min,
    float clamp_max) {
    
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    
    const int H_out = H / kernel_size;
    const int W_out = W / kernel_size;
    auto output = torch::empty({N, C, H_out, W_out}, input.options());
    
    const dim3 block(16, 16);
    const dim3 grid(
        (W_out + block.x - 1) / block.x,
        (H_out + block.y - 1) / block.y,
        N * C
    );
    
    max_pool_clamp_kernel<<<grid, block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        kernel_size,
        clamp_min,
        clamp_max);
        
    return output;
}
"""

# Compile kernels
group_norm_scale = load_inline(
    name='group_norm_scale',
    cpp_sources="torch::Tensor group_norm_scale_cuda(torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, int, float);",
    cuda_sources=group_norm_scale_source,
    functions=['group_norm_scale_cuda'],
    verbose=True
)

max_pool_clamp = load_inline(
    name='max_pool_clamp',
    cpp_sources="torch::Tensor max_pool_clamp_cuda(torch::Tensor, int, float, float);",
    cuda_sources=max_pool_clamp_source,
    functions=['max_pool_clamp_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        self.scale = nn.Parameter(torch.ones(scale_shape))
        self.num_groups = num_groups
        self.eps = 1e-5
        self.maxpool_kernel_size = maxpool_kernel_size
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        x = self.conv(x)
        x = group_norm_scale.group_norm_scale_cuda(
            x, 
            self.gamma, 
            self.beta,
            self.scale.squeeze(),
            self.num_groups,
            self.eps
        )
        x = max_pool_clamp.max_pool_clamp_cuda(
            x,
            self.maxpool_kernel_size,
            self.clamp_min,
            self.clamp_max
        )
        return x
```
