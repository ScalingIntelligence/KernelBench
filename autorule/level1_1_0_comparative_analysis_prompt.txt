You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 45.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template<typename scalar_t>
__global__ void fused_matmul_kernel(const scalar_t* A, const scalar_t* B, scalar_t* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if(row >= N || col >= N) return;
    
    scalar_t c = static_cast<scalar_t>(0);
    __shared__ scalar_t SAs[16][16], Sab[16][16];
    
    for(int m = 0; m < (N +15)/16; ++m) {
        // Load A tile
        int a_row = row;
        int a_col = m * 16 + threadIdx.x;
        SAs[threadIdx.y][threadIdx.x] = (a_col < N) ? A[a_row*N + a_col] : static_cast<scalar_t>(0);
        
        // Load B tile
        int b_row = m * 16 + threadIdx.y;
        int b_col = col;
        Sab[threadIdx.y][threadIdx.x] = (b_row < N) ? B[b_row*N + b_col] : static_cast<scalar_t>(0);
        
        __syncthreads();
        
        // Compute partial sums
        for(int k = 0; k < 16; ++k) {
            c += SAs[threadIdx.y][k] * Sab[k][threadIdx.x];
        }
        __syncthreads();
    }
    C[row*N + col] = c;
}

torch::Tensor fused_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Inputs must be 2D");
    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1), "Matrices must be square");
    int N = A.size(0);
    auto C = torch::empty({N, N}, A.options());
    
    dim3 threadsPerBlock(16, 16);
    dim3 blocksPerGrid((N+15)/16, (N+15)/16);
    
    AT_DISPATCH_FLOATING_TYPES(A.scalar_type(), "fused_matmul", ([&]{
        fused_matmul_kernel<scalar_t><<<blocksPerGrid, threadsPerBlock>>>(
            A.data_ptr<scalar_t>(),
            B.data_ptr<scalar_t>(),
            C.data_ptr<scalar_t>(),
            N
        );
    }));
    
    return C;
}
"""

matmul_cpp_source = "torch::Tensor fused_matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul_ext = load_inline(
    name='matmul_ext',
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=['fused_matmul_cuda'],
    verbose=False,
    extra_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.fused_matmul = matmul_ext

    def forward(self, A, B):
        return self.fused_matmul.fused_matmul_cuda(A.contiguous(), B.contiguous())
```

Kernel 2 (runtime: 45.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define TILE_SIZE 16

__global__ void fused_matmul_kernel(const float* A, const float* B, float* C, int N) {
    __shared__ float Atile[TILE_SIZE][TILE_SIZE];
    __shared__ float Btile[TILE_SIZE][TILE_SIZE];

    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;

    float sum = 0.0f;

    for (int m = 0; m < N/TILE_SIZE; ++m) {
        // Load tiles into shared memory
        Atile[ty][tx] = A[row*N + (m*TILE_SIZE + tx)];
        Btile[ty][tx] = B[(m*TILE_SIZE + ty)*N + col];
        
        __syncthreads();

        // Compute partial sums
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += Atile[ty][k] * Btile[k][tx];
        }

        __syncthreads();
    }

    if (row < N && col < N) {
        C[row*N + col] = sum;
    }
}

torch::Tensor fused_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Matrices must be 2D");
    int N = A.size(0);
    TORCH_CHECK(A.size(0) == N && A.size(1) == N && B.size(0) == N && B.size(1) == N, "Matrices must be square");

    auto C = torch::empty({N, N}, A.options());
    dim3 blocks((N + TILE_SIZE-1)/TILE_SIZE, (N + TILE_SIZE-1)/TILE_SIZE);
    dim3 threads(TILE_SIZE, TILE_SIZE);

    fused_matmul_kernel<<<blocks, threads>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        N
    );

    return C;
}
"""

matmul_cpp_source = "torch::Tensor fused_matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul_ext = load_inline(
    name='fused_matmul',
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=['fused_matmul_cuda'],
    verbose=False,
    extra_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.fused_matmul = matmul_ext.fused_matmul_cuda

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        if A.device.type != 'cuda':
            A = A.cuda(non_blocking=True)
            B = B.cuda(non_blocking=True)
        return self.fused_matmul(A.contiguous(), B.contiguous())
```
