You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 23.6 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused softmax+bias+scale+sigmoid CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_softmax_bias_scale_sigmoid_kernel(
    const float* input,
    const float* bias,
    float scaling_factor,
    float* output,
    int batch_size,
    int num_channels,
    int height,
    int width
) {
    extern __shared__ float shared[];
    
    int b = blockIdx.x / (height * width);
    int hw = blockIdx.x % (height * width);
    int h = hw / width;
    int w = hw % width;
    int c = threadIdx.x;

    if (b >= batch_size || h >= height || w >= width || c >= num_channels) return;

    int input_idx = b * num_channels * height * width + c * height * width + h * width + w;
    float x = input[input_idx];

    // Block-wide max reduction
    shared[c] = x;
    __syncthreads();
    
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(c < stride && shared[c] < shared[c + stride]) {
            shared[c] = shared[c + stride];
        }
        __syncthreads();
    }
    
    float max_val = shared[0];
    __syncthreads();

    // Compute exp and sum
    float exp_val = expf(x - max_val);
    shared[c] = exp_val;
    __syncthreads();

    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(c < stride) {
            shared[c] += shared[c + stride];
        }
        __syncthreads();
    }
    
    float sum = shared[0];
    float softmax_val = exp_val / sum;

    // Apply bias, scaling, sigmoid
    float result = softmax_val + bias[c];
    result *= scaling_factor;
    result = 1.0f / (1.0f + expf(-result));
    
    output[input_idx] = result;
}

torch::Tensor fused_operations_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling_factor
) {
    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    TORCH_CHECK(bias.dim() == 3, "Bias must be 3D");
    
    auto output = torch::empty_like(input);
    int batch_size = input.size(0);
    int num_channels = input.size(1);
    int height = input.size(2);
    int width = input.size(3);

    int num_blocks = batch_size * height * width;
    int block_size = num_channels;
    size_t shared_mem = block_size * sizeof(float);

    fused_softmax_bias_scale_sigmoid_kernel<<<num_blocks, block_size, shared_mem>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        batch_size,
        num_channels,
        height,
        width
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=["fused_operations_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_operations_cuda(x, self.bias, self.scaling_factor)
        return x
```

Kernel 2 (runtime: 28.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__inline__ __device__ float warpReduceMax(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    }
    return val;
}

__inline__ __device__ float blockReduceMax(float val, float* shared) {
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;
    
    val = warpReduceMax(val);
    
    if (lane == 0) {
        shared[wid] = val;
    }
    __syncthreads();
    
    if (wid == 0) {
        val = (lane < (blockDim.x / 32)) ? shared[lane] : -INFINITY;
        val = warpReduceMax(val);
    }
    
    // Broadcast max value to all threads
    __syncthreads();
    if (threadIdx.x == 0) shared[0] = val;
    __syncthreads();
    val = shared[0];
    
    return val;
}

__inline__ __device__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}

__inline__ __device__ float blockReduceSum(float val, float* shared) {
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;
    
    val = warpReduceSum(val);
    
    if (lane == 0) {
        shared[wid] = val;
    }
    __syncthreads();
    
    if (wid == 0) {
        val = (lane < (blockDim.x / 32)) ? shared[lane] : 0.0f;
        val = warpReduceSum(val);
    }
    
    // Broadcast sum to all threads
    __syncthreads();
    if (threadIdx.x == 0) shared[0] = val;
    __syncthreads();
    val = shared[0];
    
    return val;
}

__global__ void fused_softmax_bias_scale_sigmoid_kernel(
    const float* input,
    const float* bias,
    float scaling_factor,
    float* output,
    int batch_size,
    int num_channels,
    int height,
    int width
) {
    extern __shared__ float shared[];
    float* max_shared = shared;
    float* sum_shared = &shared[blockDim.x / 32];

    int b = blockIdx.x / (height * width);
    int hw = blockIdx.x % (height * width);
    int h = hw / width;
    int w = hw % width;

    if (b >= batch_size || h >= height || w >= width) return;

    // First pass: compute max
    float thread_max = -INFINITY;
    for (int c = threadIdx.x; c < num_channels; c += blockDim.x) {
        int idx = ((b * num_channels + c) * height + h) * width + w;
        thread_max = fmaxf(thread_max, input[idx]);
    }
    float max_val = blockReduceMax(thread_max, max_shared);

    // Second pass: compute sum
    float thread_sum = 0.0f;
    for (int c = threadIdx.x; c < num_channels; c += blockDim.x) {
        int idx = ((b * num_channels + c) * height + h) * width + w;
        thread_sum += expf(input[idx] - max_val);
    }
    float sum = blockReduceSum(thread_sum, sum_shared);

    // Final pass: compute values
    for (int c = threadIdx.x; c < num_channels; c += blockDim.x) {
        int idx = ((b * num_channels + c) * height + h) * width + w;
        float exp_val = expf(input[idx] - max_val);
        float result = exp_val / sum;
        result += bias[c];
        result *= scaling_factor;
        result = 1.0f / (1.0f + expf(-result));
        output[idx] = result;
    }
}

torch::Tensor fused_operations_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling_factor
) {
    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    TORCH_CHECK(bias.dim() == 3, "Bias must be 3D");
    
    auto output = torch::empty_like(input);
    int batch_size = input.size(0);
    int num_channels = input.size(1);
    int height = input.size(2);
    int width = input.size(3);

    int num_blocks = batch_size * height * width;
    int block_size = 256;
    size_t shared_mem = 2 * (block_size / 32) * sizeof(float);

    fused_softmax_bias_scale_sigmoid_kernel<<<num_blocks, block_size, shared_mem>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        batch_size,
        num_channels,
        height,
        width
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=["fused_operations_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_operations_cuda(x, self.bias, self.scaling_factor)
        return x
```
