You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 8.42 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused Softmax-Sigmoid CUDA kernel
fused_activation_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

#define BLOCK_SIZE 256

template<typename T>
__device__ T warpReduceMax(T val) {
    for(int offset = 16; offset > 0; offset /= 2)
        val = max(val, __shfl_down_sync(0xffffffff, val, offset));
    return val;
}

template<typename T>
__device__ T blockReduceMax(T val) {
    static __shared__ T shared[32];
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    val = warpReduceMax(val);

    if(lane == 0) shared[wid] = val;
    __syncthreads();

    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : -FLT_MAX;
    val = warpReduceMax(val);
    return val;
}

template<typename T>
__device__ T warpReduceSum(T val) {
    for(int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

template<typename T>
__device__ T blockReduceSum(T val) {
    static __shared__ T shared[32];
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    val = warpReduceSum(val);

    if(lane == 0) shared[wid] = val;
    __syncthreads();

    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;
    val = warpReduceSum(val);
    return val;
}

__global__ void fused_softmax_sigmoid_kernel(
    const float* input,
    float* output,
    int batch_size,
    int channels,
    int D,
    int H,
    int W
) {
    int spatial_size = D * H * W;
    int total_elements = batch_size * spatial_size;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if(idx >= total_elements) return;
    
    int b = idx / spatial_size;
    int spatial_idx = idx % spatial_size;
    
    int d = spatial_idx / (H * W);
    int hw = spatial_idx % (H * W);
    int h = hw / W;
    int w = hw % W;
    
    const float* channel_ptr = input + b * channels * spatial_size + spatial_idx;
    float* out_ptr = output + b * channels * spatial_size + spatial_idx;
    
    // Find max value for numerical stability
    float max_val = -INFINITY;
    for(int c = 0; c < channels; ++c) {
        float val = channel_ptr[c * spatial_size];
        max_val = fmaxf(max_val, val);
    }
    
    // Compute exp and sum
    float sum = 0.0f;
    for(int c = 0; c < channels; ++c) {
        float val = channel_ptr[c * spatial_size];
        sum += expf(val - max_val);
    }
    
    // Compute softmax and apply sigmoid
    for(int c = 0; c < channels; ++c) {
        float val = channel_ptr[c * spatial_size];
        float softmax = expf(val - max_val) / sum;
        float sigmoid = 1.0f / (1.0f + expf(-softmax));
        out_ptr[c * spatial_size] = sigmoid;
    }
}

torch::Tensor fused_softmax_sigmoid_cuda(torch::Tensor input) {
    auto input_shape = input.sizes();
    int batch_size = input_shape[0];
    int channels = input_shape[1];
    int D = input_shape[2];
    int H = input_shape[3];
    int W = input_shape[4];
    
    auto output = torch::empty_like(input);
    int total_elements = batch_size * D * H * W;
    
    dim3 blocks((total_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    dim3 threads(BLOCK_SIZE);
    
    fused_softmax_sigmoid_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        D,
        H,
        W
    );
    
    return output;
}
"""

fused_activation_cpp = "torch::Tensor fused_softmax_sigmoid_cuda(torch::Tensor input);"

# Load the custom CUDA kernel
fused_activation = load_inline(
    name='fused_activation',
    cpp_sources=fused_activation_cpp,
    cuda_sources=fused_activation_source,
    functions=['fused_softmax_sigmoid_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding, bias=bias
        )
        
    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_activation.fused_softmax_sigmoid_cuda(x.contiguous())
        return x
```

Kernel 2 (runtime: 21.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused kernel with improved memory coalescing and warp-aligned access
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_softmax_sigmoid_kernel(const float* __restrict__ input,
                                            float* __restrict__ output,
                                            int batch_size, int channels,
                                            int D, int H, int W) {
    extern __shared__ float shared[];
    const int tid = threadIdx.x;
    const int warp_id = tid / 32;
    const int lane_id = tid % 32;
    const int spatial_size = D * H * W;
    
    // Process elements in batches of 32 for better coalescing
    const int batch = blockIdx.x / spatial_size;
    const int spatial_idx = blockIdx.x % spatial_size;
    
    const int d = spatial_idx / (H * W);
    const int hw = spatial_idx % (H * W);
    const int h = hw / W;
    const int w = hw % W;

    if (batch >= batch_size || tid >= channels) return;

    // Calculate base index with improved memory access pattern
    const int base_idx = (batch * channels + tid) * spatial_size + d * H * W + h * W + w;
    const float val = input[base_idx];

    // Warp-level max reduction using shuffle
    float warp_max = val;
    for (int offset = 16; offset > 0; offset >>= 1) {
        warp_max = fmaxf(warp_max, __shfl_down_sync(0xffffffff, warp_max, offset));
    }

    if (lane_id == 0) {
        shared[warp_id] = warp_max;
    }
    __syncthreads();

    // Global max reduction across warps
    if (warp_id == 0) {
        float val = (lane_id < (blockDim.x + 31)/32) ? shared[lane_id] : -INFINITY;
        for (int offset = 16; offset > 0; offset >>= 1) {
            val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
        }
        if (lane_id == 0) {
            shared[0] = val;
        }
    }
    __syncthreads();
    const float max_val = shared[0];
    __syncthreads();

    // Compute exp with fast math and warp-level sum reduction
    const float exp_val = __expf(val - max_val);
    float warp_sum = exp_val;
    for (int offset = 16; offset > 0; offset >>= 1) {
        warp_sum += __shfl_down_sync(0xffffffff, warp_sum, offset);
    }

    if (lane_id == 0) {
        shared[warp_id] = warp_sum;
    }
    __syncthreads();

    // Global sum reduction across warps
    if (warp_id == 0) {
        float sum = (lane_id < (blockDim.x + 31)/32) ? shared[lane_id] : 0.0f;
        for (int offset = 16; offset > 0; offset >>= 1) {
            sum += __shfl_down_sync(0xffffffff, sum, offset);
        }
        if (lane_id == 0) {
            shared[0] = sum;
        }
    }
    __syncthreads();
    const float sum_exp = shared[0];

    // Compute final result with numerical stability
    const float result = 1.0f / (1.0f + __expf(-(exp_val / sum_exp)));
    output[base_idx] = result;
}

torch::Tensor fused_softmax_sigmoid_cuda(torch::Tensor input) {
    const auto sizes = input.sizes();
    auto output = torch::empty_like(input);
    
    const int batch_size = sizes[0];
    const int channels = sizes[1];
    const int D = sizes[2];
    const int H = sizes[3];
    const int W = sizes[4];
    
    const int spatial_size = D * H * W;
    const dim3 blocks(batch_size * spatial_size);
    const int threads_per_block = ((channels + 31) / 32) * 32;
    const dim3 threads(threads_per_block);
    const int num_warps = threads_per_block / 32;
    const size_t shared_size = num_warps * sizeof(float);
    
    fused_softmax_sigmoid_kernel<<<blocks, threads, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        D,
        H,
        W
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_softmax_sigmoid_cuda(torch::Tensor input);"

fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_softmax_sigmoid_cuda'],
    verbose=True,
    extra_cuda_cflags=['-use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, 
                 stride, padding, output_padding, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding, bias=bias
        )
        
    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_op.fused_softmax_sigmoid_cuda(x.contiguous())
        return x
```
