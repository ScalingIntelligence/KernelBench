You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 17.6 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the optimized fused CUDA kernel with warp-level reductions
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__inline__ __device__ float warpReduceMax(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    }
    return val;
}

__inline__ __device__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}

__global__ void fused_clamp_softmax_scale_kernel(
    const float* input,
    const float* scale,
    float* output,
    float clamp_min,
    float clamp_max,
    int batch_size,
    int channels,
    int spatial_size) {

    int bc = blockIdx.x;
    int b = bc / channels;
    int c = bc % channels;

    if (b >= batch_size) return;

    extern __shared__ float shared_mem[];
    float* max_shared = shared_mem;
    float* sum_shared = &shared_mem[blockDim.x / 32];

    int tid = threadIdx.x;
    int warp_id = tid / 32;
    int lane_id = tid % 32;
    int step = blockDim.x;

    // Clamp and find max
    float max_val = -INFINITY;
    for (int i = tid; i < spatial_size; i += step) {
        int idx = (b * channels + c) * spatial_size + i;
        float val = fmaxf(fminf(input[idx], clamp_max), clamp_min);
        max_val = fmaxf(max_val, val);
    }

    // Warp-level max reduction
    max_val = warpReduceMax(max_val);
    if (lane_id == 0) {
        max_shared[warp_id] = max_val;
    }
    __syncthreads();

    // First warp reduces per-warp maxes
    if (warp_id == 0) {
        float val = (lane_id < (blockDim.x / 32)) ? max_shared[lane_id] : -INFINITY;
        val = warpReduceMax(val);
        if (lane_id == 0) {
            max_shared[0] = val;
        }
    }
    __syncthreads();

    float block_max = max_shared[0];

    // Compute sum of exp(clamped values - max)
    float sum_exp = 0.0f;
    for (int i = tid; i < spatial_size; i += step) {
        int idx = (b * channels + c) * spatial_size + i;
        float val = fmaxf(fminf(input[idx], clamp_max), clamp_min);
        sum_exp += __expf(val - block_max);
    }

    // Warp-level sum reduction
    sum_exp = warpReduceSum(sum_exp);
    if (lane_id == 0) {
        sum_shared[warp_id] = sum_exp;
    }
    __syncthreads();

    // First warp reduces per-warp sums
    if (warp_id == 0) {
        float val = (lane_id < (blockDim.x / 32)) ? sum_shared[lane_id] : 0.0f;
        val = warpReduceSum(val);
        if (lane_id == 0) {
            sum_shared[0] = val;
        }
    }
    __syncthreads();

    float block_sum = sum_shared[0];
    float scale_val = scale[c];

    // Compute final values with scaling
    for (int i = tid; i < spatial_size; i += step) {
        int idx = (b * channels + c) * spatial_size + i;
        float val = fmaxf(fminf(input[idx], clamp_max), clamp_min);
        output[idx] = __expf(val - block_max) / block_sum * scale_val;
    }
}

torch::Tensor fused_clamp_softmax_scale_cuda(torch::Tensor input, torch::Tensor scale, float clamp_min, float clamp_max) {
    auto input_shape = input.sizes();
    int batch_size = input_shape[0];
    int channels = input_shape[1];
    int spatial_size = input_shape[2] * input_shape[3] * input_shape[4];

    auto output = torch::empty_like(input);

    int num_blocks = batch_size * channels;
    int block_size = 256;  // Must be multiple of 32
    size_t shared_mem_size = (block_size / 32) * 2 * sizeof(float);

    fused_clamp_softmax_scale_kernel<<<num_blocks, block_size, shared_mem_size>>>(
        input.data_ptr<float>(),
        scale.data_ptr<float>(),
        output.data_ptr<float>(),
        clamp_min,
        clamp_max,
        batch_size,
        channels,
        spatial_size
    );

    return output;
}
"""

fused_kernel_cpp_source = "torch::Tensor fused_clamp_softmax_scale_cuda(torch::Tensor input, torch::Tensor scale, float clamp_min, float clamp_max);"

# Compile the optimized kernel
fused_kernel = load_inline(
    name="fused_clamp_softmax_scale",
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=["fused_clamp_softmax_scale_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=["-lcudart"],
)

class ModelNew(nn.Module):
    """
    Final optimized model with:
    - Warp-level reductions for faster softmax
    - Minimal shared memory usage
    - Fused clamp + softmax + scale operations
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        self.avg_pool = nn.AvgPool3d(pool_kernel_size)
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max
        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))
        self.fused_kernel = fused_kernel

    def forward(self, x):
        x = self.avg_pool(x)
        x = self.conv_transpose(x)
        x = self.fused_kernel.fused_clamp_softmax_scale_cuda(x, self.scale, self.clamp_min, self.clamp_max)
        return x
```

Kernel 2 (runtime: 17.4 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused kernel with vectorized loads and warp-level reductions
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_fp16.h>
#include <cuda_runtime.h>
#include <math.h>

__inline__ __device__ float warpReduceMax(float val) {
    for (int offset = 16; offset > 0; offset >>= 1)
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    return val;
}

__inline__ __device__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void fused_clamp_softmax_scale_kernel(
    const float* __restrict__ input,
    const float* __restrict__ scales,
    float* __restrict__ output,
    float clamp_min,
    float clamp_max,
    int batch_size,
    int channels,
    int spatial_size
) {
    const int spatial_size4 = spatial_size / 4;
    const int batch = blockIdx.x;
    const int channel = blockIdx.y;
    
    const float4* input_ptr = reinterpret_cast<const float4*>(
        input + (batch * channels + channel) * spatial_size
    );
    float4* output_ptr = reinterpret_cast<float4*>(
        output + (batch * channels + channel) * spatial_size
    );
    const float scale = scales[channel];
    
    extern __shared__ float smem[];
    float* shared_max = smem;
    float* shared_sum = smem + 32;  // 32 warps per block
    
    // Step 1: Compute max of clamped values
    float local_max = -INFINITY;
    for (int i = threadIdx.x; i < spatial_size4; i += blockDim.x) {
        float4 val4 = input_ptr[i];
        float a = fmaxf(fminf(val4.x, clamp_max), clamp_min);
        float b = fmaxf(fminf(val4.y, clamp_max), clamp_min);
        float c = fmaxf(fminf(val4.z, clamp_max), clamp_min);
        float d = fmaxf(fminf(val4.w, clamp_max), clamp_min);
        local_max = fmaxf(local_max, fmaxf(fmaxf(a, b), fmaxf(c, d)));
    }
    
    // Warp-level max reduction
    float warp_max = warpReduceMax(local_max);
    if (threadIdx.x % 32 == 0)
        shared_max[threadIdx.x / 32] = warp_max;
    __syncthreads();
    
    // Block-level max reduction
    if (threadIdx.x < 32) {
        float val = (threadIdx.x < blockDim.x / 32) ? shared_max[threadIdx.x] : -INFINITY;
        val = warpReduceMax(val);
        if (threadIdx.x == 0)
            shared_max[0] = val;
    }
    __syncthreads();
    float global_max = shared_max[0];
    
    // Step 2: Compute sum of exp(clamped - global_max)
    float local_sum = 0.0f;
    for (int i = threadIdx.x; i < spatial_size4; i += blockDim.x) {
        float4 val4 = input_ptr[i];
        float a = fmaxf(fminf(val4.x, clamp_max), clamp_min);
        float b = fmaxf(fminf(val4.y, clamp_max), clamp_min);
        float c = fmaxf(fminf(val4.z, clamp_max), clamp_min);
        float d = fmaxf(fminf(val4.w, clamp_max), clamp_min);
        local_sum += expf(a - global_max) + expf(b - global_max) + 
                    expf(c - global_max) + expf(d - global_max);
    }
    
    // Warp-level sum reduction
    float warp_sum = warpReduceSum(local_sum);
    if (threadIdx.x % 32 == 0)
        shared_sum[threadIdx.x / 32] = warp_sum;
    __syncthreads();
    
    // Block-level sum reduction
    if (threadIdx.x < 32) {
        float val = (threadIdx.x < blockDim.x / 32) ? shared_sum[threadIdx.x] : 0.0f;
        val = warpReduceSum(val);
        if (threadIdx.x == 0)
            shared_sum[0] = val;
    }
    __syncthreads();
    float global_sum = shared_sum[0] + 1e-12f;
    
    // Step 3: Compute softmax and scale
    for (int i = threadIdx.x; i < spatial_size4; i += blockDim.x) {
        float4 val4 = input_ptr[i];
        float a = fmaxf(fminf(val4.x, clamp_max), clamp_min);
        float b = fmaxf(fminf(val4.y, clamp_max), clamp_min);
        float c = fmaxf(fminf(val4.z, clamp_max), clamp_min);
        float d = fmaxf(fminf(val4.w, clamp_max), clamp_min);
        
        a = expf(a - global_max) / global_sum * scale;
        b = expf(b - global_max) / global_sum * scale;
        c = expf(c - global_max) / global_sum * scale;
        d = expf(d - global_max) / global_sum * scale;
        
        output_ptr[i] = make_float4(a, b, c, d);
    }
}

torch::Tensor fused_clamp_softmax_scale_cuda(
    torch::Tensor input,
    torch::Tensor scales,
    float clamp_min,
    float clamp_max
) {
    TORCH_CHECK(input.dim() == 5, "Input must be 5D");
    TORCH_CHECK(scales.size(1) == input.size(1), "Channel mismatch");
    
    int batch_size = input.size(0);
    int channels = input.size(1);
    int spatial_size = input.size(2) * input.size(3) * input.size(4);
    TORCH_CHECK(spatial_size % 4 == 0, "Spatial size must be divisible by 4");
    
    auto input_contig = input.contiguous();
    auto output = torch::empty_like(input_contig);
    
    dim3 grid(batch_size, channels);
    int block_size = 1024;
    size_t shared_mem = (32 + 32) * sizeof(float);  // Max 32 warps per block
    
    fused_clamp_softmax_scale_kernel<<<grid, block_size, shared_mem>>>(
        input_contig.data_ptr<float>(),
        scales.contiguous().data_ptr<float>(),
        output.data_ptr<float>(),
        clamp_min,
        clamp_max,
        batch_size,
        channels,
        spatial_size
    );
    
    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_clamp_softmax_scale_cuda(torch::Tensor input, torch::Tensor scales, float clamp_min, float clamp_max);
"""

fused_clamp_softmax_scale = load_inline(
    name='fused_clamp_softmax_scale',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_clamp_softmax_scale_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        self.avg_pool = nn.AvgPool3d(pool_kernel_size)
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max
        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))

    def forward(self, x):
        x = self.avg_pool(x)
        x = self.conv_transpose(x)
        x = fused_clamp_softmax_scale.fused_clamp_softmax_scale_cuda(
            x, 
            self.scale,
            self.clamp_min,
            self.clamp_max
        )
        return x
```
