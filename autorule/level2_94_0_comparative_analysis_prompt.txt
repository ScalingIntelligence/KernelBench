You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.36 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = '''
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_bias_hardtanh_mish_kernel(const float* input, const float* bias, float* output, int batch_size, int out_features) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int elements_per_thread = 4;
    int idx = tid * elements_per_thread;
    int total = batch_size * out_features;
    
    if (idx < total) {
        float4 in_data = *reinterpret_cast<const float4*>(input + idx);
        float4 bias_data = *reinterpret_cast<const float4*>(bias + (idx % out_features));
        float4 out_data;

        #pragma unroll
        for (int i = 0; i < 4; ++i) {
            float val = (&in_data.x)[i] + (&bias_data.x)[i];
            val = fmaxf(-1.0f, fminf(1.0f, val));
            float sp = log1pf(expf(val));
            float tanh_sp = tanhf(sp);
            (&out_data.x)[i] = val * tanh_sp;
        }

        // Handle partial vector at the end
        int remaining = total - idx;
        if (remaining >= 4) {
            *reinterpret_cast<float4*>(output + idx) = out_data;
        } else {
            for (int i = 0; i < remaining; ++i) {
                output[idx + i] = (&out_data.x)[i];
            }
        }
    }
}

torch::Tensor fused_bias_hardtanh_mish_cuda(torch::Tensor input, torch::Tensor bias) {
    auto batch_size = input.size(0);
    auto out_features = input.size(1);
    auto output = torch::empty_like(input);

    int total_elements = batch_size * out_features;
    const int block_size = 512;
    int num_blocks = (total_elements + block_size * 4 - 1) / (block_size * 4);

    fused_bias_hardtanh_mish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features
    );

    return output;
}

__global__ void group_norm_kernel(const float* input, const float* gamma, const float* beta, float* output, 
                                  int batch_size, int num_groups, int channels_per_group, float epsilon) {
    int sample = blockIdx.x;
    int group = blockIdx.y;
    int tid = threadIdx.x;

    int channel = group * channels_per_group + tid;
    if (channel >= num_groups * channels_per_group) return;

    int input_idx = sample * (num_groups * channels_per_group) + channel;
    float x = input[input_idx];

    // Warp-wide reduction using shuffle instructions
    float sum = x;
    float sum_sq = x * x;
    for (int offset = 16; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
        sum_sq += __shfl_down_sync(0xffffffff, sum_sq, offset);
    }

    float mean = __shfl_sync(0xffffffff, sum, 0) / channels_per_group;
    float var = __shfl_sync(0xffffffff, sum_sq, 0) / channels_per_group - mean * mean;
    float inv_std = rsqrtf(var + epsilon);

    output[input_idx] = gamma[channel] * (x - mean) * inv_std + beta[channel];
}

torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, 
                              int num_groups, float epsilon) {
    auto batch_size = input.size(0);
    int channels = input.size(1);
    int channels_per_group = channels / num_groups;

    auto output = torch::empty_like(input);

    dim3 blocks(batch_size, num_groups);
    group_norm_kernel<<<blocks, channels_per_group>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_groups,
        channels_per_group,
        epsilon
    );

    return output;
}
'''

fused_ops_cpp_source = '''
torch::Tensor fused_bias_hardtanh_mish_cuda(torch::Tensor input, torch::Tensor bias);
torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int num_groups, float epsilon);
'''

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=['fused_bias_hardtanh_mish_cuda', 'group_norm_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.num_groups = num_groups
        self.epsilon = 1e-5
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))

    def forward(self, x):
        x = self.gemm(x)
        x = fused_ops.fused_bias_hardtanh_mish_cuda(x, self.bias)
        x = fused_ops.group_norm_cuda(x, self.gamma, self.beta, self.num_groups, self.epsilon)
        return x
```

Kernel 2 (runtime: 7.51 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused BiasAdd + Hardtanh + Mish kernel with vectorized access
fused_bias_act_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_fp16.h>

template<typename T>
__device__ __forceinline__ T mish(T x) {
    T softplus = log1pf(expf(x));
    T tanh_softplus = tanhf(softplus);
    return x * tanh_softplus;
}

__global__ void fused_bias_hardtanh_mish_kernel(
    const float* __restrict__ x,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int64_t num_elements,
    int64_t out_features
) {
    constexpr int VEC_SIZE = 4;
    int64_t base_idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;
    if (base_idx >= num_elements) return;

    #pragma unroll
    for (int i = 0; i < VEC_SIZE; ++i) {
        int64_t idx = base_idx + i;
        if (idx >= num_elements) break;
        
        int64_t feature_idx = idx % out_features;
        float val = x[idx] + bias[feature_idx];
        val = fmaxf(-1.0f, fminf(val, 1.0f));
        output[idx] = mish(val);
    }
}

torch::Tensor fused_bias_hardtanh_mish_cuda(torch::Tensor x, torch::Tensor bias) {
    auto output = torch::empty_like(x);
    int64_t num_elements = x.numel();
    int64_t out_features = x.size(1);
    
    constexpr int block_size = 256;
    constexpr int vec_size = 4;
    int grid_size = (num_elements + block_size * vec_size - 1) / (block_size * vec_size);
    
    fused_bias_hardtanh_mish_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        out_features
    );
    
    return output;
}
"""

fused_bias_act_cpp = "torch::Tensor fused_bias_hardtanh_mish_cuda(torch::Tensor x, torch::Tensor bias);"

# Optimized GroupNorm kernel with block-wide reduction
group_norm_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_fp16.h>

__global__ void group_norm_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    int64_t batch_size,
    int64_t num_features,
    int64_t num_groups,
    float eps
) {
    extern __shared__ float smem[];
    float* sum_smem = smem;
    float* sum_sq_smem = smem + blockDim.x;

    const int group_id = blockIdx.x;
    const int sample_id = group_id / num_groups;
    const int group_idx = group_id % num_groups;
    const int channels_per_group = num_features / num_groups;
    const int start_idx = sample_id * num_features + group_idx * channels_per_group;
    const int tid = threadIdx.x;

    float val = input[start_idx + tid];
    float sum = val;
    float sum_sq = val * val;

    sum_smem[tid] = sum;
    sum_sq_smem[tid] = sum_sq;
    __syncthreads();

    // Block reduction for sum and sum_sq
    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sum_smem[tid] += sum_smem[tid + stride];
            sum_sq_smem[tid] += sum_sq_smem[tid + stride];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float total_sum = sum_smem[0];
        float total_sum_sq = sum_sq_smem[0];
        float mean = total_sum / channels_per_group;
        float var = total_sum_sq / channels_per_group - mean * mean;
        float inv_std = rsqrtf(var + eps);
        
        smem[0] = mean;
        smem[1] = inv_std;
    }
    __syncthreads();

    const float mean = smem[0];
    const float inv_std = smem[1];
    output[start_idx + tid] = (val - mean) * inv_std * gamma[group_idx * channels_per_group + tid] 
                             + beta[group_idx * channels_per_group + tid];
}

torch::Tensor group_norm_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int64_t num_groups,
    float eps=1e-5
) {
    auto output = torch::empty_like(input);
    const int64_t batch_size = input.size(0);
    const int64_t num_features = input.size(1);
    const int64_t channels_per_group = num_features / num_groups;
    
    TORCH_CHECK(num_features % num_groups == 0, "num_features must be divisible by num_groups");
    TORCH_CHECK(gamma.numel() == num_features, "gamma must have num_features elements");
    TORCH_CHECK(beta.numel() == num_features, "beta must have num_features elements");
    
    const int total_groups = batch_size * num_groups;
    dim3 blocks(total_groups);
    dim3 threads(channels_per_group);
    size_t shmem_size = 2 * channels_per_group * sizeof(float);
    
    group_norm_kernel<<<blocks, threads, shmem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        batch_size,
        num_features,
        num_groups,
        eps
    );
    
    return output;
}
"""

group_norm_cpp = "torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int64_t num_groups, float eps=1e-5);"

# Load CUDA extensions
fused_bias_act = load_inline(
    name='fused_bias_act',
    cpp_sources=fused_bias_act_cpp,
    cuda_sources=fused_bias_act_source,
    functions=['fused_bias_hardtanh_mish_cuda'],
    verbose=True
)

group_norm = load_inline(
    name='group_norm',
    cpp_sources=group_norm_cpp,
    cuda_sources=group_norm_source,
    functions=['group_norm_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        self.num_groups = num_groups
        self.eps = 1e-5

    def forward(self, x):
        x = self.gemm(x)
        x = fused_bias_act.fused_bias_hardtanh_mish_cuda(x, self.bias)
        x = group_norm.group_norm_cuda(x, self.gamma, self.beta, self.num_groups, self.eps)
        return x
```
