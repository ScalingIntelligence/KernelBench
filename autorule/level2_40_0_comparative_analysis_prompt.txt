You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 29.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused scale-add kernel with vectorized memory access and optimal grid configuration
fused_scale_add_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ __launch_bounds__(256) void fused_scale_add_kernel(
    const float* __restrict__ x,
    float* __restrict__ out,
    int size,
    float combined_scale) {
    
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int elements_per_thread = 4;
    const int idx = tid * elements_per_thread;
    
    if (idx >= size) return;
    
    // Vectorized load with boundary checks
    float4 local_x;
    const int remaining = size - idx;
    if (remaining >= 4) {
        local_x = *reinterpret_cast<const float4*>(x + idx);
    } else {
        local_x = {0.0f, 0.0f, 0.0f, 0.0f};
        #pragma unroll
        for(int i = 0; i < remaining; ++i) {
            (&local_x.x)[i] = x[idx + i];
        }
    }

    // Apply combined scale using vectorized operations
    float4 result;
    result.x = local_x.x * combined_scale;
    result.y = local_x.y * combined_scale;
    result.z = local_x.z * combined_scale;
    result.w = local_x.w * combined_scale;

    // Vectorized store with boundary checks
    if (remaining >= 4) {
        *reinterpret_cast<float4*>(out + idx) = result;
    } else {
        #pragma unroll
        for(int i = 0; i < remaining; ++i) {
            out[idx + i] = (&result.x)[i];
        }
    }
}

torch::Tensor fused_scale_add_cuda(torch::Tensor x, float scaling_factor) {
    auto out = torch::empty_like(x);
    const int size = x.numel();
    const float combined_scale = scaling_factor + 1.0f;
    
    const int block_size = 256;
    const int elements_per_thread = 4;
    const int grid_size = (size + (block_size * elements_per_thread) - 1) / (block_size * elements_per_thread);
    
    fused_scale_add_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        size,
        combined_scale
    );
    
    return out;
}
"""

fused_scale_add_cpp_source = "torch::Tensor fused_scale_add_cuda(torch::Tensor x, float scaling_factor);"

fused_scale_add = load_inline(
    name="fused_scale_add",
    cpp_sources=fused_scale_add_cpp_source,
    cuda_sources=fused_scale_add_source,
    functions=["fused_scale_add_cuda"],
    verbose=True,
    extra_cuda_cflags=["-O3", "--use_fast_math", "-Xptxas=--warn-on-local-memory-usage"]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scaling_factor):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.matmul(x)
        x = fused_scale_add.fused_scale_add_cuda(x, self.scaling_factor)
        return x

batch_size = 16384
in_features = 4096
out_features = 4096
scaling_factor = 0.5

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features, scaling_factor]
```

Kernel 2 (runtime: 29.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom kernel for fused scaling and residual addition with vectorization
scale_add_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scale_add_kernel(
    float* input,
    float combined_factor,
    int num_elements
) {
    const int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);
    
    if (idx + 3 < num_elements) {
        float4 vec = reinterpret_cast<float4*>(input + idx)[0];
        vec.x *= combined_factor;
        vec.y *= combined_factor;
        vec.z *= combined_factor;
        vec.w *= combined_factor;
        reinterpret_cast<float4*>(input + idx)[0] = vec;
    }
    else {
        // Handle remaining elements
        for (int i = 0; i < 4; i++) {
            if (idx + i < num_elements) {
                input[idx + i] *= combined_factor;
            }
        }
    }
}

torch::Tensor scale_add_cuda(torch::Tensor input, float scaling_factor) {
    const float combined_factor = 1.0f + scaling_factor;
    const int num_elements = input.numel();
    
    const int block_size = 256;  // Optimal for float4 vectorization
    const int grid_size = (num_elements + 4 * block_size - 1) / (4 * block_size);
    
    scale_add_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        combined_factor,
        num_elements
    );
    
    return input;
}
"""

scale_add_cpp_source = "torch::Tensor scale_add_cuda(torch::Tensor input, float scaling_factor);"

# Compile the custom CUDA extension
scale_add = load_inline(
    name="scale_add",
    cpp_sources=scale_add_cpp_source,
    cuda_sources=scale_add_source,
    functions=["scale_add_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scaling_factor):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.scaling_factor = scaling_factor
        self.scale_add = scale_add

    def forward(self, x):
        x = self.matmul(x)
        x = self.scale_add.scale_add_cuda(x, self.scaling_factor)
        return x
```
