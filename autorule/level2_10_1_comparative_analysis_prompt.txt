You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 36.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom CUDA kernels for fused operations
custom_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void maxpool_hardtanh_kernel(const float* input, float* output, 
    int num_elements, int channels, int height, int width, 
    float min_val, float max_val) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= num_elements) return;

    int n = index / (channels * (height/2) * (width/2));
    int c = (index % (channels * (height/2) * (width/2))) / ((height/2) * (width/2));
    int hw = index % ((height/2) * (width/2));
    int h = hw / (width/2);
    int w = hw % (width/2);

    int input_h_start = h * 2;
    int input_w_start = w * 2;

    float max_val_tmp = -INFINITY;
    for (int i = 0; i < 2; ++i) {
        for (int j = 0; j < 2; ++j) {
            int input_h = input_h_start + i;
            int input_w = input_w_start + j;
            if (input_h < height && input_w < width) {
                int input_idx = n * channels * height * width + c * height * width + input_h * width + input_w;
                max_val_tmp = fmaxf(max_val_tmp, input[input_idx]);
            }
        }
    }

    // Apply Hardtanh
    max_val_tmp = fminf(fmaxf(max_val_tmp, min_val), max_val);
    output[index] = max_val_tmp;
}

torch::Tensor maxpool_hardtanh_cuda(torch::Tensor input, float min_val, float max_val) {
    auto batch_size = input.size(0);
    auto channels = input.size(1);
    auto height = input.size(2);
    auto width = input.size(3);

    int output_height = height / 2;
    int output_width = width / 2;
    int num_elements = batch_size * channels * output_height * output_width;

    auto output = torch::zeros({batch_size, channels, output_height, output_width}, input.options());

    const int block_size = 256;
    int grid_size = (num_elements + block_size - 1) / block_size;

    maxpool_hardtanh_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        channels,
        height,
        width,
        min_val,
        max_val
    );

    return output;
}

__global__ void mean_tanh_kernel(const float* input, float* output, 
    int batch_size, int channels, int height, int width) {
    extern __shared__ float sdata[];
    
    int index = blockIdx.x;
    int n = index / channels;
    int c = index % channels;
    if (n >= batch_size || c >= channels) return;

    int num_elements = height * width;
    int tid = threadIdx.x;
    int elements_per_thread = (num_elements + blockDim.x - 1) / blockDim.x;

    float sum = 0.0f;
    for (int i = 0; i < elements_per_thread; ++i) {
        int elem_idx = tid + i * blockDim.x;
        if (elem_idx < num_elements) {
            int h = elem_idx / width;
            int w = elem_idx % width;
            int input_idx = n * channels * height * width + c * height * width + h * width + w;
            sum += input[input_idx];
        }
    }

    sdata[tid] = sum;
    __syncthreads();

    // Block reduction
    for (int s = blockDim.x/2; s>0; s>>=1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float mean = sdata[0] / (height * width);
        output[n * channels + c] = tanhf(mean);
    }
}

torch::Tensor mean_tanh_cuda(torch::Tensor input) {
    auto batch_size = input.size(0);
    auto channels = input.size(1);
    auto height = input.size(2);
    auto width = input.size(3);

    auto output = torch::zeros({batch_size, channels, 1, 1}, input.options());

    int num_blocks = batch_size * channels;
    const int block_size = 256;
    size_t shared_mem_size = block_size * sizeof(float);

    mean_tanh_kernel<<<num_blocks, block_size, shared_mem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        height,
        width
    );

    return output;
}
"""

custom_ops_cpp_source = """
torch::Tensor maxpool_hardtanh_cuda(torch::Tensor input, float min_val, float max_val);
torch::Tensor mean_tanh_cuda(torch::Tensor input);
"""

# Load the custom CUDA operations
custom_ops = load_inline(
    name="custom_ops",
    cpp_sources=custom_ops_cpp_source,
    cuda_sources=custom_ops_source,
    functions=["maxpool_hardtanh_cuda", "mean_tanh_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.hardtanh_min = hardtanh_min
        self.hardtanh_max = hardtanh_max

    def forward(self, x):
        x = self.conv_transpose(x)
        x = custom_ops.maxpool_hardtanh_cuda(x, self.hardtanh_min, self.hardtanh_max)
        x = custom_ops.mean_tanh_cuda(x)
        return x

batch_size = 128
in_channels  = 64  
out_channels = 64  
height = width = 256  
kernel_size  = 3
stride = 1
padding = 1
maxpool_kernel_size = 2
maxpool_stride = 2
hardtanh_min = -1
hardtanh_max = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max]
```

Kernel 2 (runtime: 36.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused CUDA kernels for MaxPool+Hardtanh and Mean+Tanh
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_maxpool_hardtanh_kernel(
    const float* input,
    float* output,
    float min_val,
    float max_val,
    int N, int C, int H, int W,
    int out_H, int out_W) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N * C * out_H * out_W) return;

    int n = idx / (C * out_H * out_W);
    int c = (idx / (out_H * out_W)) % C;
    int out_i = (idx / out_W) % out_H;
    int out_j = idx % out_W;

    int in_start_i = out_i * 2;
    int in_start_j = out_j * 2;

    float current_max = -INFINITY;
    for(int di = 0; di < 2; ++di) {
        for(int dj = 0; dj < 2; ++dj) {
            int in_i = in_start_i + di;
            int in_j = in_start_j + dj;
            if(in_i < H && in_j < W) {
                float val = input[((n * C + c) * H + in_i) * W + in_j];
                current_max = fmaxf(current_max, val);
            }
        }
    }

    output[idx] = fminf(fmaxf(current_max, min_val), max_val);
}

__global__ void fused_mean_tanh_kernel(
    const float* input,
    float* output,
    int H, int W,
    int N, int C) {

    extern __shared__ float shared[];

    int n = blockIdx.x;
    int c = blockIdx.y;
    int tid = threadIdx.x;
    int elements = H * W;

    float sum = 0.0f;
    for(int i = tid; i < elements; i += blockDim.x) {
        int h = i / W;
        int w = i % W;
        sum += input[((n * C + c) * H + h) * W + w];
    }

    shared[tid] = sum;
    __syncthreads();

    for(int s = blockDim.x/2; s > 0; s >>= 1) {
        if(tid < s) shared[tid] += shared[tid + s];
        __syncthreads();
    }

    if(tid == 0) {
        float mean = shared[0] / (H * W);
        output[n * C + c] = tanhf(mean);
    }
}

torch::Tensor fused_maxpool_hardtanh_cuda(torch::Tensor input, float min_val, float max_val) {
    int N = input.size(0), C = input.size(1), H = input.size(2), W = input.size(3);
    int out_H = H / 2, out_W = W / 2;
    auto output = torch::zeros({N, C, out_H, out_W}, input.options());

    int total_elements = N * C * out_H * out_W;
    int block_size = 256;
    dim3 grid((total_elements + block_size - 1) / block_size);

    fused_maxpool_hardtanh_kernel<<<grid, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        min_val, max_val,
        N, C, H, W,
        out_H, out_W
    );

    return output;
}

torch::Tensor fused_mean_tanh_cuda(torch::Tensor input) {
    int N = input.size(0), C = input.size(1), H = input.size(2), W = input.size(3);
    auto output = torch::zeros({N, C, 1, 1}, input.options());

    dim3 grid(N, C);
    int block_size = 256;
    size_t shared_size = block_size * sizeof(float);

    fused_mean_tanh_kernel<<<grid, block_size, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        H, W, N, C
    );

    return output;
}
"""

fused_ops_cpp = """
torch::Tensor fused_maxpool_hardtanh_cuda(torch::Tensor input, float min_val, float max_val);
torch::Tensor fused_mean_tanh_cuda(torch::Tensor input);
"""

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=["fused_maxpool_hardtanh_cuda", "fused_mean_tanh_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=["-lcudart"]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)
        self.hardtanh_min = hardtanh_min
        self.hardtanh_max = hardtanh_max
        self.fused_mp_ht = fused_ops.fused_maxpool_hardtanh_cuda
        self.fused_mean_tanh = fused_ops.fused_mean_tanh_cuda

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.fused_mp_ht(x, self.hardtanh_min, self.hardtanh_max)
        x = self.fused_mean_tanh(x)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max]
```
