You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized fused kernel with shared memory and read-only cache
fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_ops_kernel(const float* __restrict__ conv_out,
                                float* __restrict__ output,
                                const float* __restrict__ bias,
                                float constant_val,
                                float scaling_factor,
                                int64_t batch_size,
                                int64_t channels,
                                int64_t height,
                                int64_t width) {
    extern __shared__ float s_bias[];
    
    const int tid = threadIdx.x;
    const int num_threads = blockDim.x;
    
    // Coalesced loading of bias into shared memory using read-only cache
    for (int i = tid; i < channels; i += num_threads) {
        s_bias[i] = __ldg(&bias[i]);
    }
    __syncthreads();

    const int64_t idx = blockIdx.x * blockDim.x + tid;
    const int64_t total_elements = batch_size * channels * height * width;
    
    if (idx >= total_elements) return;
    
    // Calculate channel index using optimized integer arithmetic
    const int64_t elements_per_channel = height * width;
    const int64_t channel_idx = (idx / elements_per_channel) % channels;
    
    // Fused operations with read-only cache for input
    const float val = fminf(__ldg(&conv_out[idx]), constant_val);
    output[idx] = (val + s_bias[channel_idx]) * scaling_factor;
}

torch::Tensor fused_ops_cuda(torch::Tensor conv_out, torch::Tensor bias, 
                            float constant_val, float scaling_factor) {
    auto output = torch::empty_like(conv_out);
    const int64_t total_elements = conv_out.numel();
    
    const int64_t batch_size = conv_out.size(0);
    const int64_t channels = conv_out.size(1);
    const int64_t height = conv_out.size(2);
    const int64_t width = conv_out.size(3);

    // Optimal block size for memory-bound kernels
    const int block_size = 256;
    const int grid_size = (total_elements + block_size - 1) / block_size;
    const size_t shared_mem_size = channels * sizeof(float);
    
    fused_ops_kernel<<<grid_size, block_size, shared_mem_size>>>(
        conv_out.data_ptr<float>(),
        output.data_ptr<float>(),
        bias.data_ptr<float>(),
        constant_val,
        scaling_factor,
        batch_size,
        channels,
        height,
        width
    );
    
    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_ops_cuda(torch::Tensor conv_out, torch::Tensor bias, float constant_val, float scaling_factor);"

# Load the optimized CUDA extension with compiler optimizations
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_ops_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.constant_value = constant_value
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_ops_cuda(x, self.bias, self.constant_value, self.scaling_factor)
        return x
```

Kernel 2 (runtime: 12.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_min_add_scale_kernel(
    const float* input,
    float* output,
    float constant,
    const float* bias,
    float scaling_factor,
    int N, int C, int H, int W
) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N * C * H * W) return;

    // Compute channel index
    const int c = (idx / (H * W)) % C;

    // Load input and bias values
    const float val = input[idx];
    const float bias_val = bias[c];

    // Apply operations
    float result = fminf(val, constant);
    result += bias_val;
    result *= scaling_factor;

    output[idx] = result;
}

torch::Tensor fused_elementwise_ops_cuda(
    torch::Tensor input,
    float constant,
    torch::Tensor bias,
    float scaling_factor
) {
    TORCH_CHECK(input.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(bias.sizes() == torch::IntArrayRef({input.size(1), 1, 1}), "Bias shape must be (C,1,1)");
    
    auto output = torch::empty_like(input);
    const int numel = input.numel();

    const int threads_per_block = 256;
    const int blocks_per_grid = (numel + threads_per_block - 1) / threads_per_block;

    fused_min_add_scale_kernel<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        constant,
        bias.data_ptr<float>(),
        scaling_factor,
        input.size(0),
        input.size(1),
        input.size(2),
        input.size(3)
    );

    return output;
}
"""

fused_ops_cpp = """
torch::Tensor fused_elementwise_ops_cuda(
    torch::Tensor input,
    float constant,
    torch::Tensor bias,
    float scaling_factor
);
"""

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_elementwise_ops_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.constant_value = constant_value
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_elementwise_ops_cuda(
            x,
            self.constant_value,
            self.bias,
            self.scaling_factor
        )
        return x
```
