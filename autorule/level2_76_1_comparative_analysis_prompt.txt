You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.03 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_bias_relu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_bias_relu_kernel(float* x, const float* bias, int batch_size, int features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * features) {
        int i = idx / features; // row index (batch)
        int j = idx % features; // column index (feature)
        float val = x[idx] + bias[j];
        x[idx] = fmaxf(val, 0.0f);
    }
}

torch::Tensor fused_bias_relu_cuda(torch::Tensor x, torch::Tensor bias) {
    int batch_size = x.size(0);
    int features = x.size(1);
    int size = x.numel();

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    fused_bias_relu_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        batch_size,
        features
    );

    return x;
}
"""

fused_bias_relu_header = "torch::Tensor fused_bias_relu_cuda(torch::Tensor x, torch::Tensor bias);"

# Compile the inline CUDA code
fused_bias_relu = load_inline(
    name='fused_bias_relu',
    cpp_sources=fused_bias_relu_header,
    cuda_sources=fused_bias_relu_source,
    functions=['fused_bias_relu_cuda'],
    verbose=True,
    extra_cflags=[],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias_shape):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=False)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_op = fused_bias_relu

    def forward(self, x):
        x = self.gemm(x)
        x = self.fused_op.fused_bias_relu_cuda(x, self.bias)
        return x
```

Kernel 2 (runtime: 6.93 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_add_relu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_relu_kernel(const float* x_gemm, const float* bias, float* out, int batch_size, int out_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_features) return;
    
    int i = idx / out_features;
    int j = idx % out_features;
    
    float val = x_gemm[i * out_features + j] + bias[j];
    out[idx] = fmaxf(val, 0.0f);
}

torch::Tensor fused_add_relu_cuda(torch::Tensor x_gemm, torch::Tensor bias) {
    TORCH_CHECK(x_gemm.dim() == 2 && bias.dim() == 1, "Input tensors must be 2D");
    int batch_size = x_gemm.size(0);
    int out_features = x_gemm.size(1);
    TORCH_CHECK(bias.size(0) == out_features, "Bias dimension mismatch");

    auto out = torch::empty_like(x_gemm);
    int total_elements = batch_size * out_features;
    
    const int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    fused_add_relu_kernel<<<grid_size, block_size>>>(
        x_gemm.data_ptr<float>(),
        bias.data_ptr<float>(),
        out.data_ptr<float>(),
        batch_size,
        out_features
    );
    
    return out;
}
"""

fused_add_relu_cpp = "torch::Tensor fused_add_relu_cuda(torch::Tensor x_gemm, torch::Tensor bias);"

fused_add_relu = load_inline(
    name='fused_add_relu',
    cpp_sources=fused_add_relu_cpp,
    cuda_sources=fused_add_relu_source,
    functions=['fused_add_relu_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias_shape):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=False)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_add_relu = fused_add_relu

    def forward(self, x):
        x = self.gemm(x)
        x = self.fused_add_relu.fused_add_relu_cuda(x, self.bias)
        return x
```
