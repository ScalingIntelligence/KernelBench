You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 17.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom Swish+Bias fused kernel
swish_bias_cpp = """
torch::Tensor swish_bias_cuda(torch::Tensor input, torch::Tensor bias);
"""

swish_bias_cuda = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void swish_bias_kernel(const float* input, const float* bias, float* output, int num_elements, int out_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        int j = idx % out_features;
        float x = input[idx];
        float s = 1.0f / (1.0f + expf(-x));
        output[idx] = x * s + bias[j];
    }
}

torch::Tensor swish_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    auto num_elements = input.numel();
    int out_features = input.size(1);
    auto output = torch::empty_like(input);

    const int block_size = 256;
    int grid_size = (num_elements + block_size - 1) / block_size;

    swish_bias_kernel<<<grid_size, block_size>>>(input.data_ptr<float>(), bias.data_ptr<float>(), output.data_ptr<float>(), num_elements, out_features);

    return output;
}
"""

# Custom GroupNorm kernel
group_norm_cpp = """
torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int num_groups, float eps=1e-5);
"""

group_norm_cuda = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void group_norm_kernel(const float* input, const float* gamma, const float* beta, float* output, 
                                  int batch_size, int num_groups, int group_size, float eps) {
    int sample_idx = blockIdx.x;
    int group_idx = blockIdx.y;
    
    if (sample_idx >= batch_size || group_idx >= num_groups) return;
    
    extern __shared__ float shared_data[];
    float* sum = shared_data;
    float* sum_sq = (float*)&sum[group_size];
    
    int feature_start = group_idx * group_size;
    int input_offset = sample_idx * (num_groups * group_size) + feature_start;
    
    int tid = threadIdx.x;
    float val = (tid < group_size) ? input[input_offset + tid] : 0.0f;
    
    sum[tid] = val;
    sum_sq[tid] = val * val;
    __syncthreads();
    
    // Parallel reduction
    for (int s = blockDim.x/2; s>0; s>>=1) {
        if (tid < s) {
            sum[tid] += sum[tid + s];
            sum_sq[tid] += sum_sq[tid + s];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        float mean = sum[0] / group_size;
        float var = (sum_sq[0]/group_size) - (mean*mean);
        sum[0] = mean;
        sum[1] = 1.0f / sqrt(var + eps);
    }
    __syncthreads();
    
    if (tid < group_size) {
        float normalized = (val - sum[0]) * sum[1];
        output[input_offset + tid] = normalized * gamma[feature_start + tid] + beta[feature_start + tid];
    }
}

torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int num_groups, float eps) {
    auto sizes = input.sizes();
    int batch_size = sizes[0];
    int out_features = sizes[1];
    int group_size = out_features / num_groups;
    
    auto output = torch::empty_like(input);
    
    dim3 grid(batch_size, num_groups);
    dim3 block(group_size);
    size_t shared_size = 2 * group_size * sizeof(float);
    
    group_norm_kernel<<<grid, block, shared_size>>>(
        input.data_ptr<float>(), gamma.data_ptr<float>(), beta.data_ptr<float>(), 
        output.data_ptr<float>(), batch_size, num_groups, group_size, eps
    );
    
    return output;
}
"""

# Load extensions
swish_bias_ext = load_inline(
    name='swish_bias',
    cpp_sources=swish_bias_cpp,
    cuda_sources=swish_bias_cuda,
    functions=['swish_bias_cuda'],
    verbose=True
)

group_norm_ext = load_inline(
    name='group_norm',
    cpp_sources=group_norm_cpp,
    cuda_sources=group_norm_cuda,
    functions=['group_norm_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.num_groups = num_groups
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        self.eps = 1e-5

    def forward(self, x):
        x = self.matmul(x)
        x = swish_bias_ext.swish_bias_cuda(x, self.bias)
        x = group_norm_ext.group_norm_cuda(x, self.gamma, self.beta, self.num_groups, self.eps)
        return x
```

Kernel 2 (runtime: 17.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized SwishBias with vectorized float4 loads
swish_bias_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_fp16.h>

__global__ void swish_bias_kernel(const float* __restrict__ input, 
                                  const float* __restrict__ bias, 
                                  float* __restrict__ output, 
                                  int batch_size, int out_features) {
    const int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
    const int total = batch_size * out_features;
    if (idx >= total) return;

    const int col = idx % out_features;
    const int bias_idx = col / 4;

    float4 in4 = reinterpret_cast<const float4*>(input)[idx/4];
    float4 bias4 = reinterpret_cast<const float4*>(bias)[bias_idx];

    float4 out4;
    out4.x = in4.x * (1.0f / (1.0f + expf(-in4.x))) + bias4.x;
    out4.y = in4.y * (1.0f / (1.0f + expf(-in4.y))) + bias4.y;
    out4.z = in4.z * (1.0f / (1.0f + expf(-in4.z))) + bias4.z;
    out4.w = in4.w * (1.0f / (1.0f + expf(-in4.w))) + bias4.w;

    reinterpret_cast<float4*>(output)[idx/4] = out4;
}

torch::Tensor swish_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    auto batch_size = input.size(0);
    auto out_features = input.size(1);
    auto output = torch::empty_like(input);

    int total_elements = batch_size * out_features;
    int block_size = 256;
    int num_blocks = (total_elements + 4*block_size - 1) / (4*block_size);

    swish_bias_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features
    );

    return output;
}
"""

swish_bias_cpp = "torch::Tensor swish_bias_cuda(torch::Tensor input, torch::Tensor bias);"

# Optimized GroupNorm with warp-level reductions and minimal shared memory
group_norm_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void group_norm_kernel(const float* __restrict__ input,
                                  const float* __restrict__ weight,
                                  const float* __restrict__ bias,
                                  float* __restrict__ output,
                                  int batch_size, int out_features,
                                  int num_groups, float eps) {
    const int group_size = out_features / num_groups;
    const int sample_idx = blockIdx.y;
    const int group_idx = blockIdx.x;
    const int tid = threadIdx.x;

    if (sample_idx >= batch_size || group_idx >= num_groups || tid >= group_size) return;

    const int channel = group_idx * group_size + tid;
    const int idx = sample_idx * out_features + channel;

    const float val = input[idx];
    const float val_sq = val * val;

    // Warp reduction for sum and sum_sq
    float sum = val;
    float sum_sq = val_sq;
    for (int offset = 16; offset > 0; offset >>= 1) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
        sum_sq += __shfl_down_sync(0xffffffff, sum_sq, offset);
    }

    __shared__ float warp_sums[2][2];
    if (tid % 32 == 0) {
        int warp_id = tid / 32;
        warp_sums[warp_id][0] = sum;
        warp_sums[warp_id][1] = sum_sq;
    }
    __syncthreads();

    if (tid == 0) {
        float total_sum = 0.0f, total_sum_sq = 0.0f;
        const int num_warps = (group_size + 31) / 32;
        for (int w = 0; w < num_warps; ++w) {
            total_sum += warp_sums[w][0];
            total_sum_sq += warp_sums[w][1];
        }
        const float mean = total_sum / group_size;
        const float var = total_sum_sq / group_size - mean * mean;
        const float inv_std = rsqrtf(var + eps);

        warp_sums[0][0] = mean;
        warp_sums[0][1] = inv_std;
    }
    __syncthreads();

    output[idx] = (val - warp_sums[0][0]) * warp_sums[0][1] * weight[channel] + bias[channel];
}

torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor weight, 
                             torch::Tensor bias, int num_groups, float eps) {
    auto batch_size = input.size(0);
    auto out_features = input.size(1);
    const int group_size = out_features / num_groups;

    auto output = torch::empty_like(input);
    dim3 grid(num_groups, batch_size);
    dim3 block(group_size);

    group_norm_kernel<<<grid, block, sizeof(float)*2*2>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features,
        num_groups,
        eps
    );

    return output;
}
"""

group_norm_cpp = "torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int num_groups, float eps);"

# Load extensions with fast math optimization
swish_bias_ext = load_inline(
    name='swish_bias',
    cpp_sources=swish_bias_cpp,
    cuda_sources=swish_bias_source,
    functions=['swish_bias_cuda'],
    extra_cuda_cflags=['-use_fast_math'],
    verbose=True
)

group_norm_ext = load_inline(
    name='group_norm',
    cpp_sources=group_norm_cpp,
    cuda_sources=group_norm_source,
    functions=['group_norm_cuda'],
    extra_cuda_cflags=['-use_fast_math'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.num_groups = num_groups
        self.group_norm_weight = nn.Parameter(torch.ones(out_features))
        self.group_norm_bias = nn.Parameter(torch.zeros(out_features))
        self.eps = 1e-5

    def forward(self, x):
        x = self.matmul(x)
        x = swish_bias_ext.swish_bias_cuda(x, self.bias)
        x = group_norm_ext.group_norm_cuda(x, self.group_norm_weight, 
                                         self.group_norm_bias, self.num_groups, self.eps)
        return x
```
