You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 14.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused GroupNorm + Scale CUDA kernel
group_norm_scale_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void group_norm_scale_kernel(
    const float* input,
    const float* gamma,
    const float* beta,
    const float* scale,
    float* output,
    int N, int C, int H, int W,
    int G,
    float eps) {

    extern __shared__ float smem[];
    float* sum_smem = smem;
    float* sum_sq_smem = smem + blockDim.x;

    const int n = blockIdx.y;
    const int g = blockIdx.x;
    const int tid = threadIdx.x;

    const int channels_per_group = C / G;
    const int group_start = g * channels_per_group;
    const int group_end = group_start + channels_per_group;

    float sum = 0.0f;
    float sum_sq = 0.0f;

    // Each thread processes multiple channels in the group
    for (int c = group_start + tid; c < group_end; c += blockDim.x) {
        for (int h = 0; h < H; ++h) {
            for (int w = 0; w < W; ++w) {
                const int idx = ((n * C + c) * H + h) * W + w;
                const float val = input[idx];
                sum += val;
                sum_sq += val * val;
            }
        }
    }

    sum_smem[tid] = sum;
    sum_sq_smem[tid] = sum_sq;
    __syncthreads();

    // Block reduction for sum and sum_sq
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sum_smem[tid] += sum_smem[tid + s];
            sum_sq_smem[tid] += sum_sq_smem[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        const float total_sum = sum_smem[0];
        const float total_sum_sq = sum_sq_smem[0];
        const int num_elements = channels_per_group * H * W;
        const float mean = total_sum / num_elements;
        const float var = (total_sum_sq / num_elements) - (mean * mean);
        const float inv_std = rsqrtf(var + eps);

        // Store mean and inv_std in shared memory
        smem[0] = mean;
        smem[1] = inv_std;
    }
    __syncthreads();

    const float mean = smem[0];
    const float inv_std = smem[1];

    // Apply normalization and scaling
    for (int c = group_start + tid; c < group_end; c += blockDim.x) {
        const float combined_gamma = gamma[c] * scale[c];
        const float combined_beta = beta[c] * scale[c];
        for (int h = 0; h < H; ++h) {
            for (int w = 0; w < W; ++w) {
                const int idx = ((n * C + c) * H + h) * W + w;
                const float val = input[idx];
                const float normalized = (val - mean) * inv_std;
                output[idx] = normalized * combined_gamma + combined_beta;
            }
        }
    }
}

torch::Tensor group_norm_scale_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor scale,
    int num_groups,
    float eps) {
    
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    const int G = num_groups;

    auto output = torch::empty_like(input);

    const int block_size = 256;
    const dim3 blocks(G, N); // Each block handles a group and sample
    const size_t smem_size = 2 * block_size * sizeof(float);

    group_norm_scale_kernel<<<blocks, block_size, smem_size>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        scale.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        G,
        eps);
        
    return output;
}
"""

# Fused MaxPool + Clamp CUDA kernel
max_pool_clamp_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_pool_clamp_kernel(
    const float* input,
    float* output,
    int N, int C, int H, int W,
    int kernel_size,
    float clamp_min,
    float clamp_max) {
    
    const int h_out = blockIdx.y * blockDim.y + threadIdx.y;
    const int w_out = blockIdx.x * blockDim.x + threadIdx.x;
    const int nc = blockIdx.z;
    const int n = nc / C;
    const int c = nc % C;
    
    const int H_out = H / kernel_size;
    const int W_out = W / kernel_size;
    
    if(n >= N || c >= C || h_out >= H_out || w_out >= W_out) return;
    
    float max_val = -INFINITY;
    const int h_start = h_out * kernel_size;
    const int w_start = w_out * kernel_size;
    const int h_end = min(h_start + kernel_size, H);
    const int w_end = min(w_start + kernel_size, W);
    
    for(int h = h_start; h < h_end; ++h) {
        for(int w = w_start; w < w_end; ++w) {
            const float val = input[((n * C + c) * H + h) * W + w];
            max_val = fmaxf(max_val, val);
        }
    }
    
    output[((n * C + c) * H_out + h_out) * W_out + w_out] = 
        fminf(fmaxf(max_val, clamp_min), clamp_max);
}

torch::Tensor max_pool_clamp_cuda(
    torch::Tensor input,
    int kernel_size,
    float clamp_min,
    float clamp_max) {
    
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    
    const int H_out = H / kernel_size;
    const int W_out = W / kernel_size;
    auto output = torch::empty({N, C, H_out, W_out}, input.options());
    
    const dim3 block(16, 16);
    const dim3 grid(
        (W_out + block.x - 1) / block.x,
        (H_out + block.y - 1) / block.y,
        N * C
    );
    
    max_pool_clamp_kernel<<<grid, block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        kernel_size,
        clamp_min,
        clamp_max);
        
    return output;
}
"""

# Compile kernels
group_norm_scale = load_inline(
    name='group_norm_scale',
    cpp_sources="torch::Tensor group_norm_scale_cuda(torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, int, float);",
    cuda_sources=group_norm_scale_source,
    functions=['group_norm_scale_cuda'],
    verbose=True
)

max_pool_clamp = load_inline(
    name='max_pool_clamp',
    cpp_sources="torch::Tensor max_pool_clamp_cuda(torch::Tensor, int, float, float);",
    cuda_sources=max_pool_clamp_source,
    functions=['max_pool_clamp_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        self.scale = nn.Parameter(torch.ones(scale_shape))
        self.num_groups = num_groups
        self.eps = 1e-5
        self.maxpool_kernel_size = maxpool_kernel_size
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        x = self.conv(x)
        x = group_norm_scale.group_norm_scale_cuda(
            x, 
            self.gamma, 
            self.beta,
            self.scale.squeeze(),
            self.num_groups,
            self.eps
        )
        x = max_pool_clamp.max_pool_clamp_cuda(
            x,
            self.maxpool_kernel_size,
            self.clamp_min,
            self.clamp_max
        )
        return x
```

Kernel 2 (runtime: 5.84 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused kernel for scale, maxpool, and clamp operations
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_scale_maxpool_clamp_kernel(
    const float* input,
    const float* scale,
    float* output,
    int N, int C, int H, int W,
    int H_out, int W_out,
    float clamp_min, float clamp_max
) {
    // Shared memory for scale parameter
    __shared__ float shared_scale;

    const int n = blockIdx.x;
    const int c = blockIdx.y;
    const int i_out = threadIdx.y;
    const int j_out = threadIdx.z;

    // Load scale value once per block
    if (threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0) {
        shared_scale = scale[c];
    }
    __syncthreads();

    const float current_scale = shared_scale;
    float max_val = -INFINITY;

    const int i_start = i_out * 4;
    const int j_start = j_out * 4;

    // Process 4x4 window
    for (int di = 0; di < 4; ++di) {
        for (int dj = 0; dj < 4; ++dj) {
            const int i_in = i_start + di;
            const int j_in = j_start + dj;
            if (i_in < H && j_in < W) {
                const int idx = n*C*H*W + c*H*W + i_in*W + j_in;
                const float val = input[idx] * current_scale;
                max_val = fmaxf(val, max_val);
            }
        }
    }

    // Apply clamp and store result
    max_val = fminf(fmaxf(max_val, clamp_min), clamp_max);
    const int out_idx = n*C*H_out*W_out + c*H_out*W_out + i_out*W_out + j_out;
    output[out_idx] = max_val;
}

torch::Tensor fused_scale_maxpool_clamp_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    int H_out,
    int W_out,
    float clamp_min,
    float clamp_max
) {
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto output = torch::empty({N, C, H_out, W_out}, input.options());

    dim3 grid(N, C);
    dim3 block(1, H_out, W_out);  // x=1, y=H_out, z=W_out

    fused_scale_maxpool_clamp_kernel<<<grid, block>>>(
        input.data_ptr<float>(),
        scale.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        H_out, W_out,
        clamp_min, clamp_max
    );

    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_scale_maxpool_clamp_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    int H_out,
    int W_out,
    float clamp_min,
    float clamp_max
);
"""

# Load the custom CUDA kernel
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_scale_maxpool_clamp_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, scale_shape, maxpool_kernel_size, clamp_min, clamp_max):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)
        self.scale = nn.Parameter(torch.ones(scale_shape))
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max
        assert maxpool_kernel_size == 4, "Kernel only supports 4x4 maxpool"

    def forward(self, x):
        x = self.conv(x)
        x = self.group_norm(x)
        
        # Calculate output dimensions
        H, W = x.shape[2], x.shape[3]
        H_out, W_out = H // 4, W // 4
        
        # Apply fused kernel
        x = fused_ops.fused_scale_maxpool_clamp_cuda(
            x.contiguous(),
            self.scale,
            H_out,
            W_out,
            self.clamp_min,
            self.clamp_max
        )
        return x
```
