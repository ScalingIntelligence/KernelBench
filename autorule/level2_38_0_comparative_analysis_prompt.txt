You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 17.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized fused kernel with shared memory caching for scales
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_clamp_softmax_scale_kernel(
    const float* input,
    const float* scales,
    float* output,
    float clamp_min,
    float clamp_max,
    int batch_size,
    int channels,
    int spatial_size
) {
    // Block handles single batch and channel
    const int batch = blockIdx.x;
    const int channel = blockIdx.y;
    
    const float* input_ptr = input + (batch * channels + channel) * spatial_size;
    float* output_ptr = output + (batch * channels + channel) * spatial_size;

    extern __shared__ float shared[];
    float& scale = shared[0];  // First element stores scale
    float* shared_max = &shared[1];
    float* shared_sum = &shared[1 + blockDim.x];

    // Load scale once per block using first thread
    if (threadIdx.x == 0) {
        scale = scales[channel];
    }
    __syncthreads();

    // Step 1: Compute max of clamped values
    float local_max = -INFINITY;
    for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
        float val = input_ptr[i];
        val = fmaxf(fminf(val, clamp_max), clamp_min);
        local_max = fmaxf(local_max, val);
    }

    shared_max[threadIdx.x] = local_max;
    __syncthreads();

    // Warp-level reduction for max
    for (int offset = blockDim.x/2; offset > 32; offset >>= 1) {
        if (threadIdx.x < offset) {
            shared_max[threadIdx.x] = fmaxf(shared_max[threadIdx.x], shared_max[threadIdx.x + offset]);
        }
        __syncthreads();
    }
    for (int offset = 32; offset > 0; offset >>= 1) {
        if (threadIdx.x < offset) {
            shared_max[threadIdx.x] = fmaxf(shared_max[threadIdx.x], shared_max[threadIdx.x + offset]);
        }
        __syncwarp();
    }
    const float global_max = shared_max[0];
    __syncthreads();

    // Step 2: Compute sum of exp(clamped - global_max)
    float local_sum = 0.0f;
    for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
        float val = input_ptr[i];
        val = fmaxf(fminf(val, clamp_max), clamp_min);
        local_sum += expf(val - global_max);
    }

    shared_sum[threadIdx.x] = local_sum;
    __syncthreads();

    // Warp-level reduction for sum
    for (int offset = blockDim.x/2; offset > 32; offset >>= 1) {
        if (threadIdx.x < offset) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + offset];
        }
        __syncthreads();
    }
    for (int offset = 32; offset > 0; offset >>= 1) {
        if (threadIdx.x < offset) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + offset];
        }
        __syncwarp();
    }
    const float global_sum = shared_sum[0];
    __syncthreads();

    // Step 3: Compute softmax and scale using cached scale
    for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
        float val = input_ptr[i];
        val = fmaxf(fminf(val, clamp_max), clamp_min);
        output_ptr[i] = (expf(val - global_max) / global_sum) * scale;
    }
}

torch::Tensor fused_clamp_softmax_scale_cuda(
    torch::Tensor input,
    torch::Tensor scales,
    float clamp_min,
    float clamp_max
) {
    TORCH_CHECK(input.dim() == 5, "Input must be a 5D tensor");
    TORCH_CHECK(scales.dim() == 5, "Scales must be a 5D tensor");
    TORCH_CHECK(scales.size(1) == input.size(1), "Scales must match input channels");

    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int spatial_size = input.size(2) * input.size(3) * input.size(4);

    auto input_contig = input.contiguous();
    auto scales_contig = scales.contiguous();
    auto output = torch::empty_like(input_contig);

    const float* input_data = input_contig.data_ptr<float>();
    const float* scales_data = scales_contig.data_ptr<float>();
    float* output_data = output.data_ptr<float>();

    dim3 grid(batch_size, channels);
    const int block_size = 256;
    const size_t shared_mem_size = (1 + 2 * block_size) * sizeof(float);  // Scale + max/sum buffers

    fused_clamp_softmax_scale_kernel<<<grid, block_size, shared_mem_size>>>(
        input_data,
        scales_data,
        output_data,
        clamp_min,
        clamp_max,
        batch_size,
        channels,
        spatial_size
    );

    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_clamp_softmax_scale_cuda(torch::Tensor input, torch::Tensor scales, float clamp_min, float clamp_max);
"""

# Load the optimized CUDA extension with fast math
fused_clamp_softmax_scale = load_inline(
    name='fused_clamp_softmax_scale',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_clamp_softmax_scale_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        self.avg_pool = nn.AvgPool3d(pool_kernel_size)
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max
        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))

    def forward(self, x):
        x = self.avg_pool(x)
        x = self.conv_transpose(x)
        x = fused_clamp_softmax_scale.fused_clamp_softmax_scale_cuda(
            x, 
            self.scale,
            self.clamp_min,
            self.clamp_max
        )
        return x
```

Kernel 2 (runtime: 17.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused clamp, spatial softmax, and scale CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_clamp_softmax_scale_kernel(
    const float* input,
    const float* scales,
    float* output,
    float clamp_min,
    float clamp_max,
    int batch_size,
    int channels,
    int spatial_size
) {
    // Block handles a single batch and channel
    int batch = blockIdx.x;
    int channel = blockIdx.y;

    const float* input_ptr = input + (batch * channels + channel) * spatial_size;
    float* output_ptr = output + (batch * channels + channel) * spatial_size;
    float scale = scales[channel];

    extern __shared__ float shared[];
    float* shared_max = shared;
    float* shared_sum = shared + blockDim.x;

    // Step 1: Compute max of clamped values
    float local_max = -INFINITY;
    for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
        float val = input_ptr[i];
        val = fmaxf(fminf(val, clamp_max), clamp_min);
        local_max = fmaxf(local_max, val);
    }

    shared_max[threadIdx.x] = local_max;
    __syncthreads();

    // Block max reduction
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_max[threadIdx.x] = fmaxf(shared_max[threadIdx.x], shared_max[threadIdx.x + stride]);
        }
        __syncthreads();
    }
    float global_max = shared_max[0];
    __syncthreads();

    // Step 2: Compute sum of exp(clamped - global_max)
    float local_sum = 0.0f;
    for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
        float val = input_ptr[i];
        val = fmaxf(fminf(val, clamp_max), clamp_min);
        local_sum += expf(val - global_max);
    }

    shared_sum[threadIdx.x] = local_sum;
    __syncthreads();

    // Block sum reduction
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
        }
        __syncthreads();
    }
    float global_sum = shared_sum[0];
    __syncthreads();

    // Step 3: Compute softmax and scale
    for (int i = threadIdx.x; i < spatial_size; i += blockDim.x) {
        float val = input_ptr[i];
        val = fmaxf(fminf(val, clamp_max), clamp_min);
        float softmax_val = expf(val - global_max) / global_sum;
        output_ptr[i] = softmax_val * scale;
    }
}

torch::Tensor fused_clamp_softmax_scale_cuda(
    torch::Tensor input,
    torch::Tensor scales,
    float clamp_min,
    float clamp_max
) {
    TORCH_CHECK(input.dim() == 5, "Input must be a 5D tensor");
    TORCH_CHECK(scales.dim() == 5, "Scales must be a 5D tensor");
    TORCH_CHECK(scales.size(1) == input.size(1), "Scales must match input channels");

    int batch_size = input.size(0);
    int channels = input.size(1);
    int spatial_size = input.size(2) * input.size(3) * input.size(4);

    auto input_contig = input.contiguous();
    auto scales_contig = scales.contiguous();
    auto output = torch::empty_like(input_contig);

    const float* input_data = input_contig.data_ptr<float>();
    const float* scales_data = scales_contig.data_ptr<float>();
    float* output_data = output.data_ptr<float>();

    dim3 grid(batch_size, channels);
    int block_size = 256;
    size_t shared_mem_size = 2 * block_size * sizeof(float);

    fused_clamp_softmax_scale_kernel<<<grid, block_size, shared_mem_size>>>(
        input_data,
        scales_data,
        output_data,
        clamp_min,
        clamp_max,
        batch_size,
        channels,
        spatial_size
    );

    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_clamp_softmax_scale_cuda(torch::Tensor input, torch::Tensor scales, float clamp_min, float clamp_max);
"""

# Load the custom CUDA extension
fused_clamp_softmax_scale = load_inline(
    name='fused_clamp_softmax_scale',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_clamp_softmax_scale_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        self.avg_pool = nn.AvgPool3d(pool_kernel_size)
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max
        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))

    def forward(self, x):
        x = self.avg_pool(x)
        x = self.conv_transpose(x)
        x = fused_clamp_softmax_scale.fused_clamp_softmax_scale_cuda(
            x, 
            self.scale,
            self.clamp_min,
            self.clamp_max
        )
        return x
```
