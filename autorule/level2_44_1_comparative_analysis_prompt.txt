You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 5.54 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename T>
__global__ void fused_mul_mean_kernel(
    const T* input,
    T* output,
    int batch_size,
    int channels,
    int H,
    int W,
    T multiplier) {

    int n = blockIdx.x; // batch index
    int c = blockIdx.y; // channel index
    
    extern __shared__ float sdata[];
    
    int tid = threadIdx.x;
    int num_threads = blockDim.x;
    
    int elements_per_thread = (H * W + num_threads - 1) / num_threads;
    float local_sum = 0.0f;
    
    for(int i = tid; i < H * W; i += num_threads) {
        int h = i / W;
        int w = i % W;
        int idx = n * channels * H * W + c * H * W + h * W + w;
        T val = input[idx] * multiplier;
        local_sum += static_cast<float>(val);
    }

    sdata[tid] = local_sum;
    __syncthreads();

    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(tid < stride) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();
    }

    if(tid == 0) {
        float total_elements = H * W;
        float mean_val = sdata[0] / total_elements;
        output[n * channels + c] = static_cast<T>(mean_val);
    }
}

torch::Tensor fused_mul_mean_cuda(
    torch::Tensor input,
    float multiplier) {

    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    int batch_size = input.size(0);
    int channels = input.size(1);
    int H = input.size(2);
    int W = input.size(3);
    auto options = input.options().device(input.device());
    auto output = torch::empty({batch_size, channels}, options);
    
    dim3 grid(batch_size, channels);
    int threads = 256;
    size_t smem = threads * sizeof(float);
    
    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "fused_mul_mean", ([&] {
        fused_mul_mean_kernel<scalar_t><<<grid, threads, smem>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size, channels, H, W,
            static_cast<scalar_t>(multiplier));
    }));
    
    return output.unsqueeze(-1).unsqueeze(-1);
}
"""

fused_ops_cpp = "torch::Tensor fused_mul_mean_cuda(torch::Tensor input, float multiplier);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_mul_mean_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, output_padding=output_padding
        )
        self.multiplier = multiplier

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_mul_mean_cuda(x, self.multiplier)
        return x
```

Kernel 2 (runtime: 5.54 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused multiply and global average pooling
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_multiply_mean_kernel(
    const float* input, 
    float* output, 
    float multiplier, 
    int B, 
    int C, 
    int H, 
    int W
) {
    int b = blockIdx.x;
    int c = blockIdx.y;

    if (b >= B || c >= C) return;

    int num_elements = H * W;
    int tid = threadIdx.x;
    int stride = blockDim.x;

    float sum = 0.0f;

    // Parallel reduction with strided access
    for (int i = tid; i < num_elements; i += stride) {
        int h = i / W;
        int w = i % W;
        sum += input[b * C * H * W + c * H * W + h * W + w];
    }

    // Shared memory reduction
    __shared__ float shared[256];
    shared[tid] = sum;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float mean = shared[0] / num_elements;
        output[b * C + c] = mean * multiplier;
    }
}

torch::Tensor fused_multiply_mean_cuda(torch::Tensor input, float multiplier) {
    auto sizes = input.sizes();
    int B = sizes[0];
    int C = sizes[1];
    int H = sizes[2];
    int W = sizes[3];

    auto output = torch::zeros({B, C, 1, 1}, torch::kFloat32).cuda();

    dim3 grid(B, C);
    int block_size = 256;
    fused_multiply_mean_kernel<<<grid, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        multiplier,
        B,
        C,
        H,
        W
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_multiply_mean_cuda(torch::Tensor input, float multiplier);"

# Load the custom CUDA kernel
fused_multiply_mean = load_inline(
    name='fused_multiply_mean',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_multiply_mean_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, multiplier):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding, 
            output_padding=output_padding
        )
        self.multiplier = multiplier
        self.fused_multiply_mean = fused_multiply_mean

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.fused_multiply_mean.fused_multiply_mean_cuda(x, self.multiplier)
        return x
```
