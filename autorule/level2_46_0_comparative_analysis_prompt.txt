You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 14.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <math.h>

// Fused operations: subtract1, tanh, subtract2
__global__ void fused_operations(
    const float* input, float* output,
    float s1, float s2,
    int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = input[idx];
        val -= s1;
        val = tanhf(val);
        val -= s2;
        output[idx] = val;
    }
}

torch::Tensor fused_operations_cuda(
    torch::Tensor input,
    float s1,
    float s2) {
    auto output = torch::empty_like(input);
    int numel = input.numel();
    const int block_size = 256;
    int grid_size = (numel + block_size - 1) / block_size;
    fused_operations<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        s1,
        s2,
        numel);
    return output;
}

// Custom average pooling kernel for 2x2 kernel
__global__ void avg_pool_2x2(
    const float* input,
    float* output,
    int batch_size,
    int channels,
    int input_height,
    int input_width,
    int output_height,
    int output_width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * output_height * output_width)
        return;
    
    int w_out = idx % output_width;
    int h_out = (idx / output_width) % output_height;
    int c = (idx / (output_width * output_height)) % channels;
    int n = idx / (channels * output_width * output_height);
    
    int h_start = h_out * 2;
    int w_start = w_out * 2;
    
    float sum = 0.0f;
    for(int i=0; i<2; ++i) {
        for(int j=0; j<2; ++j) {
            int h = h_start + i;
            int w = w_start + j;
            if(h < input_height && w < input_width) {
                int input_idx = n * channels * input_height * input_width +
                                c * input_height * input_width +
                                h * input_width + w;
                sum += input[input_idx];
            }
        }
    }
    output[idx] = sum / 4.0f;
}

torch::Tensor avg_pool_2x2_cuda(
    torch::Tensor input) {
    int batch_size = input.size(0);
    int channels = input.size(1);
    int input_h = input.size(2);
    int input_w = input.size(3);
    int output_h = input_h / 2;
    int output_w = input_w / 2;
    auto output = torch::empty({batch_size, channels, output_h, output_w}, input.options());
    
    int numel = output.numel();
    const int block_size = 256;
    int grid_size = (numel + block_size - 1) / block_size;
    
    avg_pool_2x2<<<grid_size, block_size>>>(
        input.contiguous().data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        input_h,
        input_w,
        output_h,
        output_w);
    
    return output;
}
"""

fused_ops_cpp = """
torch::Tensor fused_operations_cuda(torch::Tensor input, float s1, float s2);
torch::Tensor avg_pool_2x2_cuda(torch::Tensor input);
"""

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_operations_cuda', 'avg_pool_2x2_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract1_value = subtract1_value
        self.subtract2_value = subtract2_value
        
    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_operations_cuda(x, self.subtract1_value, self.subtract2_value)
        x = fused_ops.avg_pool_2x2_cuda(x)
        return x
```

Kernel 2 (runtime: 11.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused kernel with vectorized loads and optimized block configuration
fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_sub_tanh_sub_pool_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int N, int C, int H, int W,
    int pool_kernel_size,
    float sub1, float sub2
) {
    const int h_out = blockIdx.y * blockDim.y + threadIdx.y;
    const int w_out = blockIdx.x * blockDim.x + threadIdx.x;
    const int c = blockIdx.z % C;
    const int n = blockIdx.z / C;

    const int H_out = H / pool_kernel_size;
    const int W_out = W / pool_kernel_size;
    
    if (n >= N || c >= C || h_out >= H_out || w_out >= W_out) return;

    const int h_start = h_out * pool_kernel_size;
    const int w_start = w_out * pool_kernel_size;
    
    float sum = 0.0f;
    int count = 0;

    #pragma unroll
    for (int h = 0; h < pool_kernel_size; ++h) {
        const int input_h = h_start + h;
        if (input_h >= H) continue;
        
        const int base_idx = ((n * C + c) * H + input_h) * W + w_start;
        if (w_start + 1 < W) {
            // Vectorized load for 2 consecutive elements
            const float2 vals = *reinterpret_cast<const float2*>(&input[base_idx]);
            float val0 = vals.x - sub1;
            val0 = tanhf(val0);
            sum += val0 - sub2;
            count++;
            
            float val1 = vals.y - sub1;
            val1 = tanhf(val1);
            sum += val1 - sub2;
            count++;
        } else {
            // Handle edge case with single element
            const float val = input[base_idx] - sub1;
            sum += tanhf(val) - sub2;
            count++;
        }
    }
    
    const int out_idx = ((n * C + c) * H_out + h_out) * W_out + w_out;
    output[out_idx] = count > 0 ? sum / count : 0.0f;
}

torch::Tensor fused_sub_tanh_sub_pool_cuda(
    torch::Tensor input,
    float sub1,
    float sub2,
    int pool_kernel_size
) {
    const auto N = input.size(0);
    const auto C = input.size(1);
    const auto H = input.size(2);
    const auto W = input.size(3);
    
    const int H_out = H / pool_kernel_size;
    const int W_out = W / pool_kernel_size;
    auto output = torch::empty({N, C, H_out, W_out}, input.options());

    // Optimized block configuration for coalesced memory access
    const dim3 block(32, 8);  // 256 threads per block
    const dim3 grid(
        (W_out + block.x - 1) / block.x,
        (H_out + block.y - 1) / block.y,
        N * C  // Combine batch and channel dimensions
    );

    fused_sub_tanh_sub_pool_kernel<<<grid, block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W,
        pool_kernel_size,
        sub1, sub2
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_sub_tanh_sub_pool_cuda(torch::Tensor input, float sub1, float sub2, int pool_kernel_size);"

# Load custom operations with optimized CUDA configuration
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_sub_tanh_sub_pool_cuda'],
    verbose=True,
    extra_cuda_cflags=['--use_fast_math', '-O3', '-Xptxas=-v'],
    extra_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, 
                 subtract1_value, subtract2_value, kernel_size_pool):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract1_value = subtract1_value
        self.subtract2_value = subtract2_value
        self.kernel_size_pool = kernel_size_pool
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_ops.fused_sub_tanh_sub_pool_cuda(
            x, 
            self.subtract1_value,
            self.subtract2_value,
            self.kernel_size_pool
        )
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool]
```
