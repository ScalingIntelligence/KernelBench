{
  "response": "\n\nThe performance difference between the two kernels stems from their memory access patterns and reduction strategies:\n\n1. **Memory Access Patterns**:\n   - **Kernel 1** uses a strided access pattern (threads access elements with a stride of `blockDim.x`), leading to **non-coalesced memory accesses**. This is generally inefficient but may benefit from cache hits if data is reused.\n   - **Kernel 2** employs **vectorized loads (`float4`)** for contiguous memory access, improving memory throughput. However, it introduces branching overhead for handling unaligned data (when `row_size` isn't a multiple of 4).\n\n2. **Reduction Strategy**:\n   - **Kernel 1** uses a **block-wide shared memory reduction** with `log2(256) = 8` steps. While this requires multiple `__syncthreads()`, it avoids warp divergence.\n   - **Kernel 2** uses **warp-level reductions** via `__shfl_down_sync`, which are faster than shared memory operations. However, it requires two stages of warp reductions and additional shared memory writes, increasing register pressure.\n\n3. **Register Usage**:\n   - **Kernel 2**'s use of vectorization and two-stage reductions likely increases **register pressure**, reducing GPU occupancy. Lower occupancy limits parallelism, offsetting gains from efficient memory access.\n\n4. **Instruction Overhead**:\n   - **Kernel 2** has more complex logic (vectorization, branching, warp reductions), leading to higher instruction overhead. This can stall pipelines and reduce throughput.\n\n**Conclusion**: Kernel 1's simpler reduction and lower register usage allow better occupancy, outweighing its suboptimal memory access. Kernel 2's vectorization improves memory efficiency, but higher register pressure and instruction overhead negate these benefits, making it slightly slower.",
  "reasoning_trace": "Okay, let's take a look at these two CUDA kernels. The user says both are correct but Kernel 1 is faster than Kernel 2. I need to figure out why. \n\nFirst, I'll start by understanding what each kernel does. Both seem to perform a fused operation that includes finding the max value in each row, subtracting the mean (though in Kernel 1, the mean is the same as the max, which might be a mistake?), and applying a GELU activation. Wait, in Kernel 1, after computing row_max, they set mean = row_max, so diff becomes zero. That seems odd. But maybe that's a typo in the code? Because if diff is zero, the GELU would just be 0.5 * 0 * ... which is zero. But the user says both kernels are correct, so maybe there's a mistake here. Alternatively, maybe the mean subtraction is supposed to be something else, like the actual mean of the row. Hmm, maybe that's a bug in Kernel 1, but the user says both are correct. Let's set that aside for now and focus on performance differences.\n\nLooking at Kernel 1: It uses a block of 256 threads. Each thread processes elements in the row with a stride of blockDim.x. Then, they perform a parallel max reduction using shared memory. The shared memory is of size threads (256) floats. The reduction is done in stages, halving the active threads each time. Once the max is found, only thread 0 computes the GELU and writes the output. \n\nKernel 2: Uses vectorized loads (float4) to process 4 elements per thread. Then does a warp-level reduction using __shfl_down_sync. Then stores per-warp maxes into shared memory, followed by another warp-level reduction. Finally, writes zero to the output. Wait, in the code, after the max reduction, they set output[row_idx] = 0.0f. That's strange. But the user says both are correct. Maybe that's a placeholder or a mistake. But again, assuming correctness, perhaps the actual GELU computation is missing here. Wait, comparing the two kernels: in Kernel 1, after finding the max, they compute GELU on (row_max - mean), which is zero, leading to zero. But in Kernel 2, they directly write zero. So maybe the actual intended operation is to output zero, but that's probably not the case. There's a discrepancy here. But the user says both are correct, so perhaps there's a mistake in the code provided. Maybe the user intended for the GELU part but there's a code error. However, since the user states both are correct, perhaps the actual problem is different. Let's proceed under the assumption that the code is correct as per the user's statement.\n\nBut focusing on performance: Kernel 1 runs in 6.98 ms, Kernel 2 in 7.1 ms. So why is Kernel 1 faster?\n\nLooking at the max reduction approach. Kernel 1 uses a block-wide reduction with shared memory. Each thread processes multiple elements (since O could be larger than the number of threads). Then, the shared memory is used for the reduction. The first part is loading and finding the max in parallel with a loop over the elements. Then, a standard tree reduction in shared memory.\n\nKernel 2 uses vectorized loads (float4) to process 4 elements per thread. This should increase memory throughput. Then, they do a warp-level reduction using shuffle instructions, which is more efficient than shared memory. Then, per-warp maxes are stored in shared memory, and another warp reduction is done. \n\nSo why is Kernel 1 faster? Let's think about possible factors.\n\n1. **Vectorization in Kernel 2**: Using float4 should help with memory bandwidth. However, if the row size isn't a multiple of 4, there's some handling for the remaining elements, which could add overhead. But in general, vectorization should help. However, in Kernel 1, each thread processes elements with a stride of blockDim.x, which could lead to uncoalesced memory accesses. Wait, in Kernel 1, each thread accesses input[row * O + i], where i starts at tid and increments by blockDim.x. So for a row, the threads access elements 0, 256, 512, etc., which are spaced apart. This leads to non-coalesced memory accesses, which is bad. But in Kernel 2, they use vectorized loads (float4), which would read consecutive elements. So Kernel 2's memory access pattern is better, which should make it faster. But according to the runtimes, Kernel 1 is faster. So this seems contradictory. Maybe the problem is elsewhere.\n\n2. **Reduction Efficiency**: Kernel 1 uses a block-wide reduction with shared memory. The first step is each thread computes a max over their assigned elements, then they do a shared memory reduction. The shared memory reduction is O(log n) steps. For 256 threads, that's 8 steps (since 2^8=256). But in Kernel 2, after the vectorized load, they do a warp-level reduction (using shuffle instructions) which is more efficient. Then, store the per-warp max into shared memory, and do another warp reduction. Since the block size is 256, there are 8 warps (256/32=8). So the shared memory in Kernel 2 is (256+31)/32 = 8 warps, so 8 elements. Then, the second reduction is done with 32 threads (if tid <32), but only the first 8 elements are valid. Then, another warp reduction. So the total steps for reduction in Kernel 2 are: per-warp reduction (log2(32) =5 steps?), then a shared memory step, then another warp reduction. So the total steps for reduction might be less than Kernel 1's 8 steps. So why is Kernel 1 faster here?\n\n3. **Shared Memory Usage**: Kernel 1 uses 256 * sizeof(float) shared memory, which is 1KB per block. Kernel 2 uses ((256 +31)/32) * sizeof(float) = (8) *4=32 bytes per block. So Kernel 2 uses much less shared memory, which allows more blocks to be active concurrently on the GPU. However, if the kernel is launched with B blocks (where B is the batch size), each block is processing a different row. So if the number of rows is large, the occupancy might be limited by the number of blocks that can be resident. But if Kernel 1 uses more shared memory per block, that could limit occupancy. However, in this case, since each block is handling a separate row, and the grid is B blocks (for B rows), the number of blocks that can run concurrently is limited by the GPU's resources. If Kernel 1 uses more shared memory, that might reduce the number of blocks that can be active at the same time, leading to lower occupancy. But lower occupancy could lead to lower performance. However, Kernel 1 is faster, so maybe the reduction in shared memory isn't the bottleneck here.\n\n4. **Divergence and Synchronization**: In Kernel 1, during the reduction phase, threads are active in a tree pattern. For example, in each step, half the threads are active. This can lead to some divergence, but since the code uses __syncthreads() properly, it's handled. In Kernel 2, the warp-level reductions use shuffle instructions, which are more efficient as they don't require shared memory and have lower latency. So the reduction in Kernel 2 should be more efficient. But again, the runtime is worse. Hmm.\n\n5. **Final Computation**: In Kernel 1, after the max is found, thread 0 computes the GELU. In Kernel 2, after the max reduction, they write zero. Wait, that's a big difference. If Kernel 2 is writing zero instead of computing GELU, that's a major difference. But the user says both are correct. So perhaps there's a mistake in the code provided. Let me check again.\n\nLooking at Kernel 1's code: After finding row_max, they compute mean as row_max, so diff is zero. Then GELU is 0.5 * 0 * ... which is zero. So the output is zero. But in Kernel 2, they directly write zero. So both kernels are effectively outputting zero for each row. That's probably not the intended behavior, but according to the user, both are correct. So maybe the actual problem is different, but the user provided code has a mistake. However, given that the user states both are correct, perhaps the actual GELU computation is not part of the problem, and the key difference is in the max reduction and other steps.\n\nBut given that, the difference in runtime could be due to the fact that Kernel 1's code is doing a redundant computation (calculating GELU of zero) but the actual time is dominated by the max reduction. So the key is to compare the max reduction implementations.\n\nAnother point: Kernel 2 uses vectorized loads (float4) which can improve memory throughput. However, in the code, after loading the float4, they extract each component and compute the max. This is efficient because it reduces the number of memory transactions. For example, loading 4 elements with a single memory access. This should help Kernel 2. But if the row size is not a multiple of 4, there's some overhead in handling the remaining elements. But if the row size is large, this overhead is minimal. So why is Kernel 1 faster?\n\nWait, maybe the problem is in the loop structure. In Kernel 1, each thread processes elements with a stride of blockDim.x. So for a row of size O, each thread processes O / blockDim.x elements. In Kernel 2, each thread processes 4 elements per iteration, and the loop increments by blockDim.x *4. So the total number of iterations is O/(blockDim.x *4). For example, if O is 4096, blockDim.x is 256, then in Kernel 1 each thread processes 16 elements (4096 /256). In Kernel 2, each thread processes 4 elements per iteration, so 4096/(256*4) = 4 iterations, so 4*4=16 elements per thread. So same number of elements per thread. But Kernel 2 uses vectorized loads, which are more efficient. So why is Kernel 1 faster?\n\nAlternatively, maybe the problem is in the reduction steps. Let's compare the reduction phases.\n\nIn Kernel 1:\n\n- After loading, each thread has a local max. These are stored in shared memory (size 256). Then, a tree reduction in shared memory: 8 steps (since 256 is 2^8). Each step halves the active threads. So for 256 threads, first step uses 128 threads, then 64, etc. Each step requires a __syncthreads(). \n\nIn Kernel 2:\n\n- Each thread computes a local max over 4 elements (vectorized). Then, a warp-level reduction using shuffle instructions. For each warp (32 threads), the max is reduced within the warp. Then, the per-warp max is stored in shared memory (8 elements for 256 threads). Then, another warp reduction on those 8 elements. \n\nThe warp-level reductions are more efficient because they use shuffle instructions, which are faster than shared memory accesses. Also, the shared memory reduction in Kernel 1 requires multiple __syncthreads() calls, which can be costly. \n\nSo why is Kernel 1 faster? Maybe because the initial max computation in Kernel 1 is more efficient, despite the uncoalesced memory access. Or perhaps the problem is in the way the threads are scheduled.\n\nWait, in Kernel 1, the code for loading is:\n\nfor(int i = tid; i < O; i += blockDim.x) {\n    float val = input[row * O + i];\n    max_val = fmaxf(max_val, val);\n}\n\nThis is a strided access pattern. For example, thread 0 accesses 0, 256, 512, etc. Thread 1 accesses 1, 257, etc. This leads to non-coalesced global memory accesses, which are bad for performance. Each memory transaction by the GPU is 128 bytes (32 elements of 4 bytes each). If threads in a warp are accessing non-consecutive addresses, the memory transactions are not coalesced, leading to more transactions and lower memory throughput.\n\nIn contrast, Kernel 2 uses vectorized loads (float4) which read 4 consecutive elements. So each thread reads 4 elements in a contiguous block. This allows for coalesced memory access. For example, thread 0 reads elements 0-3, thread 1 reads 4-7, etc. This way, each memory transaction is fully utilized, leading to better memory efficiency. \n\nSo why is Kernel 1, which has uncoalesced memory access, faster than Kernel 2, which has coalesced access? That seems counterintuitive. There must be another factor.\n\nAh, perhaps the problem is in the way the max is computed. Let's look at the loop in Kernel 2:\n\ni = tid *4;\nwhile (i < row_size) {\n    if (i +3 < row_size) {\n        float4 vec = ...;\n        update max with vec.x, .y, .z, .w\n    } else {\n        handle remaining elements\n    }\n    i += blockDim.x *4;\n}\n\nSo each thread processes 4 elements per iteration, and the next iteration is blockDim.x *4 elements away. For example, with blockDim.x=256, each iteration processes 4 elements, then jumps 256*4=1024 elements. So for a row of size 4096, each thread would process 4 elements in each of 4 iterations (4*4=16 elements total). But the access pattern here is: thread 0 processes 0-3, 1024-1027, 2048-2051, 3072-3075. Thread 1 processes 4-7, 1028-1031, etc. This is a strided access pattern as well, but with a stride of 1024 elements between iterations. However, within each iteration, the 4 elements are contiguous, so the float4 load is coalesced. \n\nWait, but the initial access for each thread is tid*4. So for thread 0, 0-3; thread 1, 4-7; up to thread 255, 1020-1023. Then the next iteration adds blockDim.x *4 = 256*4=1024. So the next elements for thread 0 are 1024-1027, etc. So the first 256 threads process the first 1024 elements (256 threads *4 elements each), then the next 1024 elements, etc. So the memory access for each iteration is contiguous and coalesced. Because each thread in a warp (32 threads) accesses consecutive 4-element chunks. For example, warp 0 (threads 0-31) would access 0-3, 4-7, ..., 124-127, then 1024-1027, etc. So each 128-byte transaction (32 elements) would be fully utilized. So the memory access in Kernel 2 should be more efficient than Kernel 1's strided access. \n\nBut according to the runtimes, Kernel 1 is faster. So there's a contradiction here. Maybe the problem is in the reduction steps. Let's compare the reduction time.\n\nIn Kernel 1, after loading, each thread has a max, stored in shared memory. Then, a tree reduction over 256 elements takes 8 steps (log2(256) =8). Each step requires a __syncthreads(). \n\nIn Kernel 2, after loading, each thread does a warp-level reduction (5 steps for 32 elements), then stores the per-warp max into shared memory. Then, another warp reduction on the 8 elements. So the total steps for reduction are 5 (first warp reduction) + 5 (second warp reduction) =10 steps, but these are done with shuffle instructions, which are faster than shared memory accesses. Also, no __syncthreads() needed between warp reductions except after storing to shared memory. \n\nSo the reduction in Kernel 2 should be faster. But the overall runtime is slower. Hmm.\n\nAnother possibility is the amount of work per thread. In Kernel 1, each thread processes O / blockDim.x elements. In Kernel 2, each thread processes O / (blockDim.x *4) *4 = O / blockDim.x elements. So same number of elements per thread. But Kernel 2 uses vectorized loads, which are more efficient. So why is Kernel 1 faster?\n\nWait, maybe the problem is in the final steps. In Kernel 1, after the reduction, only thread 0 computes the GELU (which in this case is zero) and writes to output. In Kernel 2, after the reduction, they write zero. So both are doing the same thing. But in Kernel 2, the code for the final output is:\n\nif (tid == 0) {\n    output[row_idx] = 0.0f;  // Direct zero write after max reduction\n}\n\nBut in Kernel 1, the code is:\n\nif (tid == 0) {\n    float row_max = shared_max[0];\n    float mean = row_max;\n    float diff = row_max - mean;\n    // GELU approximation for efficiency\n    float gelu = 0.5f * diff * (1.0f + tanhf(0.7978845608f * (diff + 0.044715f * diff * diff * diff)));\n    output[row] = gelu;\n}\n\nBut since diff is zero, this is 0.5 * 0 * ... = 0. So both kernels write zero. But in Kernel 1, there's some computation (even though it's optimized away), while in Kernel 2, it's a direct write. So the computation in Kernel 1's final step is negligible. So that's not the reason for the runtime difference.\n\nAnother angle: Kernel 2 uses more registers due to the vectorized loads and the two-stage reduction. If the kernel is register-bound, that could limit occupancy. Let's think about the number of registers used.\n\nIn Kernel 1, each thread has variables like row, tid, max_val, and the loop variables. The code is relatively simple. In Kernel 2, each thread uses a float4 vector, local_max, and more complex loop variables. The use of warp-level reductions might require more registers. If Kernel 2 uses more registers per thread, the number of threads that can be scheduled per SM (Streaming Multiprocessor) is reduced, leading to lower occupancy and thus lower performance. \n\nFor example, if Kernel 1 uses 20 registers per thread and Kernel 2 uses 30, then with a block size of 256, the total registers per block for Kernel 1 would be 256*20 = 5120. For a GPU with 65536 registers per SM (e.g., V100), the maximum number of blocks per SM would be 65536 /5120 \u224812.8. For Kernel 2, 256*30=7680 per block, so 65536/7680 \u22488.5 blocks per SM. So Kernel 1 can have more blocks resident, leading to higher occupancy. Higher occupancy helps hide latency, leading to better performance. \n\nIf Kernel 2's register usage is higher, that could explain the lower performance despite better memory access patterns. \n\nAnother possibility is the grid configuration. Kernel 1 is launched with B blocks (each row is a block), each with 256 threads. Kernel 2 is launched similarly. But if the number of rows (B) is large enough, the grid size is the same. So that's not the issue.\n\nAnother factor is the use of shared memory. Kernel 1 uses more shared memory per block (256 floats = 1024 bytes), while Kernel 2 uses 8 floats (32 bytes). However, GPUs have a limited amount of shared memory per SM. For example, if an SM has 48KB of shared memory, Kernel 1 can have 48KB /1KB =48 blocks per SM. Kernel 2 can have 48KB /0.032KB= 1500 blocks per SM. But since the number of blocks is B (number of rows), and if B is large, the shared memory isn't the limiting factor. But if B is small, Kernel 1's shared memory usage could limit the number of active blocks. However, the user hasn't provided the problem size, so it's hard to say. But given that Kernel 1 is faster, perhaps the problem size is such that shared memory isn't the bottleneck.\n\nAlternatively, the way the max is computed in the initial phase. In Kernel 1, each thread processes elements with a stride of blockDim.x, leading to uncoalesced access but each thread's max is computed in a loop. In Kernel 2, the vectorized access is coalesced, but the code has to handle the case when i+3 exceeds row_size, which adds some branching. However, if the row_size is a multiple of 4, this isn't an issue. But if not, there's some overhead. But even with that, the vectorized access should be better. \n\nWait, but in Kernel 1, the code for loading is straightforward: each thread processes elements with a stride, which may lead to cache hits. For example, if multiple threads access the same memory bank, but since each thread's stride is blockDim.x, which is 256, the elements accessed by each thread are spread out. This could lead to poor cache utilization. In contrast, Kernel 2's vectorized accesses are contiguous, which are more cache-friendly. \n\nHmm. This is confusing. Let's think of possible reasons why Kernel 1 is faster:\n\n- Kernel 2's code has more instructions, leading to higher instruction overhead. For example, the loop with vectorized loads and the handling of the remainder elements. \n\n- The warp reduction in Kernel 2 may not be as efficient as the shared memory reduction in Kernel 1. Wait, but warp shuffles are supposed to be faster. \n\nAlternatively, the problem is in the way the max is computed. In Kernel 1, each thread's max is computed in a loop, which may allow the compiler to unroll the loop or optimize it better. \n\nAnother angle: the total number of memory transactions. In Kernel 1, each thread accesses O / blockDim.x elements, but with a stride, leading to more memory transactions. In Kernel 2, each thread accesses the same number of elements but in a coalesced manner, leading to fewer transactions. So Kernel 2 should have better memory efficiency. But if the problem size is small, the difference may not be significant. \n\nWait, but the runtimes are very close: 6.98 vs 7.1 ms. The difference is about 0.12 ms, which is small. Maybe the difference is due to some minor factors. \n\nAnother possibility is that in Kernel 2, after the per-warp reduction, the code writes the per-warp max to shared memory, then does another reduction. This requires a __syncthreads() after storing to shared memory. In Kernel 1, the reduction is done entirely in shared memory with multiple __syncthreads() calls. The number of __syncthreads() in Kernel 1 is log2(256) =8 steps, each requiring a __syncthreads(). In Kernel 2, there's one __syncthreads() after storing the per-warp maxes. So Kernel 2 has fewer synchronization points, which is better. \n\nBut again, why is Kernel 1 faster? \n\nWait, maybe the problem is in the grid configuration. Let's check the kernel launches. \n\nKernel 1 is launched with <<<B, threads, shared_mem>>>, where threads=256. Kernel 2 is launched with <<<grid, block, smem_size>>>, where grid is num_rows (same as B), block is BLOCK_SIZE=256. So same configuration. \n\nAnother thought: the GELU computation in Kernel 1 is done by thread 0, which involves a tanh computation. But in the code provided, since diff is zero, this computation is trivial. However, in Kernel 2, they just write zero. So maybe the compiler optimizes away the GELU computation in Kernel 1, making it a simple write. But in that case, both would be similar. \n\nAlternatively, the difference is in the reduction phase. Let's calculate the number of operations for reduction.\n\nIn Kernel 1: 256 elements in shared memory. Each step reduces the number of elements by half. So 256 ->128 ->64 ->32 ->16 ->8 ->4 ->2 ->1. That's 8 steps. Each step, half the threads are active. \n\nIn Kernel 2: First, each warp (32 threads) reduces to one value using shuffle instructions. For 256 threads, there are 8 warps, so 8 values. Then, these 8 values are reduced in another warp (using the first 8 threads of a warp). So the first reduction is log2(32) =5 steps (since 32 threads in a warp). The second reduction is log2(8) =3 steps (since 8 elements). Total steps:5+3=8. But these are done with shuffle instructions, which are faster than shared memory accesses. \n\nSo the total number of steps is the same, but the implementation is different. However, shuffle instructions are more efficient. So why is Kernel 1 faster? \n\nMaybe the problem is that in Kernel 2, the code has more branches and complexity. For example, the loop in Kernel 2 has a conditional to check if i+3 is within bounds. This could lead to warp divergence. If the row size is not a multiple of 4, some threads in the warp take the else path, leading to divergence. However, if the row size is a multiple of 4, this is not an issue. But if it's not, this could cause some overhead. \n\nAnother possibility is that the vectorized loads in Kernel 2 require aligned memory accesses. If the input tensor is not aligned to 16-byte boundaries (required for float4), this could cause unaligned loads, which are slower. But PyTorch tensors are generally aligned, so this might not be the case. \n\nAlternatively, the use of reinterpret_cast to float4 in Kernel 2 could introduce some overhead, but in CUDA, this is a common practice and should be efficient. \n\nHmm. This is tricky. Let's summarize the possible factors:\n\n- Kernel 1 has uncoalesced memory access but simpler code.\n- Kernel 2 has coalesced access via vectorization but more complex code with branches.\n- Kernel 2 uses warp shuffles for reduction, which are faster than shared memory reductions.\n- Kernel 1 uses more shared memory, which could limit occupancy, but in this case, it's not the bottleneck.\n- Kernel 2 may use more registers, leading to lower occupancy.\n\nIf Kernel 2 uses more registers, leading to lower occupancy, that could explain the slower runtime. Let's check the possible register usage.\n\nIn Kernel 1, the variables are:\n\n- row, tid, i, val, max_val, shared_max.\n\nIn Kernel 2:\n\n- row_idx, tid, row_start, local_max, i, vec (float4), j, other (during reduction), etc.\n\nThe use of float4 and more variables in Kernel 2 could increase register pressure. If the compiler can't optimize these, Kernel 2 may use more registers per thread, reducing the number of threads that can be active concurrently. This would lower occupancy and thus performance. \n\nFor example, if Kernel 2 uses 32 registers per thread and Kernel 1 uses 20, then with 256 threads per block, the total registers per block for Kernel 2 would be 256*32=8192. If the GPU has 65536 registers per SM, then 65536 /8192=8 blocks per SM. For Kernel 1, 256*20=5120 per block, 65536/5120\u224812.8 blocks per SM. So Kernel 1 can have more blocks resident, leading to better occupancy and hiding of memory latency. \n\nHigher occupancy allows the GPU to switch between warps more efficiently, especially when waiting for memory accesses. If Kernel 2 has lower occupancy due to higher register usage, it could be slower despite better memory access patterns. \n\nAnother factor is the use of tanhf in Kernel 1. Wait, but in the code, since diff is zero, the tanhf is computed for zero, which is a quick operation. The compiler might optimize this out. So the actual computation is negligible. \n\nIn conclusion, the most likely reason for Kernel 1 being faster is that Kernel 2's use of more registers leads to lower occupancy, which outweighs the benefits of vectorized memory access and warp-level reductions. Additionally, the uncoalesced access in Kernel 1 might not be as detrimental if the data is small enough to fit into cache, or if the memory throughput isn't the bottleneck. The combination of higher register usage and possible instruction overhead in Kernel 2 could result in slightly slower performance despite better memory access patterns.\n",
  "usage": {
    "inputTokens": 1713,
    "outputTokens": 6652,
    "totalTokens": 8365
  }
}