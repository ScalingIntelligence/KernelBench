You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.08 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused kernel with corrected memory alignment and vectorization
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

constexpr float GELU_COEF = 0.044715f;
constexpr float SQRT_2_OVER_PI = 0.7978845608028654f;

__device__ __forceinline__ float gelu_approx(float x) {
    float x_cubed = x * x * x;
    return 0.5f * x * (1.0f + tanhf(SQRT_2_OVER_PI * (x + GELU_COEF * x_cubed)));
}

__global__ void fused_avg_gelu_scale_max_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int batch_size,
    int in_features,
    int pool_size,
    float scale
) {
    extern __shared__ float shared[];
    float* group_vals = shared;
    float* block_max = &shared[in_features / pool_size];
    
    int bid = blockIdx.x;
    int tid = threadIdx.x;
    
    if (bid >= batch_size) return;
    
    const float* batch_start = input + bid * in_features;
    const int num_groups = in_features / pool_size;
    
    // Process multiple groups per thread with vectorized loads
    for (int g = tid; g < num_groups; g += blockDim.x) {
        const float* group_start = batch_start + g * pool_size;
        float sum = 0.0f;
        int i = 0;
        
        // Vectorized accumulation using float4
        for (; i <= pool_size - 4; i += 4) {
            float4 vec = *reinterpret_cast<const float4*>(group_start + i);
            sum += vec.x + vec.y + vec.z + vec.w;
        }
        
        // Handle remaining elements
        for (; i < pool_size; ++i) {
            sum += group_start[i];
        }
        
        group_vals[g] = gelu_approx(sum / pool_size) * scale;
    }
    
    __syncthreads();
    
    // Two-stage parallel reduction for max
    float local_max = -INFINITY;
    for (int g = tid; g < num_groups; g += blockDim.x) {
        local_max = fmaxf(local_max, group_vals[g]);
    }
    block_max[tid] = local_max;
    __syncthreads();
    
    // Block-wide reduction
    for (int active = blockDim.x/2; active > 0; active >>= 1) {
        if (tid < active) {
            block_max[tid] = fmaxf(block_max[tid], block_max[tid + active]);
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        output[bid] = block_max[0];
    }
}

torch::Tensor fused_avg_gelu_scale_max_cuda(
    torch::Tensor input,
    int pool_size,
    float scale
) {
    int batch_size = input.size(0);
    int in_features = input.size(1);
    int num_groups = in_features / pool_size;
    
    auto output = torch::zeros({batch_size}, input.options());
    
    // Optimized thread configuration
    int threads = min(1024, (num_groups + 31)/32*32);
    dim3 blocks(batch_size);
    size_t shared_mem = (num_groups + threads) * sizeof(float);
    
    fused_avg_gelu_scale_max_kernel<<<blocks, threads, shared_mem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        in_features,
        pool_size,
        scale
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_avg_gelu_scale_max_cuda(torch::Tensor input, int pool_size, float scale);"

# Load the optimized kernel with proper compilation flags
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_avg_gelu_scale_max_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):
        super().__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.pool_size = pool_kernel_size
        self.scale = scale_factor
        
        # Validate dimensions for vectorization
        assert out_features % pool_kernel_size == 0, "Features must be divisible by pool size"
        assert pool_kernel_size % 4 == 0, "Pool size must be divisible by 4 for vectorization"

    def forward(self, x):
        x = self.matmul(x)
        x = fused_ops.fused_avg_gelu_scale_max_cuda(x, self.pool_size, self.scale)
        return x
```

Kernel 2 (runtime: 7.15 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for optimized fused AvgPool1d+GELU+Scale+Max
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

#define M_SQRT1_2 0.70710678118654752440f

__global__ void fused_avg_gelu_scale_max_kernel(
    const float* matmul_output,
    float* output,
    int out_features,
    int pool_kernel_size,
    float scale_factor,
    int batch_size
) {
    int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;

    const float* batch_data = matmul_output + batch_idx * out_features;
    extern __shared__ float shared_max[];
    
    int num_groups = out_features / pool_kernel_size;
    int tid = threadIdx.x;
    float thread_max = -INFINITY;

    // Process multiple groups per thread with vectorized loads
    for (int group_idx = tid; group_idx < num_groups; group_idx += blockDim.x) {
        int start = group_idx * pool_kernel_size;
        const float4* ptr = reinterpret_cast<const float4*>(batch_data + start);
        int num_vec = pool_kernel_size / 4;
        float sum = 0.0f;

        // Vectorized accumulation
        for (int i = 0; i < num_vec; ++i) {
            float4 vec = ptr[i];
            sum += vec.x + vec.y + vec.z + vec.w;
        }

        float avg = sum / pool_kernel_size;
        
        // Precise GELU computation
        float gelu = 0.5f * avg * (1.0f + erff(avg * M_SQRT1_2));
        float scaled = gelu * scale_factor;
        
        thread_max = fmaxf(thread_max, scaled);
    }

    // Initialize shared memory
    shared_max[tid] = thread_max;
    __syncthreads();

    // Optimized parallel reduction
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_max[tid] = fmaxf(shared_max[tid], shared_max[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[batch_idx] = shared_max[0];
    }
}

torch::Tensor fused_avg_gelu_scale_max_cuda(
    torch::Tensor matmul_output,
    int pool_kernel_size,
    float scale_factor
) {
    int batch_size = matmul_output.size(0);
    int out_features = matmul_output.size(1);
    auto output = torch::empty({batch_size}, matmul_output.options());

    const int threads_per_block = 256;
    dim3 grid(batch_size);
    dim3 block(threads_per_block);
    size_t shared_mem_size = threads_per_block * sizeof(float);

    fused_avg_gelu_scale_max_kernel<<<grid, block, shared_mem_size>>>(
        matmul_output.data_ptr<float>(),
        output.data_ptr<float>(),
        out_features,
        pool_kernel_size,
        scale_factor,
        batch_size
    );

    return output;
}
"""

fused_kernel_cpp_source = "torch::Tensor fused_avg_gelu_scale_max_cuda(torch::Tensor matmul_output, int pool_kernel_size, float scale_factor);"

# Compile with optimized settings
fused_kernel = load_inline(
    name='fused_avg_gelu_scale_max',
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_avg_gelu_scale_max_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_cuda_cflags=['-O3', '--use_fast_math'],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        assert out_features % pool_kernel_size == 0, "out_features must be divisible by pool_kernel_size"
        assert pool_kernel_size % 4 == 0, "pool_kernel_size must be multiple of 4 for vectorization"
        self.matmul = nn.Linear(in_features, out_features)
        self.pool_kernel_size = pool_kernel_size
        self.scale_factor = scale_factor
        self.fused_kernel = fused_kernel

    def forward(self, x):
        x = self.matmul(x)
        x = self.fused_kernel.fused_avg_gelu_scale_max_cuda(
            x, 
            self.pool_kernel_size,
            self.scale_factor
        )
        return x
```
