You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 13.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized fused add-scale-sigmoid CUDA kernel with grid-stride loops and fast math
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_scale_sigmoid_kernel(
    const float* __restrict__ input,
    const float* __restrict__ bias,
    const float* __restrict__ scale,
    float* __restrict__ output,
    int channels,
    int hw,
    int total_elements
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;
    
    for (; idx < total_elements; idx += stride) {
        // Optimized channel calculation using integer division properties
        int c = (idx / hw) % channels;
        
        // Fused operations with fast math exponential
        float val = input[idx] + bias[c];
        val *= scale[c];
        output[idx] = 1.0f / (1.0f + __expf(-val));
    }
}

torch::Tensor fused_add_scale_sigmoid_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    torch::Tensor scale
) {
    auto output = torch::empty_like(input);
    int total_elements = input.numel();
    int channels = input.size(1);
    int hw = input.size(2) * input.size(3);

    const int block_size = 256;
    int num_blocks = (total_elements + block_size - 1) / block_size;
    // Cap blocks for better occupancy on most GPUs
    num_blocks = num_blocks > 4096 ? 4096 : num_blocks;

    fused_add_scale_sigmoid_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scale.data_ptr<float>(),
        output.data_ptr<float>(),
        channels,
        hw,
        total_elements
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_add_scale_sigmoid_cuda(torch::Tensor, torch::Tensor, torch::Tensor);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=["fused_add_scale_sigmoid_cuda"],
    verbose=True,
    extra_cuda_cflags=["-use_fast_math"]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_channels)
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        # Ensure contiguous memory layout for optimal kernel performance
        x = x.contiguous()
        x = self.fused_ops.fused_add_scale_sigmoid_cuda(x, self.bias, self.scale)
        x = self.group_norm(x)
        return x
```

Kernel 2 (runtime: 13.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused add-scale-sigmoid CUDA kernel
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_scale_sigmoid_kernel(
    const float* input,
    const float* bias,
    const float* scale,
    float* output,
    int channels,
    int hw,
    int total_elements
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;

    int c = (idx / hw) % channels;
    float val = input[idx] + bias[c];
    val *= scale[c];
    output[idx] = 1.0f / (1.0f + expf(-val));
}

torch::Tensor fused_add_scale_sigmoid_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    torch::Tensor scale
) {
    auto output = torch::empty_like(input);
    int total_elements = input.numel();
    int channels = input.size(1);
    int hw = input.size(2) * input.size(3);

    const int block_size = 256;
    int num_blocks = (total_elements + block_size - 1) / block_size;

    fused_add_scale_sigmoid_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scale.data_ptr<float>(),
        output.data_ptr<float>(),
        channels,
        hw,
        total_elements
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_add_scale_sigmoid_cuda(torch::Tensor, torch::Tensor, torch::Tensor);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=["fused_add_scale_sigmoid_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_channels)
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_ops.fused_add_scale_sigmoid_cuda(x, self.bias, self.scale)
        x = self.group_norm(x)
        return x
```
