You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.29 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused GELU-Softmax kernel with reduced shared memory accesses
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__inline__ __device__ float warp_reduce_max(float val) {
    for (int offset = 16; offset > 0; offset >>= 1)
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    return val;
}

__inline__ __device__ float warp_reduce_sum(float val) {
    for (int offset = 16; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void fused_gelu_softmax_kernel(float* input, float* output, int rows, int cols) {
    extern __shared__ float gelu_shared[];
    const int row_idx = blockIdx.x;
    const int tid = threadIdx.x;
    const int row_offset = row_idx * cols;
    const int warp_id = tid / 32;
    const int lane_id = tid % 32;
    const int num_warps = blockDim.x / 32;
    const int vec_size = 4;

    // Phase 1: Compute GELU and track local max
    float local_max = -INFINITY;
    for (int i = tid * vec_size; i < cols; i += blockDim.x * vec_size) {
        if (i + vec_size <= cols) {
            float4 vec = reinterpret_cast<float4*>(input + row_offset)[i / vec_size];
            #pragma unroll
            for (int j = 0; j < 4; j++) {
                float x = ((float*)&vec)[j];
                float gelu = x * 0.5f * (1.0f + tanhf(0.7978845608028654f * (x + 0.044715f * x * x * x)));
                gelu_shared[i + j] = gelu;
                local_max = fmaxf(local_max, gelu);
            }
        } else {
            for (int j = 0; j < vec_size; j++) {
                int pos = i + j;
                if (pos >= cols) break;
                float x = input[row_offset + pos];
                float gelu = x * 0.5f * (1.0f + tanhf(0.7978845608028654f * (x + 0.044715f * x * x * x)));
                gelu_shared[pos] = gelu;
                local_max = fmaxf(local_max, gelu);
            }
        }
    }
    __syncthreads();

    // Phase 2: Find global max
    float warp_max = warp_reduce_max(local_max);
    __shared__ float shared_warps_max[8];
    if (lane_id == 0) {
        shared_warps_max[warp_id] = warp_max;
    }
    __syncthreads();

    float block_max = -INFINITY;
    if (warp_id == 0) {
        block_max = lane_id < num_warps ? shared_warps_max[lane_id] : -INFINITY;
        block_max = warp_reduce_max(block_max);
    }
    __syncthreads();

    __shared__ float final_max;
    if (tid == 0) final_max = block_max;
    __syncthreads();
    float max_val = final_max;

    // Phase 3: Compute local sum
    float local_sum = 0.0f;
    for (int i = tid * vec_size; i < cols; i += blockDim.x * vec_size) {
        for (int j = 0; j < vec_size; j++) {
            int pos = i + j;
            if (pos >= cols) break;
            local_sum += expf(gelu_shared[pos] - max_val);
        }
    }

    // Phase 4: Find global sum
    float warp_sum = warp_reduce_sum(local_sum);
    __shared__ float shared_warps_sum[8];
    if (lane_id == 0) {
        shared_warps_sum[warp_id] = warp_sum;
    }
    __syncthreads();

    float block_sum = 0.0f;
    if (warp_id == 0) {
        block_sum = lane_id < num_warps ? shared_warps_sum[lane_id] : 0.0f;
        block_sum = warp_reduce_sum(block_sum);
    }
    __syncthreads();

    __shared__ float final_sum;
    if (tid == 0) final_sum = block_sum + 1e-6f;
    __syncthreads();
    float sum_val = final_sum;

    // Phase 5: Write normalized values
    for (int i = tid * vec_size; i < cols; i += blockDim.x * vec_size) {
        if (i + vec_size <= cols) {
            float4 result;
            #pragma unroll
            for (int j = 0; j < 4; j++) {
                int pos = i + j;
                result.x = (j == 0) ? expf(gelu_shared[pos] - max_val) / sum_val : result.x;
                result.y = (j == 1) ? expf(gelu_shared[pos] - max_val) / sum_val : result.y;
                result.z = (j == 2) ? expf(gelu_shared[pos] - max_val) / sum_val : result.z;
                result.w = (j == 3) ? expf(gelu_shared[pos] - max_val) / sum_val : result.w;
            }
            reinterpret_cast<float4*>(output + row_offset)[i / vec_size] = result;
        } else {
            for (int j = 0; j < vec_size; j++) {
                int pos = i + j;
                if (pos >= cols) break;
                output[row_offset + pos] = expf(gelu_shared[pos] - max_val) / sum_val;
            }
        }
    }
}

torch::Tensor fused_gelu_softmax_cuda(torch::Tensor input) {
    auto rows = input.size(0);
    auto cols = input.size(1);
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    dim3 grid(rows);
    size_t shared_mem = cols * sizeof(float);
    
    fused_gelu_softmax_kernel<<<grid, block_size, shared_mem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        rows,
        cols
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_gelu_softmax_cuda(torch::Tensor input);"

# Load the optimized CUDA kernel
fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_gelu_softmax_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.fused_op = fused_op

    def forward(self, x):
        x = self.linear(x)
        x = self.fused_op.fused_gelu_softmax_cuda(x)
        return x

# Helper functions
batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features]
```

Kernel 2 (runtime: 7.06 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void gelu_sigmoid_sum_kernel(float* input, float* output, int batch_size, int features) {
    extern __shared__ float smem[];
    int row = blockIdx.x;
    int tid = threadIdx.x;
    
    if (row >= batch_size || tid >= features) return;

    // GELU calculation
    float x = input[row * features + tid];
    float sqrt_2_over_pi = sqrt(2.f / M_PI);
    float sigmoid_part = 1.0f / (1.0f + expf(-(x * 0.7978845608f + 0.044715f * x * x)));
    float gelu_val = 0.5f * x * (1.0f + sigmoid_part);
    
    // Compute sum for normalization
    float thread_sum = 0.0f;
    for(int i=tid; i<features; i+=blockDim.x) {
        thread_sum += expf(gelu_val);
    }

    // Store intermediate sum in shared memory
    smem[tid] = thread_sum;
    __syncthreads();

    // Reduction step
    for(int stride=blockDim.x/2; stride>0; stride>>=1) {
        if(tid < stride) {
            smem[tid] += smem[tid + stride];
        }
        __syncthreads();
    }

    // Normalize and write output
    if(tid == 0) {
        float inv_total = 1.0f / smem[0];
        atomicAdd(&output[row * features + tid], expf(gelu_val) * inv_total);
    }
}

torch::Tensor fused_gelu_softmax_cuda(torch::Tensor input) {
    int batch_size = input.size(0);
    int features = input.size(1);
    auto output = torch::empty_like(input);
    
    const int block_size = 1024;
    dim3 grid(batch_size);
    size_t smem_size = block_size * sizeof(float);
    
    gelu_sigmoid_sum_kernel<<<grid, block_size, smem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        features
    );
    
    return output;
}
"""

cpp_wrapper = "torch::Tensor fused_gelu_softmax_cuda(torch::Tensor input);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=cpp_wrapper,
    cuda_sources=fused_ops_source,
    functions=['fused_gelu_softmax_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.batch_size = 1024  # Fixed from get_inputs()

    def forward(self, x):
        x = self.linear(x)
        x = fused_ops.fused_gelu_softmax_cuda(x.contiguous())
        return x
```
