You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.08 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused kernel for AvgPool1d + GELU + Scale + Max
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

constexpr float GELU_COEF = 0.044715f;
constexpr float SQRT_2_OVER_PI = 0.7978845608028654f;

__device__ float gelu_approx(float x) {
    float x_cubed = x * x * x;
    return 0.5f * x * (1.0f + tanhf(SQRT_2_OVER_PI * (x + GELU_COEF * x_cubed)));
}

__global__ void fused_avg_gelu_scale_max_kernel(
    const float* input,
    float* output,
    int batch_size,
    int in_features,
    int pool_size,
    float scale
) {
    extern __shared__ float shared[];
    
    int bid = blockIdx.x;  // Batch index
    int tid = threadIdx.x; // Processing group index
    
    if (bid >= batch_size) return;
    
    const float* batch_start = input + bid * in_features;
    int num_groups = in_features / pool_size;
    
    if (tid < num_groups) {
        float sum = 0.0f;
        #pragma unroll
        for (int i = 0; i < pool_size; ++i) {
            sum += batch_start[tid * pool_size + i];
        }
        float avg = sum / pool_size;
        float val = gelu_approx(avg) * scale;
        shared[tid] = val;
    }
    
    __syncthreads();
    
    // Parallel reduction for max
    for (int active_threads = num_groups / 2; active_threads > 0; active_threads >>= 1) {
        if (tid < active_threads) {
            shared[tid] = fmaxf(shared[tid], shared[tid + active_threads]);
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        output[bid] = shared[0];
    }
}

torch::Tensor fused_avg_gelu_scale_max_cuda(
    torch::Tensor input,
    int pool_size,
    float scale
) {
    int batch_size = input.size(0);
    int in_features = input.size(1);
    int num_groups = in_features / pool_size;
    
    auto output = torch::zeros({batch_size}, input.options());
    
    dim3 blocks(batch_size);
    dim3 threads(max(1024, (num_groups + 31) / 32 * 32));  // Use 1024 threads or next multiple of 32
    
    size_t shared_mem = num_groups * sizeof(float);
    
    fused_avg_gelu_scale_max_kernel<<<blocks, threads, shared_mem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        in_features,
        pool_size,
        scale
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_avg_gelu_scale_max_cuda(torch::Tensor input, int pool_size, float scale);"

# Load the custom CUDA kernel
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_avg_gelu_scale_max_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):
        super().__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.pool_size = pool_kernel_size
        self.scale = scale_factor
        
        # Validate dimensions
        assert out_features % pool_kernel_size == 0, "Features must be divisible by pool size"

    def forward(self, x):
        x = self.matmul(x)
        x = fused_ops.fused_avg_gelu_scale_max_cuda(x, self.pool_size, self.scale)
        return x
```

Kernel 2 (runtime: 7.15 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for optimized fused AvgPool1d+GELU+Scale+Max
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

#define M_SQRT1_2 0.70710678118654752440f

__global__ void fused_avg_gelu_scale_max_kernel(
    const float* matmul_output,
    float* output,
    int out_features,
    int pool_kernel_size,
    float scale_factor,
    int batch_size
) {
    int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;

    const float* batch_data = matmul_output + batch_idx * out_features;
    extern __shared__ float shared_max[];
    
    int num_groups = out_features / pool_kernel_size;
    int tid = threadIdx.x;
    float thread_max = -INFINITY;

    // Process multiple groups per thread with vectorized loads
    for (int group_idx = tid; group_idx < num_groups; group_idx += blockDim.x) {
        int start = group_idx * pool_kernel_size;
        const float4* ptr = reinterpret_cast<const float4*>(batch_data + start);
        int num_vec = pool_kernel_size / 4;
        float sum = 0.0f;

        // Vectorized accumulation
        for (int i = 0; i < num_vec; ++i) {
            float4 vec = ptr[i];
            sum += vec.x + vec.y + vec.z + vec.w;
        }

        float avg = sum / pool_kernel_size;
        
        // Precise GELU computation
        float gelu = 0.5f * avg * (1.0f + erff(avg * M_SQRT1_2));
        float scaled = gelu * scale_factor;
        
        thread_max = fmaxf(thread_max, scaled);
    }

    // Initialize shared memory
    shared_max[tid] = thread_max;
    __syncthreads();

    // Optimized parallel reduction
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_max[tid] = fmaxf(shared_max[tid], shared_max[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[batch_idx] = shared_max[0];
    }
}

torch::Tensor fused_avg_gelu_scale_max_cuda(
    torch::Tensor matmul_output,
    int pool_kernel_size,
    float scale_factor
) {
    int batch_size = matmul_output.size(0);
    int out_features = matmul_output.size(1);
    auto output = torch::empty({batch_size}, matmul_output.options());

    const int threads_per_block = 256;
    dim3 grid(batch_size);
    dim3 block(threads_per_block);
    size_t shared_mem_size = threads_per_block * sizeof(float);

    fused_avg_gelu_scale_max_kernel<<<grid, block, shared_mem_size>>>(
        matmul_output.data_ptr<float>(),
        output.data_ptr<float>(),
        out_features,
        pool_kernel_size,
        scale_factor,
        batch_size
    );

    return output;
}
"""

fused_kernel_cpp_source = "torch::Tensor fused_avg_gelu_scale_max_cuda(torch::Tensor matmul_output, int pool_kernel_size, float scale_factor);"

# Compile with optimized settings
fused_kernel = load_inline(
    name='fused_avg_gelu_scale_max',
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_avg_gelu_scale_max_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_cuda_cflags=['-O3', '--use_fast_math'],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        assert out_features % pool_kernel_size == 0, "out_features must be divisible by pool_kernel_size"
        assert pool_kernel_size % 4 == 0, "pool_kernel_size must be multiple of 4 for vectorization"
        self.matmul = nn.Linear(in_features, out_features)
        self.pool_kernel_size = pool_kernel_size
        self.scale_factor = scale_factor
        self.fused_kernel = fused_kernel

    def forward(self, x):
        x = self.matmul(x)
        x = self.fused_kernel.fused_avg_gelu_scale_max_cuda(
            x, 
            self.pool_kernel_size,
            self.scale_factor
        )
        return x
```
