You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 31.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused kernel for avgpool + sigmoid + sum
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_avgpool_sigmoid_sum_kernel(
    const float* conv_output,
    float* output,
    int C,
    int H_conv,
    int W_conv,
    int pool_kernel_size,
    int H_pool,
    int W_pool
) {
    int batch_idx = blockIdx.x;
    int tid = threadIdx.x;

    int total_elements = C * H_pool * W_pool;
    int elements_per_thread = (total_elements + blockDim.x - 1) / blockDim.x;

    float local_sum = 0.0f;

    for (int i = 0; i < elements_per_thread; ++i) {
        int element_idx = tid + i * blockDim.x;
        if (element_idx >= total_elements) break;

        int c = element_idx / (H_pool * W_pool);
        int hw = element_idx % (H_pool * W_pool);
        int h_pool = hw / W_pool;
        int w_pool = hw % W_pool;

        float sum = 0.0f;
        for (int ph = 0; ph < pool_kernel_size; ++ph) {
            for (int pw = 0; pw < pool_kernel_size; ++pw) {
                int h_conv = h_pool * pool_kernel_size + ph;
                int w_conv = w_pool * pool_kernel_size + pw;
                if (h_conv < H_conv && w_conv < W_conv) {
                    int idx = batch_idx * C * H_conv * W_conv + c * H_conv * W_conv + h_conv * W_conv + w_conv;
                    sum += __ldg(&conv_output[idx]);
                }
            }
        }
        float avg = sum / (pool_kernel_size * pool_kernel_size);
        float sigmoid_val = 1.0f / (1.0f + expf(-avg));
        local_sum += sigmoid_val;
    }

    __shared__ float shared_sum[256];
    shared_sum[tid] = local_sum;
    __syncthreads();

    // Block reduction
    for (int s = blockDim.x/2; s>0; s>>=1) {
        if (tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(&output[batch_idx], shared_sum[0]);
    }
}

torch::Tensor fused_avgpool_sigmoid_sum_cuda(torch::Tensor conv_output, int pool_kernel_size) {
    auto input_contig = conv_output.contiguous();
    int batch_size = input_contig.size(0);
    int C = input_contig.size(1);
    int H_conv = input_contig.size(2);
    int W_conv = input_contig.size(3);

    int H_pool = H_conv / pool_kernel_size;
    int W_pool = W_conv / pool_kernel_size;

    auto output = torch::zeros({batch_size}, input_contig.options());

    dim3 blocks(batch_size);
    dim3 threads(256);

    fused_avgpool_sigmoid_sum_kernel<<<blocks, threads>>>(
        input_contig.data_ptr<float>(),
        output.data_ptr<float>(),
        C,
        H_conv,
        W_conv,
        pool_kernel_size,
        H_pool,
        W_pool
    );

    return output;
}
"""

fused_kernel_cpp_source = "torch::Tensor fused_avgpool_sigmoid_sum_cuda(torch::Tensor conv_output, int pool_kernel_size);"

fused_avgpool_sigmoid_sum = load_inline(
    name='fused_avgpool_sigmoid_sum',
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_avgpool_sigmoid_sum_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.pool_kernel_size = pool_kernel_size

    def forward(self, x):
        x = self.conv(x)
        x = fused_avgpool_sigmoid_sum.fused_avgpool_sigmoid_sum_cuda(x, self.pool_kernel_size)
        return x
```

Kernel 2 (runtime: 31.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused kernel with warp-level reduction and scalar accesses
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__inline__ __device__ float warp_reduce(float val) {
    for (int offset = 16; offset > 0; offset >>= 1) 
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void fused_avgpool_sigmoid_sum_kernel(
    const float* __restrict__ conv_output,
    float* __restrict__ output,
    int C,
    int H_conv,
    int W_conv,
    int pool_kernel_size,
    int H_pool,
    int W_pool,
    float inv_area
) {
    const int batch_idx = blockIdx.x;
    const int tid = threadIdx.x;
    const int warp_size = 32;
    const int num_warps = blockDim.x / warp_size;
    const int warp_id = tid / warp_size;
    const int lane_id = tid % warp_size;

    const int total_elements = C * H_pool * W_pool;
    const int elements_per_warp = (total_elements + num_warps - 1) / num_warps;

    const int start_element = warp_id * elements_per_warp;
    const int end_element = min((warp_id + 1) * elements_per_warp, total_elements);

    float warp_sum = 0.0f;

    for (int element_idx = start_element + lane_id; element_idx < end_element; element_idx += warp_size) {
        const int c = element_idx / (H_pool * W_pool);
        const int hw = element_idx % (H_pool * W_pool);
        const int h_pool = hw / W_pool;
        const int w_pool = hw % W_pool;

        float window_sum = 0.0f;
        const int w_conv_base = w_pool * pool_kernel_size;
        
        #pragma unroll
        for (int ph = 0; ph < pool_kernel_size; ++ph) {
            const int h_conv = h_pool * pool_kernel_size + ph;
            if (h_conv >= H_conv) continue;
            
            const int idx_base = batch_idx * C * H_conv * W_conv 
                               + c * H_conv * W_conv 
                               + h_conv * W_conv 
                               + w_conv_base;

            #pragma unroll
            for (int pw = 0; pw < pool_kernel_size; ++pw) {
                const int w_conv = w_conv_base + pw;
                if (w_conv < W_conv) {
                    window_sum += __ldg(conv_output + idx_base + pw);
                }
            }
        }
        
        const float sigmoid_val = 1.0f / (1.0f + __expf(-(window_sum * inv_area)));
        warp_sum += sigmoid_val;
    }

    // Warp-level reduction
    warp_sum = warp_reduce(warp_sum);

    // Block-level reduction using shared memory
    __shared__ float shared_sums[32];
    if (lane_id == 0) {
        shared_sums[warp_id] = warp_sum;
    }
    __syncthreads();

    if (warp_id == 0) {
        float block_sum = lane_id < num_warps ? shared_sums[lane_id] : 0.0f;
        block_sum = warp_reduce(block_sum);
        if (lane_id == 0) {
            output[batch_idx] = block_sum;
        }
    }
}

torch::Tensor fused_avgpool_sigmoid_sum_cuda(torch::Tensor conv_output, int pool_kernel_size) {
    auto input_contig = conv_output.contiguous();
    const int batch_size = input_contig.size(0);
    const int C = input_contig.size(1);
    const int H_conv = input_contig.size(2);
    const int W_conv = input_contig.size(3);

    const int H_pool = H_conv / pool_kernel_size;
    const int W_pool = W_conv / pool_kernel_size;
    const float inv_area = 1.0f / (pool_kernel_size * pool_kernel_size);

    auto output = torch::zeros({batch_size}, input_contig.options());

    const int block_size = 256;
    dim3 blocks(batch_size);
    dim3 threads(block_size);

    fused_avgpool_sigmoid_sum_kernel<<<blocks, threads>>>(
        input_contig.data_ptr<float>(),
        output.data_ptr<float>(),
        C,
        H_conv,
        W_conv,
        pool_kernel_size,
        H_pool,
        W_pool,
        inv_area
    );

    return output;
}
"""

fused_kernel_cpp_source = "torch::Tensor fused_avgpool_sigmoid_sum_cuda(torch::Tensor conv_output, int pool_kernel_size);"

fused_avgpool_sigmoid_sum = load_inline(
    name='fused_avgpool_sigmoid_sum',
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_avgpool_sigmoid_sum_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.pool_kernel_size = pool_kernel_size

    def forward(self, x):
        x = self.conv(x)
        x = fused_avgpool_sigmoid_sum.fused_avgpool_sigmoid_sum_cuda(x, self.pool_kernel_size)
        return x
```
