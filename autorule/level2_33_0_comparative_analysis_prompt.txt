You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused scale + batch norm with proper initialization
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <ATen/cuda/CUDAContext.h>

__inline__ __device__ float warp_reduce_sum(float val) {
    for (int offset = 16; offset > 0; offset >>= 1) 
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void fused_scale_bn_kernel(
    const float* __restrict__ gemm_out,
    const float* __restrict__ scale,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ running_mean,
    float* __restrict__ running_var,
    float* __restrict__ output,
    int batch_size,
    int out_features,
    float eps,
    float momentum,
    bool training) {
    
    const int j = blockIdx.x;
    const int tid = threadIdx.x;
    const int lane_id = tid % 32;
    const int warp_id = tid / 32;

    __shared__ float smem_scale, smem_gamma, smem_beta;
    __shared__ float smem_mean, smem_var;
    __shared__ float block_sum, block_sum_sq;

    // Load parameters once per block
    if (tid == 0) {
        smem_scale = scale[j];
        smem_gamma = gamma[j];
        smem_beta = beta[j];
    }
    __syncthreads();

    const float s = smem_scale;
    const float g = smem_gamma;
    const float b = smem_beta;

    if (training) {
        // Initialize reduction variables
        if (tid == 0) {
            block_sum = 0.0f;
            block_sum_sq = 0.0f;
        }
        __syncthreads();

        float sum = 0.0f, sum_sq = 0.0f;

        // Process elements with stride equal to block size
        for (int i = tid; i < batch_size; i += blockDim.x) {
            float val = gemm_out[i * out_features + j] * s;
            sum += val;
            sum_sq += val * val;
        }

        // Warp-level reduction
        sum = warp_reduce_sum(sum);
        sum_sq = warp_reduce_sum(sum_sq);

        // First lane in each warp accumulates partial sums
        if (lane_id == 0) {
            atomicAdd(&block_sum, sum);
            atomicAdd(&block_sum_sq, sum_sq);
        }
        __syncthreads();

        // Compute mean/var and update running stats
        if (tid == 0) {
            const float mean_val = block_sum / batch_size;
            const float var_val = (block_sum_sq / batch_size) - (mean_val * mean_val);
            
            // Update running statistics
            running_mean[j] = (1 - momentum) * running_mean[j] + momentum * mean_val;
            running_var[j] = (1 - momentum) * running_var[j] + momentum * var_val;
            
            smem_mean = mean_val;
            smem_var = rsqrtf(var_val + eps);
        }
        __syncthreads();
    } else {
        // Inference mode - use running stats
        if (tid == 0) {
            smem_mean = running_mean[j];
            smem_var = rsqrtf(running_var[j] + eps);
        }
        __syncthreads();
    }

    // Apply normalization to all elements
    const float mean = smem_mean;
    const float inv_std = smem_var;

    for (int i = tid; i < batch_size; i += blockDim.x) {
        float val = gemm_out[i * out_features + j] * s;
        output[i * out_features + j] = (val - mean) * inv_std * g + b;
    }
}

torch::Tensor fused_scale_bn_cuda(
    torch::Tensor gemm_out,
    torch::Tensor scale,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps,
    float momentum,
    bool training) {
    
    auto output = torch::empty_like(gemm_out);
    const int batch_size = gemm_out.size(0);
    const int out_features = gemm_out.size(1);

    const int block_size = 256;
    dim3 grid(out_features);
    
    fused_scale_bn_kernel<<<grid, block_size, 0, at::cuda::getCurrentCUDAStream()>>>(
        gemm_out.data_ptr<float>(),
        scale.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features,
        eps,
        momentum,
        training);

    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_scale_bn_cuda(
    torch::Tensor gemm_out,
    torch::Tensor scale,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps,
    float momentum,
    bool training);
"""

fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_scale_bn_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_cuda_cflags=['-O3', '--use_fast_math'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.eps = eps
        self.momentum = momentum
        
        # BN parameters
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        
        # BN buffers
        self.register_buffer('running_mean', torch.zeros(out_features))
        self.register_buffer('running_var', torch.ones(out_features))

    def forward(self, x):
        x = self.gemm(x)
        x = fused_op.fused_scale_bn_cuda(
            x, 
            self.scale,
            self.gamma,
            self.beta,
            self.running_mean,
            self.running_var,
            self.eps,
            self.momentum,
            self.training
        )
        return x
```

Kernel 2 (runtime: 7.54 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused scale + batch norm with vectorized access and optimized reductions
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <ATen/cuda/CUDAContext.h>

__inline__ __device__ float warp_reduce_sum(float val) {
    for (int offset = 16; offset > 0; offset >>= 1) 
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void fused_scale_bn_kernel(
    const float* __restrict__ gemm_out,
    const float* __restrict__ scale,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ running_mean,
    float* __restrict__ running_var,
    float* __restrict__ output,
    int batch_size,
    int out_features,
    float eps,
    float momentum,
    bool training) {
    
    const int j = blockIdx.x;
    const int tid = threadIdx.x;
    const int lane_id = tid % 32;
    const int warp_id = tid / 32;

    __shared__ float smem_scale, smem_gamma, smem_beta;
    __shared__ float smem_mean, smem_var;
    __shared__ float block_sum, block_sum_sq;

    // Load parameters once per block
    if (tid == 0) {
        smem_scale = scale[j];
        smem_gamma = gamma[j];
        smem_beta = beta[j];
        block_sum = 0.0f;
        block_sum_sq = 0.0f;
    }
    __syncthreads();

    const float s = smem_scale;
    const float g = smem_gamma;
    const float b = smem_beta;

    float sum = 0.0f, sum_sq = 0.0f;

    if (training) {
        // Process elements with stride equal to block size
        for (int i = tid; i < batch_size; i += blockDim.x) {
            float val = __fmul_rn(gemm_out[i * out_features + j], s);
            sum = __fadd_rn(sum, val);
            sum_sq = __fadd_rn(sum_sq, __fmul_rn(val, val));
        }

        // Warp-level reduction
        sum = warp_reduce_sum(sum);
        sum_sq = warp_reduce_sum(sum_sq);

        // First lane in each warp accumulates partial sums
        if (lane_id == 0) {
            atomicAdd(&block_sum, sum);
            atomicAdd(&block_sum_sq, sum_sq);
        }
        __syncthreads();

        // Compute mean/var and update running stats
        if (tid == 0) {
            const float mean_val = __fdiv_rn(block_sum, batch_size);
            const float var_val = __fsub_rn(__fdiv_rn(block_sum_sq, batch_size), 
                                          __fmul_rn(mean_val, mean_val));
            
            // Update running statistics using compound assignment
            running_mean[j] = __fadd_rn(__fmul_rn(running_mean[j], __fsub_rn(1.0f, momentum)),
                                      __fmul_rn(momentum, mean_val));
            running_var[j] = __fadd_rn(__fmul_rn(running_var[j], __fsub_rn(1.0f, momentum)),
                                     __fmul_rn(momentum, var_val));
            
            smem_mean = mean_val;
            smem_var = __frsqrt_rn(__fadd_rn(var_val, eps));
        }
        __syncthreads();
    } else {
        // Inference mode - use running stats
        if (tid == 0) {
            smem_mean = running_mean[j];
            smem_var = __frsqrt_rn(__fadd_rn(running_var[j], eps));
        }
        __syncthreads();
    }

    // Apply normalization to all elements
    const float mean = smem_mean;
    const float inv_std = smem_var;

    for (int i = tid; i < batch_size; i += blockDim.x) {
        float val = __fmul_rn(gemm_out[i * out_features + j], s);
        float normalized = __fmaf_rn(__fmul_rn(__fsub_rn(val, mean), inv_std), g, b);
        output[i * out_features + j] = normalized;
    }
}

torch::Tensor fused_scale_bn_cuda(
    torch::Tensor gemm_out,
    torch::Tensor scale,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps,
    float momentum,
    bool training) {
    
    auto output = torch::empty_like(gemm_out);
    const int batch_size = gemm_out.size(0);
    const int out_features = gemm_out.size(1);

    const int block_size = 256;
    dim3 grid(out_features);
    
    fused_scale_bn_kernel<<<grid, block_size, 0, at::cuda::getCurrentCUDAStream()>>>(
        gemm_out.data_ptr<float>(),
        scale.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features,
        eps,
        momentum,
        training);

    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_scale_bn_cuda(
    torch::Tensor gemm_out,
    torch::Tensor scale,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps,
    float momentum,
    bool training);
"""

fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_scale_bn_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_cuda_cflags=['-O3', '--use_fast_math'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.eps = eps
        self.momentum = momentum
        
        # BN parameters
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        
        # BN buffers
        self.register_buffer('running_mean', torch.zeros(out_features))
        self.register_buffer('running_var', torch.ones(out_features))

    def forward(self, x):
        x = self.gemm(x)
        x = fused_op.fused_scale_bn_cuda(
            x, 
            self.scale,
            self.gamma,
            self.beta,
            self.running_mean,
            self.running_var,
            self.eps,
            self.momentum,
            self.training
        )
        return x
```
