You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 0.794 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

gelu_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void gelu_kernel(const float* __restrict__ x, 
                            float* __restrict__ out, 
                            int num_elements) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        const float xi = x[idx];
        const float cube = xi * xi * xi;
        const float inner = 0.7978845608f * (xi + 0.044715f * cube);
        const float tanh_val = tanhf(inner);
        out[idx] = 0.5f * xi * (1.0f + tanh_val);
    }
}

torch::Tensor gelu_forward_cuda(torch::Tensor x) {
    auto out = torch::empty_like(x);
    const int num_elements = x.numel();
    
    const int threads_per_block = 512;
    const int num_blocks = (num_elements + threads_per_block - 1) / threads_per_block;
    
    gelu_kernel<<<num_blocks, threads_per_block>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        num_elements
    );
    
    return out;
}
"""

gelu_cpp_source = "torch::Tensor gelu_forward_cuda(torch::Tensor x);"

gelu_extension = load_inline(
    name="gelu_extension",
    cpp_sources=gelu_cpp_source,
    cuda_sources=gelu_kernel_source,
    functions=["gelu_forward_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-use_fast_math"],  # Enable fast math approximations
    extra_ldflags=[""]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x):
        return gelu_extension.gelu_forward_cuda(x)
```

Kernel 2 (runtime: 0.791 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom GELU CUDA kernel
gelu_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void gelu_forward_kernel(const float* x, float* out, int n) {
    const float sqrt_2_over_pi = 0.7978845608028654f;  // sqrt(2/pi)
    const float coeff = 0.044715f;
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float x_val = x[idx];
        float x_cubed = x_val * x_val * x_val;
        float inner = sqrt_2_over_pi * (x_val + coeff * x_cubed);
        float tanh_val = tanhf(inner);
        out[idx] = 0.5f * x_val * (1.0f + tanh_val);
    }
}

torch::Tensor gelu_cuda_forward(torch::Tensor x) {
    auto out = torch::empty_like(x);
    int n = x.numel();
    
    const int threads_per_block = 256;
    int num_blocks = (n + threads_per_block - 1) / threads_per_block;
    
    gelu_forward_kernel<<<num_blocks, threads_per_block>>>(
        x.data_ptr<float>(), 
        out.data_ptr<float>(),
        n
    );
    
    return out;
}
"""

gelu_cpp_source = "torch::Tensor gelu_cuda_forward(torch::Tensor x);"

# Compile the CUDA extension
gelu_extension = load_inline(
    name='gelu_extension',
    cpp_sources=gelu_cpp_source,
    cuda_sources=gelu_kernel_source,
    functions=['gelu_cuda_forward'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math'],
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x):
        return gelu_extension.gelu_cuda_forward(x)
```
