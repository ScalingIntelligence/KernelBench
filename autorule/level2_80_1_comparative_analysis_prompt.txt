You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 6.98 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused max reduction, mean subtraction, and GELU
fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_ops_kernel(const float* input, float* output, int O) {
    int row = blockIdx.x;
    int tid = threadIdx.x;

    extern __shared__ float shared_max[];

    // Load and find max in parallel
    float max_val = -INFINITY;
    for(int i = tid; i < O; i += blockDim.x) {
        float val = input[row * O + i];
        max_val = fmaxf(max_val, val);
    }
    shared_max[tid] = max_val;
    __syncthreads();

    // Parallel max reduction
    for(int s = blockDim.x/2; s > 0; s >>= 1) {
        if(tid < s) {
            shared_max[tid] = fmaxf(shared_max[tid], shared_max[tid + s]);
        }
        __syncthreads();
    }

    // Final calculations and write result
    if(tid == 0) {
        float row_max = shared_max[0];
        float mean = row_max;
        float diff = row_max - mean;
        // GELU approximation for efficiency
        float gelu = 0.5f * diff * (1.0f + tanhf(0.7978845608f * (diff + 0.044715f * diff * diff * diff)));
        output[row] = gelu;
    }
}

torch::Tensor fused_ops_cuda(torch::Tensor input) {
    int B = input.size(0);
    int O = input.size(1);
    auto output = torch::empty({B, 1}, input.options());

    const int threads = 256;
    int shared_mem = threads * sizeof(float);
    
    fused_ops_kernel<<<B, threads, shared_mem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        O
    );

    return output;
}
"""

fused_ops_cpp_source = "torch::Tensor fused_ops_cuda(torch::Tensor input);"

# Compile the CUDA extension
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=['fused_ops_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, max_dim):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        assert max_dim == 1, "Custom kernel only supports max_dim=1"
        
    def forward(self, x):
        x = self.gemm(x)
        x = fused_ops.fused_ops_cuda(x)
        return x
```

Kernel 2 (runtime: 7.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused kernel with vectorized loads and warp reductions
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

#define BLOCK_SIZE 256

__global__ void fused_ops_kernel(
    const float* input,
    float* output,
    int num_rows,
    int row_size
) {
    extern __shared__ float smem[];
    float* max_smem = smem;

    int row_idx = blockIdx.x;
    int tid = threadIdx.x;

    if (row_idx >= num_rows) return;

    const float* row_start = input + row_idx * row_size;
    float local_max = -INFINITY;

    // Vectorized processing (4 elements per thread)
    int i = tid * 4;
    while (i < row_size) {
        if (i + 3 < row_size) {
            float4 vec = *reinterpret_cast<const float4*>(row_start + i);
            local_max = fmaxf(local_max, vec.x);
            local_max = fmaxf(local_max, vec.y);
            local_max = fmaxf(local_max, vec.z);
            local_max = fmaxf(local_max, vec.w);
        } else {
            for (int j = 0; j < 4; j++) {
                if (i + j < row_size) {
                    float val = row_start[i + j];
                    local_max = fmaxf(local_max, val);
                }
            }
        }
        i += blockDim.x * 4;
    }

    // Warp-level max reduction
    for (int offset = 16; offset > 0; offset >>= 1) {
        float other = __shfl_down_sync(0xffffffff, local_max, offset);
        local_max = fmaxf(local_max, other);
    }

    if (tid % 32 == 0) {
        int warp_id = tid / 32;
        max_smem[warp_id] = local_max;
    }

    __syncthreads();

    if (tid < 32) {
        local_max = (tid < (blockDim.x + 31) / 32) ? max_smem[tid] : -INFINITY;
        
        for (int offset = 16; offset > 0; offset >>= 1) {
            float other = __shfl_down_sync(0xffffffff, local_max, offset);
            local_max = fmaxf(local_max, other);
        }

        if (tid == 0) {
            output[row_idx] = 0.0f;  // Direct zero write after max reduction
        }
    }
}

torch::Tensor fused_operations_cuda(torch::Tensor input) {
    int num_rows = input.size(0);
    int row_size = input.size(1);
    
    auto output = torch::zeros({num_rows, 1}, input.options());
    
    dim3 grid(num_rows);
    dim3 block(BLOCK_SIZE);
    size_t smem_size = ((BLOCK_SIZE + 31) / 32) * sizeof(float);
    
    fused_ops_kernel<<<grid, block, smem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_rows,
        row_size
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_operations_cuda(torch::Tensor input);"

fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_operations_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O2']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, max_dim):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.max_dim = max_dim  # Preserved for compatibility
        
    def forward(self, x):
        x = self.gemm(x)
        x = fused_op.fused_operations_cuda(x)
        return x
```
