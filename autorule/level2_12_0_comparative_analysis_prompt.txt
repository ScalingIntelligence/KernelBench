You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 6.98 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_mul_leakyrelu_kernel(float* input, float multiplier, float negative_slope, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = input[idx] * multiplier;
        input[idx] = val > 0 ? val : val * negative_slope;
    }
}

torch::Tensor fused_mul_leakyrelu_cuda(torch::Tensor input, float multiplier, float negative_slope) {
    auto input_ = input.contiguous();
    int numel = input_.numel();
    dim3 blocks((numel + 255)/256);
    dim3 threads(256);
    
    fused_mul_leakyrelu_kernel<<<blocks, threads>>>(input_.data_ptr<float>(), multiplier, negative_slope, numel);
    
    return input_;
}
"""

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources="torch::Tensor fused_mul_leakyrelu_cuda(torch::Tensor input, float multiplier, float negative_slope);",
    cuda_sources=fused_ops_source,
    functions=['fused_mul_leakyrelu_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, multiplier, negative_slope):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.multiplier = multiplier
        self.negative_slope = negative_slope

    def forward(self, x):
        x = self.gemm(x)
        x = fused_ops.fused_mul_leakyrelu_cuda(x, self.multiplier, self.negative_slope)
        return x
```

Kernel 2 (runtime: 6.99 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_mul_leakyrelu_kernel(float* input, float multiplier, float negative_slope, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = input[idx] * multiplier;
        input[idx] = val > 0 ? val : val * negative_slope;
    }
}

torch::Tensor fused_mul_leakyrelu_cuda(torch::Tensor input, float multiplier, float negative_slope) {
    auto input_ = input.contiguous();
    int numel = input_.numel();
    dim3 blocks((numel + 255)/256);
    dim3 threads(256);
    
    fused_mul_leakyrelu_kernel<<<blocks, threads>>>(input_.data_ptr<float>(), multiplier, negative_slope, numel);
    
    return input_;
}
"""

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources="torch::Tensor fused_mul_leakyrelu_cuda(torch::Tensor input, float multiplier, float negative_slope);",
    cuda_sources=fused_ops_source,
    functions=['fused_mul_leakyrelu_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, multiplier, negative_slope):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.multiplier = multiplier
        self.negative_slope = negative_slope

    def forward(self, x):
        x = self.gemm(x)
        x = fused_ops.fused_mul_leakyrelu_cuda(x, self.multiplier, self.negative_slope)
        return x
```
