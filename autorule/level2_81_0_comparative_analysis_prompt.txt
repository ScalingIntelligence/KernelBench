You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.06 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused kernel for Swish/Div/Clamp/Tanh/Clamp operations
fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_ops_kernel(const float* input, float* output, int num_elements) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float x = input[idx];
        
        // Swish: x * sigmoid(x)
        const float sigmoid_x = 1.0f / (1.0f + expf(-x));
        x *= sigmoid_x;
        
        // Divide by 2.0
        x /= 2.0f;
        
        // First clamp
        x = fmaxf(fminf(x, 1.0f), -1.0f);
        
        // Tanh activation
        x = tanhf(x);
        
        // Final clamp
        x = fmaxf(fminf(x, 1.0f), -1.0f);
        
        output[idx] = x;
    }
}

torch::Tensor fused_ops_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    const int num_elements = input.numel();
    
    const int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;
    
    fused_ops_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    
    return output;
}
"""

fused_ops_cpp_source = "torch::Tensor fused_ops_cuda(torch::Tensor input);"

# Compile the fused operations kernel
fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=["fused_ops_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=["-lcudart"]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.gemm(x)
        x = self.fused_ops.fused_ops_cuda(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features]
```

Kernel 2 (runtime: 7.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom fused operations kernel (swish/div/clamp/tanh/clamp)
fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_ops_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx];
        
        // Swish: x * sigmoid(x)
        float sig = 1.0f / (1.0f + expf(-val));
        val *= sig;
        
        // Divide by 2.0
        val /= 2.0f;
        
        // Clamp between -1 and 1
        val = fminf(fmaxf(val, -1.0f), 1.0f);
        
        // Tanh activation
        val = tanhf(val);
        
        // Final clamp between -1 and 1
        val = fminf(fmaxf(val, -1.0f), 1.0f);
        
        output[idx] = val;
    }
}

torch::Tensor fused_ops_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    
    fused_ops_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );
    
    return output;
}
"""

fused_ops_cpp_source = "torch::Tensor fused_ops_cuda(torch::Tensor input);"

# Load the CUDA kernel as a PyTorch extension
fused_ops_extension = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=['fused_ops_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.fused_ops = fused_ops_extension

    def forward(self, x):
        x = self.gemm(x)
        x = self.fused_ops.fused_ops_cuda(x)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features]

batch_size = 1024
in_features = 8192
out_features = 8192
```
