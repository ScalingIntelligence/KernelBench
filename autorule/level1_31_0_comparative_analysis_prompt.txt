You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 18.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom ELU CUDA kernel
elu_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void elu_kernel(const float* x, float* out, float alpha, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = x[idx];
        out[idx] = val > 0 ? val : alpha * (expf(val) - 1);
    }
}

torch::Tensor elu_cuda(torch::Tensor x, float alpha) {
    auto out = torch::empty_like(x);
    int size = x.numel();
    
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    
    elu_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        alpha,
        size
    );
    
    return out;
}
"""

elu_cpp_source = "torch::Tensor elu_cuda(torch::Tensor x, float alpha);"

# Load the custom CUDA kernel
elu_extension = load_inline(
    name="elu_extension",
    cpp_sources=elu_cpp_source,
    cuda_sources=elu_source,
    functions=["elu_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3", "--use_fast_math"],
)

class ModelNew(nn.Module):
    def __init__(self, alpha: float = 1.0):
        super().__init__()
        self.alpha = alpha
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return elu_extension.elu_cuda(x, self.alpha)
```

Kernel 2 (runtime: 18.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom ELU CUDA kernel implementation
elu_cuda_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cmath>

__global__ void elu_kernel(const float* input, float* output, float alpha, int num_elements) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        const float x = input[idx];
        output[idx] = x > 0 ? x : alpha * (expf(x) - 1);
    }
}

torch::Tensor elu_forward_cuda(torch::Tensor input, float alpha) {
    auto output = torch::empty_like(input);
    const int num_elements = input.numel();
    
    const int threads_per_block = 256;
    const int num_blocks = (num_elements + threads_per_block - 1) / threads_per_block;
    
    elu_kernel<<<num_blocks, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        alpha,
        num_elements
    );
    
    return output;
}
"""

# C++ interface code
elu_cpp_source = """
torch::Tensor elu_forward_cuda(torch::Tensor input, float alpha);
"""

# Load the custom CUDA extension
custom_elu = load_inline(
    name="custom_elu",
    cpp_sources=elu_cpp_source,
    cuda_sources=elu_cuda_source,
    functions=["elu_forward_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=["-lcudart", "-lcuda"]
)

class ModelNew(nn.Module):
    def __init__(self, alpha: float = 1.0):
        super(ModelNew, self).__init__()
        self.alpha = alpha
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return custom_elu.elu_forward_cuda(x, self.alpha)

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()
    return [x]

def get_init_inputs():
    return [1.0]
```
