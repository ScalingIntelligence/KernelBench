You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 21.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

softmax_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename scalar_t>
__device__ scalar_t warp_reduce_sum(scalar_t val) {
    for(int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

template<typename scalar_t>
__device__ scalar_t block_reduce_sum(scalar_t val, scalar_t* smem) {
    static __shared__ scalar_t sdata[32];
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    sdata[lane] = val;
    __syncthreads();

    if(wid == 0) {
        for(int i=0; i<32; ++i)
            smem[i] = sdata[i];
    }
    __syncthreads();

    val = smem[lane];
    return warp_reduce_sum(val);
}

__global__ void softmax_kernel(const float* input, float* output, int num_cols) {
    extern __shared__ float smem[];
    int row = blockIdx.x;
    int tid = threadIdx.x;
    int col = tid * 32;
    const float* row_input = input + row * num_cols;
    float* row_output = output + row * num_cols;

    // Phase 1: Find max
    float max_val = -INFINITY;
    for(int i=0; i<32 && col+i<num_cols; ++i) {
        max_val = fmaxf(max_val, row_input[col+i]);
    }

    float local_max = max_val;
    for(int stride=16; stride>0; stride/=2) {
        local_max = fmaxf(local_max, __shfl_down_sync(0xffffffff, local_max, stride));
    }
    max_val = block_reduce_sum(local_max, smem);
    __syncthreads();

    // Phase 2: Compute exp terms
    float exp_sum = 0.0f;
    for(int i=0; i<32 && col+i<num_cols; ++i) {
        float diff = row_input[col+i] - max_val;
        exp_sum += expf(diff);
    }

    float local_exp_sum = exp_sum;
    for(int stride=16; stride>0; stride/=2) {
        local_exp_sum += __shfl_down_sync(0xffffffff, local_exp_sum, stride);
    }
    exp_sum = block_reduce_sum(local_exp_sum, smem);
    __syncthreads();

    // Phase 3: Write results
    for(int i=0; i<32 && col+i<num_cols; ++i) {
        float diff = row_input[col+i] - max_val;
        row_output[col+i] = expf(diff) / exp_sum;
    }
}

torch::Tensor softmax_forward(torch::Tensor input) {
    TORCH_CHECK(input.dim() == 2, "Input must be 2D");
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");

    auto output = torch::empty_like(input);
    int num_rows = input.size(0);
    int num_cols = input.size(1);
    dim3 grid(num_rows);
    int threads_per_block = 1024;
    size_t shared_mem_size = sizeof(float)*32;

    softmax_kernel<<<grid, threads_per_block, shared_mem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_cols
    );

    return output;
}
"""

softmax_cpp = "torch::Tensor softmax_forward(torch::Tensor input);"

softmax_ext = load_inline(
    name='softmax_ext',
    cpp_sources=softmax_cpp,
    cuda_sources=softmax_source,
    functions=['softmax_forward'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.softmax_forward = softmax_ext.softmax_forward

    def forward(self, x):
        return self.softmax_forward(x.cuda())
```

Kernel 2 (runtime: 36.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

softmax_source = """
#include <torch/extension.h>
#include <cuda_fp16.h>
#include <cuda_runtime.h>
#include <math.h>

#define UNROLL_FACTOR 4
#define WARP_SIZE 32

__global__ void fused_softmax_kernel(float* input, float* output, int dim) {
    extern __shared__ float shared_mem[];
    int row_idx = blockIdx.x;
    int tid = threadIdx.x;
    int warp_id = tid / WARP_SIZE;
    int lane_id = tid % WARP_SIZE;

    float* row_ptr = input + row_idx * dim;
    float* out_ptr = output + row_idx * dim;

    int vec_dim = dim / 4;
    float4* row_ptr_vec = reinterpret_cast<float4*>(row_ptr);
    float4* out_ptr_vec = reinterpret_cast<float4*>(out_ptr);

    // Phase 1: Vectorized max reduction with 4x unrolling
    float thread_max = -INFINITY;
    for (int i = tid; i < vec_dim; i += blockDim.x * UNROLL_FACTOR) {
        #pragma unroll
        for (int j = 0; j < UNROLL_FACTOR; j++) {
            int idx = i + j * blockDim.x;
            if (idx >= vec_dim) break;
            float4 vec = row_ptr_vec[idx];
            thread_max = fmaxf(thread_max, fmaxf(fmaxf(vec.x, vec.y), fmaxf(vec.z, vec.w)));
        }
    }

    // Warp-level max reduction
    for (int offset = 16; offset > 0; offset >>= 1) {
        thread_max = fmaxf(thread_max, __shfl_xor_sync(0xffffffff, thread_max, offset));
    }

    // Store warp max to shared memory
    if (lane_id == 0) {
        shared_mem[warp_id] = thread_max;
    }
    __syncthreads();

    // Block-level max reduction
    if (tid < WARP_SIZE) {
        float val = tid < (blockDim.x / WARP_SIZE) ? shared_mem[tid] : -INFINITY;
        for (int offset = 16; offset > 0; offset >>= 1) {
            val = fmaxf(val, __shfl_xor_sync(0xffffffff, val, offset));
        }
        if (tid == 0) shared_mem[0] = val;
    }
    __syncthreads();

    float row_max = shared_mem[0];

    // Phase 2: Vectorized sum reduction with 4x unrolling
    float thread_sum = 0.0f;
    for (int i = tid; i < vec_dim; i += blockDim.x * UNROLL_FACTOR) {
        #pragma unroll
        for (int j = 0; j < UNROLL_FACTOR; j++) {
            int idx = i + j * blockDim.x;
            if (idx >= vec_dim) break;
            float4 vec = row_ptr_vec[idx];
            thread_sum += expf(vec.x - row_max) + expf(vec.y - row_max)
                        + expf(vec.z - row_max) + expf(vec.w - row_max);
        }
    }

    // Warp-level sum reduction
    for (int offset = 16; offset > 0; offset >>= 1) {
        thread_sum += __shfl_xor_sync(0xffffffff, thread_sum, offset);
    }

    // Store warp sum to shared memory
    if (lane_id == 0) {
        shared_mem[warp_id] = thread_sum;
    }
    __syncthreads();

    // Block-level sum reduction
    if (tid < WARP_SIZE) {
        float val = tid < (blockDim.x / WARP_SIZE) ? shared_mem[tid] : 0.0f;
        for (int offset = 16; offset > 0; offset >>= 1) {
            val += __shfl_xor_sync(0xffffffff, val, offset);
        }
        if (tid == 0) shared_mem[0] = val;
    }
    __syncthreads();

    float row_sum = shared_mem[0];

    // Phase 3: Vectorized output with 4x unrolling
    for (int i = tid; i < vec_dim; i += blockDim.x * UNROLL_FACTOR) {
        #pragma unroll
        for (int j = 0; j < UNROLL_FACTOR; j++) {
            int idx = i + j * blockDim.x;
            if (idx >= vec_dim) break;
            float4 vec = row_ptr_vec[idx];
            vec.x = expf(vec.x - row_max) / row_sum;
            vec.y = expf(vec.y - row_max) / row_sum;
            vec.z = expf(vec.z - row_max) / row_sum;
            vec.w = expf(vec.w - row_max) / row_sum;
            out_ptr_vec[idx] = vec;
        }
    }
}

torch::Tensor fused_softmax_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int batch_size = input.size(0);
    int dim = input.size(1);
    
    const int block_size = 1024;
    dim3 grid(batch_size);
    dim3 block(block_size);
    
    fused_softmax_kernel<<<grid, block, (block_size/WARP_SIZE)*sizeof(float)>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim
    );
    
    return output;
}
"""

softmax_cpp = "torch::Tensor fused_softmax_cuda(torch::Tensor input);"

softmax_ext = load_inline(
    name="softmax_ext",
    cpp_sources=softmax_cpp,
    cuda_sources=softmax_source,
    functions=["fused_softmax_cuda"],
    verbose=True,
    extra_cuda_cflags=['-use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return softmax_ext.fused_softmax_cuda(x)
```
