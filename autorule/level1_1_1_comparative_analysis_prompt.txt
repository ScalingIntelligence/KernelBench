You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 45.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication with shared memory
matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define TILE_WIDTH 16

__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {
    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    int Row = by * TILE_WIDTH + ty;
    int Col = bx * TILE_WIDTH + tx;

    float Cvalue = 0;

    for (int m = 0; m < (N / TILE_WIDTH); ++m) {
        As[ty][tx] = A[Row * N + m * TILE_WIDTH + tx];
        Bs[ty][tx] = B[(m * TILE_WIDTH + ty) * N + Col];
        __syncthreads();

        for (int k = 0; k < TILE_WIDTH; ++k) {
            Cvalue += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    if (Row < N && Col < N) {
        C[Row * N + Col] = Cvalue;
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int N = A.size(0);
    auto C = torch::zeros({N, N}, A.options());

    dim3 threads_per_block(TILE_WIDTH, TILE_WIDTH);
    dim3 blocks_per_grid((N + TILE_WIDTH - 1) / TILE_WIDTH, (N + TILE_WIDTH - 1) / TILE_WIDTH);

    matmul_kernel<<<blocks_per_grid, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);

    return C;
}
"""

matmul_cpp_source = (
    "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"
)

# Compile the inline CUDA code for matrix multiplication
matmul = load_inline(
    name="matmul",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.matmul = matmul

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matmul.matmul_cuda(A, B)
```

Kernel 2 (runtime: 15.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized matrix multiplication CUDA kernel with corrected memory access
matmul_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_SIZE 32
#define SUB_TILE 4  // Each thread computes 4x4 submatrix

__global__ void matrix_mult_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {
    __shared__ float As[TILE_SIZE][TILE_SIZE + 1];  // Padded to avoid bank conflicts
    __shared__ float Bs[TILE_SIZE][TILE_SIZE + 1];

    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = blockIdx.y * TILE_SIZE + ty * SUB_TILE;
    int col = blockIdx.x * TILE_SIZE + tx * SUB_TILE;

    float sum[SUB_TILE][SUB_TILE] = {{0}};

    for (int k = 0; k < N; k += TILE_SIZE) {
        // Load tiles into shared memory with coalesced access
        #pragma unroll
        for (int i = 0; i < SUB_TILE; i++) {
            #pragma unroll
            for (int j = 0; j < SUB_TILE; j++) {
                // Load A tile
                int a_row = row + i;
                int a_col = k + tx * SUB_TILE + j;
                if (a_row < N && a_col < N) {
                    As[ty * SUB_TILE + i][tx * SUB_TILE + j] = A[a_row * N + a_col];
                } else {
                    As[ty * SUB_TILE + i][tx * SUB_TILE + j] = 0.0f;
                }

                // Load B tile with transposed access
                int b_row = k + ty * SUB_TILE + i;
                int b_col = col + j;
                if (b_row < N && b_col < N) {
                    Bs[ty * SUB_TILE + i][tx * SUB_TILE + j] = B[b_row * N + b_col];
                } else {
                    Bs[ty * SUB_TILE + i][tx * SUB_TILE + j] = 0.0f;
                }
            }
        }
        __syncthreads();

        // Compute partial sums using register tiling
        #pragma unroll
        for (int kb = 0; kb < TILE_SIZE; kb++) {
            #pragma unroll
            for (int i = 0; i < SUB_TILE; i++) {
                #pragma unroll
                for (int j = 0; j < SUB_TILE; j++) {
                    sum[i][j] += As[ty * SUB_TILE + i][kb] * Bs[kb][tx * SUB_TILE + j];
                }
            }
        }
        __syncthreads();
    }

    // Write results with coalesced writes
    #pragma unroll
    for (int i = 0; i < SUB_TILE; i++) {
        #pragma unroll
        for (int j = 0; j < SUB_TILE; j++) {
            int c_row = row + i;
            int c_col = col + j;
            if (c_row < N && c_col < N) {
                C[c_row * N + c_col] = sum[i][j];
            }
        }
    }
}

torch::Tensor matrix_mult_cuda(torch::Tensor A, torch::Tensor B) {
    int N = A.size(0);
    auto C = torch::zeros({N, N}, A.options());

    dim3 block(TILE_SIZE/SUB_TILE, TILE_SIZE/SUB_TILE);
    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);

    matrix_mult_kernel<<<grid, block>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        N
    );

    return C;
}
"""

matmul_cpp_source = "torch::Tensor matrix_mult_cuda(torch::Tensor A, torch::Tensor B);"

# Compile the optimized CUDA code
matmul_extension = load_inline(
    name="matmul_extension",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_kernel_source,
    functions=["matrix_mult_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.matmul_op = matmul_extension

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matmul_op.matrix_mult_cuda(A, B)
```
