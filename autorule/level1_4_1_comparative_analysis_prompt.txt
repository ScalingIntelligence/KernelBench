You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized matrix-vector multiplication CUDA kernel with vectorized loads and warp reduction
matvec_mult_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matvec_mult_kernel(const float* A, const float* B, float* output, int M, int K) {
    int row = blockIdx.x;
    int tid = threadIdx.x;
    const int threads_per_block = blockDim.x;

    float sum = 0.0f;

    // Vectorized processing: 4 elements per load
    for (int i_base = tid * 4; i_base < K; i_base += threads_per_block * 4) {
        if (i_base + 3 < K) {
            // Vectorized load using float4
            const float4* a_vec = reinterpret_cast<const float4*>(A + row*K + i_base);
            const float4* b_vec = reinterpret_cast<const float4*>(B + i_base);
            float4 a = __ldg(a_vec);
            float4 b = __ldg(b_vec);
            
            sum += a.x * b.x + a.y * b.y + a.z * b.z + a.w * b.w;
        } else {
            // Handle remaining elements with scalar loads
            for (int j = i_base; j < min(i_base + 4, K); j++) {
                sum += __ldg(A + row*K + j) * __ldg(B + j);
            }
        }
    }

    // Warp-level reduction using shuffle instructions
    float val = sum;
    for (int offset = 16; offset > 0; offset >>= 1) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }

    // First thread in each warp stores partial sum
    __shared__ float warp_sums[8];
    if (tid % 32 == 0) {
        warp_sums[tid/32] = val;
    }
    __syncthreads();

    // Final reduction across warps
    if (tid < 8) {
        val = warp_sums[tid];
        for (int offset = 4; offset > 0; offset >>= 1) {
            val += __shfl_down_sync(0xffffffff, val, offset);
        }
        if (tid == 0) {
            output[row] = val;
        }
    }
}

torch::Tensor matvec_mult_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2, "A must be 2D matrix");
    TORCH_CHECK(B.dim() == 2 && B.size(1) == 1, "B must be column vector");
    TORCH_CHECK(A.size(1) == B.size(0), "Dimension mismatch");

    int M = A.size(0);
    int K = A.size(1);
    auto output = torch::zeros({M, 1}, A.options());

    const int block_size = 256;
    dim3 grid(M);
    dim3 block(block_size);

    matvec_mult_kernel<<<grid, block>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        output.data_ptr<float>(),
        M,
        K
    );

    return output;
}
"""

matvec_mult_cpp_source = "torch::Tensor matvec_mult_cuda(torch::Tensor A, torch::Tensor B);"

# Load the optimized CUDA extension with fast math
matvec_mult = load_inline(
    name='matvec_mult',
    cpp_sources=matvec_mult_cpp_source,
    cuda_sources=matvec_mult_source,
    functions=['matvec_mult_cuda'],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3", "-use_fast_math"],
    extra_ldflags=["-lcudart"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.matvec_mult = matvec_mult

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matvec_mult.matvec_mult_cuda(A.contiguous(), B.contiguous())

M = 256 * 8
K = 131072 * 8

def get_inputs():
    A = torch.rand(M, K).cuda()
    B = torch.rand(K, 1).cuda()
    return [A, B]

def get_init_inputs():
    return []
```

Kernel 2 (runtime: 12.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized matrix-vector multiplication CUDA kernel with correct vectorization and reduction
matvec_mult_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matvec_mult_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ output, int M, int K) {
    const int row = blockIdx.x;
    const int tid = threadIdx.x;
    const int threads_per_block = blockDim.x;
    
    float sum = 0.0f;

    // Vectorized processing with boundary checks
    for (int i_base = tid * 4; i_base < K; i_base += threads_per_block * 4) {
        if (i_base + 3 < K) {  // Full vector load
            const float4* a_vec = reinterpret_cast<const float4*>(A + row*K + i_base);
            const float4* b_vec = reinterpret_cast<const float4*>(B + i_base);
            const float4 a = __ldg(a_vec);
            const float4 b = __ldg(b_vec);
            
            sum += a.x * b.x + a.y * b.y + a.z * b.z + a.w * b.w;
        } else {  // Handle remaining elements
            for (int j = i_base; j < min(i_base + 4, K); j++) {
                sum += __ldg(A + row*K + j) * __ldg(B + j);
            }
        }
    }

    // Warp-level reduction using shuffle instructions
    float val = sum;
    for (int offset = 16; offset > 0; offset >>= 1) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }

    // Store warp sums in shared memory (8 warps per block for 256 threads)
    __shared__ float warp_sums[8];
    if (tid % 32 == 0) {
        warp_sums[tid/32] = val;
    }
    __syncthreads();

    // Final reduction across warps
    if (tid == 0) {
        float total = 0.0f;
        #pragma unroll
        for (int i = 0; i < 8; ++i) {
            total += warp_sums[i];
        }
        output[row] = total;
    }
}

torch::Tensor matvec_mult_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2, "A must be 2D matrix");
    TORCH_CHECK(B.dim() == 2 && B.size(1) == 1, "B must be column vector");
    TORCH_CHECK(A.size(1) == B.size(0), "Dimension mismatch");

    const int M = A.size(0);
    const int K = A.size(1);
    auto output = torch::zeros({M, 1}, A.options());

    // 256 threads per block for optimal memory throughput
    const int block_size = 256;
    dim3 grid(M);
    dim3 block(block_size);

    matvec_mult_kernel<<<grid, block>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        output.data_ptr<float>(),
        M,
        K
    );

    return output;
}
"""

matvec_mult_cpp_source = "torch::Tensor matvec_mult_cuda(torch::Tensor A, torch::Tensor B);"

# Load the optimized CUDA extension
matvec_mult = load_inline(
    name='matvec_mult',
    cpp_sources=matvec_mult_cpp_source,
    cuda_sources=matvec_mult_source,
    functions=['matvec_mult_cuda'],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3", "-use_fast_math", "--restrict"],
    extra_ldflags=["-lcudart"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.matvec_mult = matvec_mult

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matvec_mult.matvec_mult_cuda(A.contiguous(), B.contiguous())

M = 256 * 8
K = 131072 * 8

def get_inputs():
    A = torch.rand(M, K).cuda()
    B = torch.rand(K, 1).cuda()
    return [A, B]

def get_init_inputs():
    return []
```
