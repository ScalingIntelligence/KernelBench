You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.26 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused GroupNorm+HardTanh kernel with corrected reduction
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_group_norm_hardtanh_kernel(
    const float* __restrict__ input,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ output,
    int num_features,
    int num_groups,
    float eps,
    float min_val,
    float max_val) {
    
    extern __shared__ float shmem[];
    const int batch_idx = blockIdx.x;
    const int group_idx = blockIdx.y;
    const int group_size = num_features / num_groups;
    const int start = group_idx * group_size;
    const int tid = threadIdx.x;

    float* sum_shmem = shmem;
    float* sum_sq_shmem = shmem + blockDim.x;
    float* mean_inv_std = shmem + 2 * blockDim.x;

    // Vectorized load indices
    const int vec_idx = start + 2 * tid;
    const float2 input_vec = *reinterpret_cast<const float2*>(input + batch_idx * num_features + vec_idx);

    // Compute thread-local sums
    const float sum = input_vec.x + input_vec.y;
    const float sum_sq = input_vec.x * input_vec.x + input_vec.y * input_vec.y;

    sum_shmem[tid] = sum;
    sum_sq_shmem[tid] = sum_sq;
    __syncthreads();

    // Block reduction
    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sum_shmem[tid] += sum_shmem[tid + stride];
            sum_sq_shmem[tid] += sum_sq_shmem[tid + stride];
        }
        __syncthreads();
    }

    // Compute statistics
    if (tid == 0) {
        const float total_sum = sum_shmem[0];
        const float total_sum_sq = sum_sq_shmem[0];
        const float mean = total_sum / group_size;
        const float var = (total_sum_sq / group_size - mean * mean) + eps;
        mean_inv_std[0] = mean;
        mean_inv_std[1] = rsqrtf(var);
    }
    __syncthreads();

    // Apply normalization and activation
    const float mean = mean_inv_std[0];
    const float inv_std = mean_inv_std[1];
    const float2 gamma_vec = *reinterpret_cast<const float2*>(gamma + vec_idx);
    const float2 beta_vec = *reinterpret_cast<const float2*>(beta + vec_idx);

    float2 output_vec;
    output_vec.x = fmaxf(fminf((input_vec.x - mean) * inv_std * gamma_vec.x + beta_vec.x, max_val), min_val);
    output_vec.y = fmaxf(fminf((input_vec.y - mean) * inv_std * gamma_vec.y + beta_vec.y, max_val), min_val);

    // Vectorized store
    *reinterpret_cast<float2*>(output + batch_idx * num_features + vec_idx) = output_vec;
}

torch::Tensor fused_group_norm_hardtanh_cuda(
    torch::Tensor input, torch::Tensor gamma, torch::Tensor beta,
    int num_groups, float eps, float min_val, float max_val) {
    
    auto batch_size = input.size(0);
    int num_features = input.size(1);
    auto output = torch::empty_like(input);

    dim3 grid(batch_size, num_groups);
    int threads = 256;
    size_t shared_size = (2 * threads + 2) * sizeof(float);

    fused_group_norm_hardtanh_kernel<<<grid, threads, shared_size>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        num_features,
        num_groups,
        eps,
        min_val,
        max_val
    );

    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_group_norm_hardtanh_cuda(
    torch::Tensor input, torch::Tensor gamma, torch::Tensor beta,
    int num_groups, float eps, float min_val, float max_val);
"""

# Load optimized kernel
fused_group_norm_hardtanh = load_inline(
    name='fused_group_norm_hardtanh',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_group_norm_hardtanh_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.num_groups = num_groups
        self.hardtanh_min = hardtanh_min
        self.hardtanh_max = hardtanh_max
        self.eps = 1e-5
        
        # Initialize GroupNorm parameters
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))

    def forward(self, x):
        x = self.gemm(x)
        return fused_group_norm_hardtanh.fused_group_norm_hardtanh_cuda(
            x, self.gamma, self.beta, 
            self.num_groups, self.eps,
            self.hardtanh_min, self.hardtanh_max
        )

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]

batch_size = 1024
in_features = 8192  
out_features = 8192
num_groups = 16
hardtanh_min = -2.0
hardtanh_max = 2.0
```

Kernel 2 (runtime: 7.17 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel with vectorized IO and efficient block reduction
cuda_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_fp16.h>

__global__ void fused_groupnorm_hardtanh_kernel(
    const float* __restrict__ input,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ output,
    int batch_size,
    int features_per_group,
    int num_groups,
    float eps,
    float min_val,
    float max_val) {
    
    const int sample_idx = blockIdx.x;
    const int group_idx = blockIdx.y;
    const int tid = threadIdx.x;
    const int group_start = group_idx * features_per_group;

    extern __shared__ float smem[];
    float* shared_sum = smem;
    float* shared_sqsum = &smem[blockDim.x];

    // Vectorized load (2 elements per thread)
    float2 vals = *reinterpret_cast<const float2*>(
        input + sample_idx * num_groups * features_per_group + group_start + 2 * tid
    );
    float val1 = vals.x;
    float val2 = vals.y;

    // Compute partial sums
    float sum = val1 + val2;
    float sqsum = val1*val1 + val2*val2;

    shared_sum[tid] = sum;
    shared_sqsum[tid] = sqsum;
    __syncthreads();

    // Parallel reduction
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
            shared_sqsum[tid] += shared_sqsum[tid + s];
        }
        __syncthreads();
    }

    // Precompute inverse for faster math
    const float inv_features = 1.0f / features_per_group;
    const float mean = shared_sum[0] * inv_features;
    const float var = (shared_sqsum[0] * inv_features) - (mean * mean);
    const float inv_std = rsqrtf(var + eps);

    // Apply normalization and activation with vectorized store
    const int base_idx = sample_idx * num_groups * features_per_group + group_start + 2 * tid;
    float2 gammas = *reinterpret_cast<const float2*>(gamma + group_start + 2 * tid);
    float2 betas = *reinterpret_cast<const float2*>(beta + group_start + 2 * tid);

    float2 results;
    results.x = fminf(max_val, fmaxf(min_val, (val1 - mean) * inv_std * gammas.x + betas.x));
    results.y = fminf(max_val, fmaxf(min_val, (val2 - mean) * inv_std * gammas.y + betas.y));

    *reinterpret_cast<float2*>(output + base_idx) = results;
}

torch::Tensor fused_groupnorm_hardtanh_cuda(
    torch::Tensor input, torch::Tensor gamma, torch::Tensor beta,
    int num_groups, float eps, float min_val, float max_val) {
    
    auto output = torch::empty_like(input);
    const int batch_size = input.size(0);
    const int num_features = input.size(1);
    const int features_per_group = num_features / num_groups;

    TORCH_CHECK(features_per_group % 2 == 0, "Features per group must be even for vectorization");

    const dim3 grid(batch_size, num_groups);
    const int block_size = features_per_group / 2;
    const size_t shared_mem = 2 * block_size * sizeof(float);

    fused_groupnorm_hardtanh_kernel<<<grid, block_size, shared_mem>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        features_per_group,
        num_groups,
        eps,
        min_val,
        max_val
    );

    return output;
}
"""

cpp_code = "torch::Tensor fused_groupnorm_hardtanh_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int num_groups, float eps, float min_val, float max_val);"

fused_op = load_inline(
    name="fused_groupnorm_hardtanh",
    cpp_sources=cpp_code,
    cuda_sources=cuda_code,
    functions=["fused_groupnorm_hardtanh_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3", "--use_fast_math"]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.num_groups = num_groups
        self.hardtanh_min = hardtanh_min
        self.hardtanh_max = hardtanh_max
        self.eps = 1e-5
        
        # Initialize parameters with proper data type and contiguity
        self.gamma = nn.Parameter(torch.ones(out_features, dtype=torch.float32))
        self.beta = nn.Parameter(torch.zeros(out_features, dtype=torch.float32))

    def forward(self, x):
        x = self.gemm(x)
        x = fused_op.fused_groupnorm_hardtanh_cuda(
            x, 
            self.gamma,
            self.beta,
            self.num_groups,
            self.eps,
            self.hardtanh_min,
            self.hardtanh_max
        )
        return x
```
