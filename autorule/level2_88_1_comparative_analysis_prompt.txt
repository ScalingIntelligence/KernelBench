You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.36 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused GroupNorm-Swish-Multiply-Swish CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_groupnorm_swish_multiply_swish_kernel(
    const float* input,
    const float* gamma,
    const float* beta,
    const float* multiply_weight,
    float* output,
    int batch_size,
    int num_features,
    int num_groups,
    float eps) {

    int batch_group_idx = blockIdx.x;
    int batch_idx = batch_group_idx / num_groups;
    int group_idx = batch_group_idx % num_groups;
    int feature_idx = threadIdx.x;

    const int channels_per_group = num_features / num_groups;
    if (batch_idx >= batch_size || group_idx >= num_groups || feature_idx >= channels_per_group) return;

    const int input_idx = batch_idx * num_features + group_idx * channels_per_group + feature_idx;
    const float x = input[input_idx];

    __shared__ float shared_sum[32];
    __shared__ float shared_sqsum[32];
    
    // First reduction for GroupNorm statistics
    shared_sum[feature_idx] = x;
    shared_sqsum[feature_idx] = x * x;
    __syncthreads();

    // Warp-level reduction
    for (int offset = 16; offset > 0; offset >>= 1) {
        if (feature_idx < offset) {
            shared_sum[feature_idx] += shared_sum[feature_idx + offset];
            shared_sqsum[feature_idx] += shared_sqsum[feature_idx + offset];
        }
        __syncthreads();
    }

    const float sum = shared_sum[0];
    const float sqsum = shared_sqsum[0];
    const float mean = sum / channels_per_group;
    const float var = (sqsum / channels_per_group) - (mean * mean);
    const float inv_std = rsqrtf(var + eps);

    const int param_idx = group_idx * channels_per_group + feature_idx;
    const float normalized = (x - mean) * inv_std * gamma[param_idx] + beta[param_idx];
    
    // Fused activation path
    const float swish1 = normalized * (1.0f / (1.0f + __expf(-normalized)));
    const float multiplied = swish1 * multiply_weight[param_idx];
    output[input_idx] = multiplied * (1.0f / (1.0f + __expf(-multiplied)));
}

torch::Tensor fused_groupnorm_swish_multiply_swish_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor multiply_weight,
    int num_groups,
    float eps) {

    const auto batch_size = input.size(0);
    const int num_features = input.size(1);
    auto output = torch::empty_like(input);

    const int channels_per_group = num_features / num_groups;
    const dim3 blocks(batch_size * num_groups);
    const dim3 threads(channels_per_group);

    fused_groupnorm_swish_multiply_swish_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        multiply_weight.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_features,
        num_groups,
        eps
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_groupnorm_swish_multiply_swish_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, torch::Tensor multiply_weight, int num_groups, float eps);"

# Load fused kernel
fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_groupnorm_swish_multiply_swish_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.num_groups = num_groups
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))

    def forward(self, x):
        x = self.gemm(x)
        x = fused_op.fused_groupnorm_swish_multiply_swish_cuda(
            x, self.gamma, self.beta, self.multiply_weight, self.num_groups, 1e-5
        )
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
num_groups = 256
multiply_weight_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features, num_groups, multiply_weight_shape]
```

Kernel 2 (runtime: 7.55 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernels for GroupNorm+Swish and Multiply+Swish fusion
kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

constexpr float EPSILON = 1e-5;

// Fused GroupNorm + Swish kernel
__global__ void group_norm_swish_kernel(
    const float* __restrict__ input,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ output,
    int N, int C, int G,
    float eps, int group_size
) {
    extern __shared__ float shared[];
    int n = blockIdx.x / G;
    int g = blockIdx.x % G;
    int c_start = g * group_size;
    int c = c_start + threadIdx.x;
    
    if (c >= C) return;
    
    int idx = n * C + c;
    float x = input[idx];
    
    // First pass: compute sum
    shared[threadIdx.x] = x;
    __syncthreads();
    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride)
            shared[threadIdx.x] += shared[threadIdx.x + stride];
        __syncthreads();
    }
    float mean = shared[0] / group_size;
    
    // Second pass: compute variance
    shared[threadIdx.x] = (x - mean) * (x - mean);
    __syncthreads();
    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride)
            shared[threadIdx.x] += shared[threadIdx.x + stride];
        __syncthreads();
    }
    float var = shared[0] / group_size;
    
    // Normalize and apply Swish
    float normalized = (x - mean) * rsqrtf(var + eps);
    float scaled = normalized * gamma[c] + beta[c];
    output[idx] = scaled * (1.0f / (1.0f + expf(-scaled)));
}

torch::Tensor group_norm_swish_cuda(
    torch::Tensor input, int num_groups,
    torch::Tensor gamma, torch::Tensor beta,
    float eps=EPSILON
) {
    int N = input.size(0);
    int C = input.size(1);
    int G = num_groups;
    int group_size = C / G;
    
    auto output = torch::empty_like(input);
    dim3 blocks(N * G);
    dim3 threads(group_size);
    
    group_norm_swish_kernel<<<blocks, threads, group_size*sizeof(float)>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, G, eps, group_size
    );
    return output;
}

// Fused Multiply + Swish kernel
__global__ void multiply_swish_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    float* __restrict__ output,
    int total_elements, int out_features
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < total_elements) {
        int c = idx % out_features;
        float x = input[idx] * weight[c];
        output[idx] = x * (1.0f / (1.0f + expf(-x)));
    }
}

torch::Tensor multiply_swish_cuda(torch::Tensor input, torch::Tensor weight) {
    int total_elements = input.numel();
    int out_features = input.size(1);
    auto output = torch::empty_like(input);
    
    const int BLOCK_SIZE = 256;
    int grid_size = (total_elements + BLOCK_SIZE - 1) / BLOCK_SIZE;
    
    multiply_swish_kernel<<<grid_size, BLOCK_SIZE>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        output.data_ptr<float>(),
        total_elements, out_features
    );
    return output;
}
"""

cpp_source = """
torch::Tensor group_norm_swish_cuda(torch::Tensor input, int num_groups, torch::Tensor gamma, torch::Tensor beta, float eps);
torch::Tensor multiply_swish_cuda(torch::Tensor input, torch::Tensor weight);
"""

custom_ops = load_inline(
    name='custom_ops',
    cpp_sources=cpp_source,
    cuda_sources=kernel_source,
    functions=['group_norm_swish_cuda', 'multiply_swish_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.num_groups = num_groups
        self.eps = 1e-5
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))
        self.custom_ops = custom_ops

    def forward(self, x):
        x = self.gemm(x)
        x = self.custom_ops.group_norm_swish_cuda(
            x, self.num_groups, self.gamma, self.beta, self.eps
        )
        x = self.custom_ops.multiply_swish_cuda(x, self.multiply_weight)
        return x
```
