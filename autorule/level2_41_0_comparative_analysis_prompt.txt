You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 30.6 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused BatchNorm+GELU+ReLU kernel with training/inference support
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_bn_gelu_relu_kernel(
    const float* __restrict__ input,
    const float* __restrict__ scale,
    const float* __restrict__ shift,
    float* __restrict__ output,
    int total_elements,
    int out_features) {

    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;

    const int feature_idx = idx % out_features;
    const float x = input[idx];
    
    // Fused BatchNorm transformation: y = x * scale + shift
    const float normalized = x * scale[feature_idx] + shift[feature_idx];
    
    // High-precision GELU approximation matching PyTorch's implementation
    const float cdf = 0.5f * (1.0f + erff(normalized * 0.7071067811865475f));
    const float gelu = normalized * cdf;
    
    // Fused ReLU with branchless implementation
    output[idx] = fmaxf(gelu, 0.0f);
}

torch::Tensor fused_bn_gelu_relu_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor shift,
    int batch_size,
    int out_features) {
    
    auto output = torch::empty_like(input);
    const int total_elements = input.numel();
    
    constexpr int block_size = 256;
    const int grid_size = (total_elements + block_size - 1) / block_size;
    
    fused_bn_gelu_relu_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        scale.data_ptr<float>(),
        shift.data_ptr<float>(),
        output.data_ptr<float>(),
        total_elements,
        out_features
    );
    
    return output;
}
"""

fused_kernel_cpp_source = """
torch::Tensor fused_bn_gelu_relu_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor shift,
    int batch_size,
    int out_features);
"""

# Load the optimized fused kernel with fast math optimizations
fused_kernel = load_inline(
    name='fused_bn_gelu_relu',
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_bn_gelu_relu_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_cuda_cflags=['-O3', '--use_fast_math'],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bn = nn.BatchNorm1d(out_features)
        self.bn.momentum = 0.1  # Match PyTorch's default

    def forward(self, x):
        x = self.gemm(x)
        batch_size, out_dim = x.shape

        if self.training:
            # Compute batch statistics using optimized PyTorch ops
            mean = x.mean(dim=0)
            var = x.var(dim=0, unbiased=False)
            
            # Update running statistics with proper in-place operations
            with torch.no_grad():
                self.bn.running_mean *= (1 - self.bn.momentum)
                self.bn.running_mean.add_(mean, alpha=self.bn.momentum)
                
                self.bn.running_var *= (1 - self.bn.momentum)
                self.bn.running_var.add_(var, alpha=self.bn.momentum)
        else:
            # Use pre-computed running statistics
            mean = self.bn.running_mean
            var = self.bn.running_var

        # Compute scale and shift parameters with numerical stability
        inv_std = 1.0 / torch.sqrt(var + self.bn.eps)
        scale = self.bn.weight * inv_std
        shift = self.bn.bias - mean * scale

        return fused_kernel.fused_bn_gelu_relu_cuda(
            x, 
            scale, 
            shift,
            batch_size,
            out_dim
        )

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]

batch_size = 16384
in_features = 4096
out_features = 4096
```

Kernel 2 (runtime: 30.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused BatchNorm+GELU+ReLU kernel with training/inference support
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_bn_gelu_relu_kernel(
    const float* __restrict__ input,
    const float* __restrict__ scale,
    const float* __restrict__ shift,
    float* __restrict__ output,
    int total_elements,
    int out_features) {

    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;

    const int feature_idx = idx % out_features;
    const float x = input[idx];
    
    // Fused BatchNorm transformation: y = x * scale + shift
    const float normalized = x * scale[feature_idx] + shift[feature_idx];
    
    // High-precision GELU approximation matching PyTorch's implementation
    const float cdf = 0.5f * (1.0f + erff(normalized * 0.7071067811865475f));
    const float gelu = normalized * cdf;
    
    // Fused ReLU with branchless implementation
    output[idx] = fmaxf(gelu, 0.0f);
}

torch::Tensor fused_bn_gelu_relu_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor shift,
    int batch_size,
    int out_features) {
    
    auto output = torch::empty_like(input);
    const int total_elements = input.numel();
    
    constexpr int block_size = 256;
    const int grid_size = (total_elements + block_size - 1) / block_size;
    
    fused_bn_gelu_relu_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        scale.data_ptr<float>(),
        shift.data_ptr<float>(),
        output.data_ptr<float>(),
        total_elements,
        out_features
    );
    
    return output;
}
"""

fused_kernel_cpp_source = """
torch::Tensor fused_bn_gelu_relu_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor shift,
    int batch_size,
    int out_features);
"""

# Load the optimized fused kernel with fast math optimizations
fused_kernel = load_inline(
    name='fused_bn_gelu_relu',
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_bn_gelu_relu_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_cuda_cflags=['-O3', '--use_fast_math'],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bn = nn.BatchNorm1d(out_features)
        self.bn.momentum = 0.1  # Match PyTorch's default

    def forward(self, x):
        x = self.gemm(x)
        batch_size, out_dim = x.shape

        if self.training:
            # Compute batch statistics using optimized PyTorch ops
            mean = x.mean(dim=0)
            var = x.var(dim=0, unbiased=False)
            
            # Update running statistics with proper in-place operations
            with torch.no_grad():
                self.bn.running_mean *= (1 - self.bn.momentum)
                self.bn.running_mean.add_(mean, alpha=self.bn.momentum)
                
                self.bn.running_var *= (1 - self.bn.momentum)
                self.bn.running_var.add_(var, alpha=self.bn.momentum)
        else:
            # Use pre-computed running statistics
            mean = self.bn.running_mean
            var = self.bn.running_var

        # Compute scale and shift parameters with numerical stability
        inv_std = 1.0 / torch.sqrt(var + self.bn.eps)
        scale = self.bn.weight * inv_std
        shift = self.bn.bias - mean * scale

        return fused_kernel.fused_bn_gelu_relu_cuda(
            x, 
            scale, 
            shift,
            batch_size,
            out_dim
        )

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]

batch_size = 16384
in_features = 4096
out_features = 4096
```
