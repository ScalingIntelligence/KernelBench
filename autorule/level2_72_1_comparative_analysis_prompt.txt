You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 31.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused average pooling CUDA kernel with correct boundary handling
fused_pool_source = """
#include <torch/extension.h>
#include <cuda_fp16.h>

__global__ void fused_pool_kernel(const float* __restrict__ input, float* __restrict__ output, 
                                 int B, int C, int D, int H, int W) {
    const int total_w = W / 4;
    const int total_h = H / 4;
    const int total_d = D / 4;
    const int elements = B * C * total_d * total_h * total_w;
    
    const int stride_b = C * D * H * W;
    const int stride_c = D * H * W;
    const int stride_d = H * W;
    const int stride_h = W;
    
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;
    
    for (int idx = tid; idx < elements; idx += stride) {
        int tmp_idx = idx;
        int w = tmp_idx % total_w;
        tmp_idx /= total_w;
        int h = tmp_idx % total_h;
        tmp_idx /= total_h;
        int d = tmp_idx % total_d;
        tmp_idx /= total_d;
        int c = tmp_idx % C;
        int b = tmp_idx / C;
        
        float sum = 0.0f;
        int count = 0;
        
        #pragma unroll
        for(int dd = 0; dd < 4; ++dd) {
            const int in_d = d*4 + dd;
            if(in_d >= D) continue;
            #pragma unroll
            for(int hh = 0; hh < 4; ++hh) {
                const int in_h = h*4 + hh;
                if(in_h >= H) continue;
                #pragma unroll
                for(int ww = 0; ww < 4; ++ww) {
                    const int in_w = w*4 + ww;
                    if(in_w >= W) continue;
                    
                    const int input_idx = b*stride_b + c*stride_c + in_d*stride_d + in_h*stride_h + in_w;
                    sum += __ldg(&input[input_idx]);
                    count++;
                }
            }
        }
        output[idx] = sum / count;
    }
}

torch::Tensor fused_pool_cuda(torch::Tensor input) {
    auto B = input.size(0);
    auto C = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);
    
    auto output = torch::zeros({B, C, D/4, H/4, W/4}, input.options());
    
    int elements = B * C * (D/4) * (H/4) * (W/4);
    int threads = 512;
    int blocks = (elements + threads - 1) / threads;
    
    fused_pool_kernel<<<blocks, threads>>>(input.data_ptr<float>(), 
                                          output.data_ptr<float>(),
                                          B, C, D, H, W);
    
    return output;
}
"""

fused_pool_cpp = "torch::Tensor fused_pool_cuda(torch::Tensor input);"

fused_pool = load_inline(
    name="fused_pool",
    cpp_sources=fused_pool_cpp,
    cuda_sources=fused_pool_source,
    functions=["fused_pool_cuda"],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride, padding)
        self.bn = nn.BatchNorm3d(out_channels)
        self.fused_pool = fused_pool

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.bn(x)
        x = self.fused_pool.fused_pool_cuda(x)
        return x
```

Kernel 2 (runtime: 32.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused average pooling CUDA kernel with unrolled loops and fast math
fused_pool_source = """
#include <torch/extension.h>
#include <cuda_fp16.h>

__global__ void fused_pool_kernel(const float* input, float* output, 
                                 int B, int C, int D, int H, int W) {
    const int total_w = W / 4;
    const int total_h = H / 4;
    const int total_d = D / 4;
    const int elements = B * C * total_d * total_h * total_w;
    
    const int stride_b = C * D * H * W;
    const int stride_c = D * H * W;
    const int stride_d = H * W;
    const int stride_h = W;
    
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;
    
    const float inv_count = 1.0f / 64.0f;  // Precompute reciprocal
    
    for (int idx = tid; idx < elements; idx += stride) {
        int tmp_idx = idx;
        int w = tmp_idx % total_w;
        tmp_idx /= total_w;
        int h = tmp_idx % total_h;
        tmp_idx /= total_h;
        int d = tmp_idx % total_d;
        tmp_idx /= total_d;
        int c = tmp_idx % C;
        int b = tmp_idx / C;
        
        float sum = 0.0f;
        
        #pragma unroll
        for(int dd = 0; dd < 4; ++dd) {
            const int in_d = (d << 2) + dd;  // Bit shift for multiplication
            #pragma unroll
            for(int hh = 0; hh < 4; ++hh) {
                const int in_h = (h << 2) + hh;
                #pragma unroll
                for(int ww = 0; ww < 4; ++ww) {
                    const int in_w = (w << 2) + ww;
                    const int input_idx = b*stride_b + c*stride_c + in_d*stride_d + in_h*stride_h + in_w;
                    sum += __ldg(&input[input_idx]);  // Cache optimized load
                }
            }
        }
        output[idx] = sum * inv_count;
    }
}

torch::Tensor fused_pool_cuda(torch::Tensor input) {
    auto B = input.size(0);
    auto C = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);
    
    auto output = torch::zeros({B, C, D/4, H/4, W/4}, input.options());
    
    int elements = B * C * (D/4) * (H/4) * (W/4);
    int threads = 512;
    int blocks = (elements + threads - 1) / threads;
    
    fused_pool_kernel<<<blocks, threads>>>(input.data_ptr<float>(), 
                                          output.data_ptr<float>(),
                                          B, C, D, H, W);
    
    return output;
}
"""

fused_pool_cpp = "torch::Tensor fused_pool_cuda(torch::Tensor input);"

fused_pool = load_inline(
    name="fused_pool",
    cpp_sources=fused_pool_cpp,
    cuda_sources=fused_pool_source,
    functions=["fused_pool_cuda"],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math', '-Xptxas=-v']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride, padding)
        self.bn = nn.BatchNorm3d(out_channels)
        self.fused_pool = fused_pool

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.bn(x)
        x = self.fused_pool.fused_pool_cuda(x)
        return x
```
