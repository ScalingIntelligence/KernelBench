You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 100.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized matrix multiplication CUDA kernel with tiling and correctness fixes
matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int N, int K) {
    const int TILE_SIZE = 32;
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;

    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    float sum = 0.0f;
    int num_tiles = (K + TILE_SIZE - 1) / TILE_SIZE;

    for (int t = 0; t < num_tiles; ++t) {
        // Load A tile
        int a_col = t * TILE_SIZE + tx;
        if (row < M && a_col < K) {
            As[ty][tx] = A[row * K + a_col];
        } else {
            As[ty][tx] = 0.0f;
        }

        // Load B tile
        int b_row = t * TILE_SIZE + ty;
        if (b_row < K && col < N) {
            Bs[ty][tx] = B[b_row * N + col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        // Compute partial sum
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    // Write result if within bounds
    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Inputs must be 2D tensors");
    int M = A.size(0), K = A.size(1), N = B.size(1);
    TORCH_CHECK(B.size(0) == K, "Matrix dimensions mismatch");

    // Ensure contiguous memory access
    A = A.contiguous();
    B = B.contiguous();

    auto C = torch::zeros({M, N}, A.options());
    const int TILE_SIZE = 32;
    dim3 blocks((N + TILE_SIZE - 1)/TILE_SIZE, (M + TILE_SIZE - 1)/TILE_SIZE);
    dim3 threads(TILE_SIZE, TILE_SIZE);
    
    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, N, K);
    return C;
}
"""

matmul_cpp_source = "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul_extension = load_inline(
    name="matmul_extension",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_cuda_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.matmul = matmul_extension.matmul_cuda

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matmul(A, B)
```

Kernel 2 (runtime: 80.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_SIZE 64
#define SUB_TILE 4

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int N, int K) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    int row = by * TILE_SIZE + ty * SUB_TILE;
    int col = bx * TILE_SIZE + tx * SUB_TILE;

    float sum[SUB_TILE][SUB_TILE] = {{0.0f}};

    for (int k_step = 0; k_step < K; k_step += TILE_SIZE) {
        // Load tiles into shared memory with coalesced access
        #pragma unroll
        for (int i = 0; i < SUB_TILE; ++i) {
            #pragma unroll
            for (int j = 0; j < SUB_TILE; ++j) {
                int a_row = row + i;
                int a_col = k_step + tx * SUB_TILE + j;
                if (a_row < M && a_col < K) {
                    As[ty * SUB_TILE + i][tx * SUB_TILE + j] = A[a_row * K + a_col];
                } else {
                    As[ty * SUB_TILE + i][tx * SUB_TILE + j] = 0.0f;
                }

                int b_row = k_step + ty * SUB_TILE + i;
                int b_col = col + j;
                if (b_row < K && b_col < N) {
                    Bs[tx * SUB_TILE + j][ty * SUB_TILE + i] = B[b_row * N + b_col];
                } else {
                    Bs[tx * SUB_TILE + j][ty * SUB_TILE + i] = 0.0f;
                }
            }
        }

        __syncthreads();

        // Compute partial sums using shared memory tiles
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; ++k) {
            #pragma unroll
            for (int i = 0; i < SUB_TILE; ++i) {
                #pragma unroll
                for (int j = 0; j < SUB_TILE; ++j) {
                    sum[i][j] += As[ty * SUB_TILE + i][k] * Bs[tx * SUB_TILE + j][k];
                }
            }
        }

        __syncthreads();
    }

    // Write results to global memory
    #pragma unroll
    for (int i = 0; i < SUB_TILE; ++i) {
        #pragma unroll
        for (int j = 0; j < SUB_TILE; ++j) {
            int c_row = row + i;
            int c_col = col + j;
            if (c_row < M && c_col < N) {
                C[c_row * N + c_col] = sum[i][j];
            }
        }
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K = A.size(1);
    int N = B.size(1);

    auto C = torch::zeros({M, N}, A.options());

    dim3 block(TILE_SIZE/SUB_TILE, TILE_SIZE/SUB_TILE);
    dim3 grid((N + TILE_SIZE - 1)/TILE_SIZE, (M + TILE_SIZE - 1)/TILE_SIZE);

    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, N, K);

    return C;
}
"""

matmul_cpp_source = "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul_extension = load_inline(
    name='matmul_extension',
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=['matmul_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.matmul = matmul_extension.matmul_cuda

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matmul(A, B)

M = 8205
K = 2949
N = 5921

def get_inputs():
    A = torch.rand(M, K).cuda()
    B = torch.rand(K, N).cuda()
    return [A, B]

def get_init_inputs():
    return []
```
