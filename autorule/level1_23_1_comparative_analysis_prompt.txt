You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 36.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

softmax_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_softmax_kernel(float* input, float* output, int dim) {
    extern __shared__ float shared_mem[];
    int row_idx = blockIdx.x;
    int tid = threadIdx.x;
    
    float* row_ptr = input + row_idx * dim;
    float* out_ptr = output + row_idx * dim;

    int vec_dim = dim / 4;
    float4* row_ptr_vec = reinterpret_cast<float4*>(row_ptr);
    float4* out_ptr_vec = reinterpret_cast<float4*>(out_ptr);

    // Phase 1: Vectorized max reduction
    float thread_max = -INFINITY;
    for (int i = tid; i < vec_dim; i += blockDim.x) {
        float4 vec = row_ptr_vec[i];
        thread_max = fmaxf(thread_max, fmaxf(fmaxf(vec.x, vec.y), fmaxf(vec.z, vec.w)));
    }

    shared_mem[tid] = thread_max;
    __syncthreads();

    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);
        }
        __syncthreads();
    }
    
    float row_max = shared_mem[0];
    __syncthreads();

    // Phase 2: Vectorized sum reduction
    float thread_sum = 0.0f;
    for (int i = tid; i < vec_dim; i += blockDim.x) {
        float4 vec = row_ptr_vec[i];
        thread_sum += expf(vec.x - row_max) + expf(vec.y - row_max) 
                     + expf(vec.z - row_max) + expf(vec.w - row_max);
    }

    shared_mem[tid] = thread_sum;
    __syncthreads();

    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_mem[tid] += shared_mem[tid + s];
        }
        __syncthreads();
    }
    
    float row_sum = shared_mem[0];
    __syncthreads();

    // Phase 3: Vectorized output calculation
    for (int i = tid; i < vec_dim; i += blockDim.x) {
        float4 vec = row_ptr_vec[i];
        float4 result;
        result.x = expf(vec.x - row_max) / row_sum;
        result.y = expf(vec.y - row_max) / row_sum;
        result.z = expf(vec.z - row_max) / row_sum;
        result.w = expf(vec.w - row_max) / row_sum;
        out_ptr_vec[i] = result;
    }
}

torch::Tensor fused_softmax_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int batch_size = input.size(0);
    int dim = input.size(1);
    
    const int block_size = 1024;
    dim3 grid(batch_size);
    dim3 block(block_size);
    
    fused_softmax_kernel<<<grid, block, block_size * sizeof(float)>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim
    );
    
    return output;
}
"""

softmax_cpp = "torch::Tensor fused_softmax_cuda(torch::Tensor input);"

softmax_ext = load_inline(
    name="softmax_ext",
    cpp_sources=softmax_cpp,
    cuda_sources=softmax_source,
    functions=["fused_softmax_cuda"],
    verbose=True,
    extra_cuda_cflags=['-use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return softmax_ext.fused_softmax_cuda(x)
```

Kernel 2 (runtime: 36.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel with vectorization, warp shuffles, and residual handling
softmax_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void softmax_kernel(const float* input, float* output, int dim) {
    const int vec_size = 4;
    int row = blockIdx.x;
    int tid = threadIdx.x;
    int warp_id = tid / 32;
    int lane_id = tid % 32;
    
    const float4* input_vec = reinterpret_cast<const float4*>(input + row * dim);
    float4* output_vec = reinterpret_cast<float4*>(output + row * dim);
    int dim_vec = dim / vec_size;
    int remaining = dim % vec_size;

    // Vectorized max reduction
    float max_val = -INFINITY;
    for(int i = tid; i < dim_vec; i += blockDim.x) {
        float4 vals = input_vec[i];
        max_val = fmaxf(max_val, fmaxf(fmaxf(vals.x, vals.y), fmaxf(vals.z, vals.w)));
    }

    // Handle residual elements for max
    if (remaining > 0 && tid < remaining) {
        float val = input[row * dim + dim_vec * vec_size + tid];
        max_val = fmaxf(max_val, val);
    }

    // Warp-level max reduction
    for(int offset = 16; offset > 0; offset >>= 1) {
        max_val = fmaxf(max_val, __shfl_down_sync(0xffffffff, max_val, offset));
    }

    // Block-level max reduction
    extern __shared__ float shared[];
    if(lane_id == 0) shared[warp_id] = max_val;
    __syncthreads();
    if(tid < 32) {
        float val = tid < (blockDim.x+31)/32 ? shared[tid] : -INFINITY;
        for(int offset = 16; offset > 0; offset >>= 1) {
            val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
        }
        if(tid == 0) shared[0] = val;
    }
    __syncthreads();
    float row_max = shared[0];

    // Vectorized sum reduction
    float sum = 0.0f;
    for(int i = tid; i < dim_vec; i += blockDim.x) {
        float4 vals = input_vec[i];
        sum += expf(vals.x - row_max) + expf(vals.y - row_max)
             + expf(vals.z - row_max) + expf(vals.w - row_max);
    }

    // Handle residual elements for sum
    if (remaining > 0 && tid < remaining) {
        float val = input[row * dim + dim_vec * vec_size + tid];
        sum += expf(val - row_max);
    }

    // Warp-level sum reduction
    for(int offset = 16; offset > 0; offset >>= 1) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
    }

    // Block-level sum reduction
    if(lane_id == 0) shared[warp_id] = sum;
    __syncthreads();
    if(tid < 32) {
        float val = tid < (blockDim.x+31)/32 ? shared[tid] : 0.0f;
        for(int offset = 16; offset > 0; offset >>= 1) {
            val += __shfl_down_sync(0xffffffff, val, offset);
        }
        if(tid == 0) shared[0] = val;
    }
    __syncthreads();
    float row_sum = shared[0] + 1e-6f;

    // Vectorized write with normalization
    for(int i = tid; i < dim_vec; i += blockDim.x) {
        float4 vals = input_vec[i];
        float4 out;
        out.x = expf(vals.x - row_max) / row_sum;
        out.y = expf(vals.y - row_max) / row_sum;
        out.z = expf(vals.z - row_max) / row_sum;
        out.w = expf(vals.w - row_max) / row_sum;
        output_vec[i] = out;
    }

    // Handle residual elements for output
    if (remaining > 0 && tid < remaining) {
        int idx = dim_vec * vec_size + tid;
        output[row * dim + idx] = expf(input[row * dim + idx] - row_max) / row_sum;
    }
}

torch::Tensor softmax_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int batch_size = input.size(0);
    int dim = input.size(1);
    
    const int block_size = 1024;
    dim3 grid(batch_size);
    dim3 block(block_size);
    size_t shared_size = ((block_size + 31)/32) * sizeof(float);
    
    softmax_kernel<<<grid, block, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim
    );
    
    return output;
}
"""

softmax_cpp_source = "torch::Tensor softmax_cuda(torch::Tensor input);"

softmax_ext = load_inline(
    name='softmax_ext',
    cpp_sources=softmax_cpp_source,
    cuda_sources=softmax_source,
    functions=['softmax_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_cuda_cflags=['-Xcompiler', '-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return softmax_ext.softmax_cuda(x)
```
