You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 13.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

avg_pool_3d_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void avg_pool_3d_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    const int batch_size,
    const int channels,
    const int input_depth,
    const int input_height,
    const int input_width,
    const int kernel_size,
    const int stride,
    const int padding,
    const int output_depth,
    const int output_height,
    const int output_width
) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int total_elements = batch_size * channels * output_depth * output_height * output_width;
    if (tid >= total_elements) return;

    // Compute indices
    const int batch = tid / (channels * output_depth * output_height * output_width);
    const int remaining = tid % (channels * output_depth * output_height * output_width);
    const int channel = remaining / (output_depth * output_height * output_width);
    const int spatial_idx = remaining % (output_depth * output_height * output_width);
    const int out_d = spatial_idx / (output_height * output_width);
    const int spatial_remaining = spatial_idx % (output_height * output_width);
    const int out_h = spatial_remaining / output_width;
    const int out_w = spatial_remaining % output_width;

    // Calculate input positions
    const int in_d_start = out_d * stride - padding;
    const int in_h_start = out_h * stride - padding;
    const int in_w_start = out_w * stride - padding;

    float sum = 0.0f;

    for (int kd = 0; kd < kernel_size; ++kd) {
        const int in_d = in_d_start + kd;
        if (in_d < 0 || in_d >= input_depth) continue;
        for (int kh = 0; kh < kernel_size; ++kh) {
            const int in_h = in_h_start + kh;
            if (in_h < 0 || in_h >= input_height) continue;
            for (int kw = 0; kw < kernel_size; ++kw) {
                const int in_w = in_w_start + kw;
                if (in_w < 0 || in_w >= input_width) continue;
                
                const int in_offset = batch * channels * input_depth * input_height * input_width +
                                      channel * input_depth * input_height * input_width +
                                      in_d * input_height * input_width +
                                      in_h * input_width +
                                      in_w;
                sum += input[in_offset];
            }
        }
    }

    const int out_offset = batch * channels * output_depth * output_height * output_width +
                          channel * output_depth * output_height * output_width +
                          out_d * output_height * output_width +
                          out_h * output_width +
                          out_w;
    output[out_offset] = sum / static_cast<float>(kernel_size * kernel_size * kernel_size);
}

torch::Tensor avg_pool_3d_cuda(torch::Tensor input, int kernel_size, int stride, int padding) {
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");
    TORCH_CHECK(input.dim() == 5, "Input must be 5D (NCDHW)");

    const auto sizes = input.sizes();
    const int batch_size = sizes[0];
    const int channels = sizes[1];
    const int input_depth = sizes[2];
    const int input_height = sizes[3];
    const int input_width = sizes[4];

    const int output_depth = (input_depth + 2 * padding - kernel_size) / stride + 1;
    const int output_height = (input_height + 2 * padding - kernel_size) / stride + 1;
    const int output_width = (input_width + 2 * padding - kernel_size) / stride + 1;

    auto output = torch::empty({batch_size, channels, output_depth, output_height, output_width}, input.options());

    const int total_elements = batch_size * channels * output_depth * output_height * output_width;
    const int threads_per_block = 256;
    const int blocks_per_grid = (total_elements + threads_per_block - 1) / threads_per_block;

    avg_pool_3d_kernel<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        input_depth,
        input_height,
        input_width,
        kernel_size,
        stride,
        padding,
        output_depth,
        output_height,
        output_width
    );

    return output;
}
"""

avg_pool_3d_cpp = "torch::Tensor avg_pool_3d_cuda(torch::Tensor input, int kernel_size, int stride, int padding);"

avg_pool_3d_ext = load_inline(
    name='avg_pool_3d',
    cpp_sources=avg_pool_3d_cpp,
    cuda_sources=avg_pool_3d_source,
    functions=['avg_pool_3d_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return avg_pool_3d_ext.avg_pool_3d_cuda(x.contiguous(), self.kernel_size, self.stride, self.padding)
```

Kernel 2 (runtime: 13.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized CUDA kernel for 3D average pooling
avg_pool3d_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void avg_pool3d_kernel(
    const float* input,
    float* output,
    int batch_size,
    int channels,
    int in_depth,
    int in_height,
    int in_width,
    int out_depth,
    int out_height,
    int out_width,
    int kernel_depth,
    int kernel_height,
    int kernel_width,
    int stride_depth,
    int stride_height,
    int stride_width,
    int pad_depth,
    int pad_height,
    int pad_width,
    float inv_kernel_volume
) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total_output_elements = batch_size * channels * out_depth * out_height * out_width;
    if (index >= total_output_elements) return;

    // Compute output indices using faster decomposition
    int idx = index;
    const int w = idx % out_width;
    idx /= out_width;
    const int h = idx % out_height;
    idx /= out_height;
    const int d = idx % out_depth;
    idx /= out_depth;
    const int c = idx % channels;
    const int b = idx / channels;

    // Input window start indices
    const int input_d_start = d * stride_depth - pad_depth;
    const int input_h_start = h * stride_height - pad_height;
    const int input_w_start = w * stride_width - pad_width;

    float sum = 0.0f;

    for (int kd = 0; kd < kernel_depth; ++kd) {
        const int input_d = input_d_start + kd;
        if (input_d < 0 || input_d >= in_depth) continue;
        
        for (int kh = 0; kh < kernel_height; ++kh) {
            const int input_h = input_h_start + kh;
            if (input_h < 0 || input_h >= in_height) continue;
            
            for (int kw = 0; kw < kernel_width; ++kw) {
                const int input_w = input_w_start + kw;
                if (input_w < 0 || input_w >= in_width) continue;
                
                const int input_index = ((b * channels + c) * in_depth + input_d) * in_height * in_width + 
                                      input_h * in_width + input_w;
                sum += input[input_index];
            }
        }
    }

    output[index] = sum * inv_kernel_volume;
}

torch::Tensor avg_pool3d_cuda(
    torch::Tensor input,
    torch::IntArrayRef kernel_size,
    torch::IntArrayRef stride,
    torch::IntArrayRef padding
) {
    input = input.contiguous();
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int in_depth = input.size(2);
    const int in_height = input.size(3);
    const int in_width = input.size(4);

    // Calculate output dimensions
    const int out_depth = (in_depth + 2 * padding[0] - kernel_size[0]) / stride[0] + 1;
    const int out_height = (in_height + 2 * padding[1] - kernel_size[1]) / stride[1] + 1;
    const int out_width = (in_width + 2 * padding[2] - kernel_size[2]) / stride[2] + 1;

    auto output = torch::empty({batch_size, channels, out_depth, out_height, out_width}, input.options());
    const float inv_kernel_volume = 1.0f / (kernel_size[0] * kernel_size[1] * kernel_size[2]);

    const int total_elements = batch_size * channels * out_depth * out_height * out_width;
    const int block_size = 256;
    const int num_blocks = (total_elements + block_size - 1) / block_size;

    avg_pool3d_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        in_depth,
        in_height,
        in_width,
        out_depth,
        out_height,
        out_width,
        kernel_size[0],
        kernel_size[1],
        kernel_size[2],
        stride[0],
        stride[1],
        stride[2],
        padding[0],
        padding[1],
        padding[2],
        inv_kernel_volume
    );

    return output;
}
"""

avg_pool3d_cpp_source = "torch::Tensor avg_pool3d_cuda(torch::Tensor input, torch::IntArrayRef kernel_size, torch::IntArrayRef stride, torch::IntArrayRef padding);"

# Load the optimized CUDA extension
avg_pool3d = load_inline(
    name="avg_pool3d",
    cpp_sources=avg_pool3d_cpp_source,
    cuda_sources=avg_pool3d_source,
    functions=["avg_pool3d_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):
        super().__init__()
        self.kernel_size = (kernel_size, kernel_size, kernel_size)
        self.stride = (stride, stride, stride) if stride is not None else self.kernel_size
        self.padding = (padding, padding, padding)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return avg_pool3d.avg_pool3d_cuda(x.contiguous().cuda(), self.kernel_size, self.stride, self.padding)
```
