You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 91.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define batched matrix multiplication CUDA kernel
bmm_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_SIZE 16
#define TILE_SIZE_K 16

__global__ void batched_bmm_kernel(const float* A, const float* B, float* C, 
                                  int batch_size, int m, int k, int n) {
    __shared__ float As[TILE_SIZE][TILE_SIZE_K];
    __shared__ float Bs[TILE_SIZE_K][TILE_SIZE];

    int batch = blockIdx.z;
    int tile_row = blockIdx.y;
    int tile_col = blockIdx.x;

    int row = tile_row * TILE_SIZE + threadIdx.y;
    int col = tile_col * TILE_SIZE + threadIdx.x;

    float sum = 0.0f;

    for (int t = 0; t < (k + TILE_SIZE_K - 1) / TILE_SIZE_K; ++t) {
        // Load A tile
        int a_col = t * TILE_SIZE_K + threadIdx.x;
        if (row < m && a_col < k) {
            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + a_col];
        } else {
            As[threadIdx.y][threadIdx.x] = 0.0f;
        }

        // Load B tile
        int b_row = t * TILE_SIZE_K + threadIdx.y;
        if (b_row < k && col < n) {
            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + b_row * n + col];
        } else {
            Bs[threadIdx.y][threadIdx.x] = 0.0f;
        }

        __syncthreads();

        // Compute partial sum
        for (int i = 0; i < TILE_SIZE_K; ++i) {
            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];
        }

        __syncthreads();
    }

    if (row < m && col < n) {
        C[batch * m * n + row * n + col] = sum;
    }
}

torch::Tensor batched_bmm_cuda(torch::Tensor A, torch::Tensor B) {
    auto batch_size = A.size(0);
    int m = A.size(1);
    int k = A.size(2);
    int n = B.size(2);

    auto C = torch::zeros({batch_size, m, n}, A.options());

    dim3 block(TILE_SIZE, TILE_SIZE);
    dim3 grid(
        (n + TILE_SIZE - 1) / TILE_SIZE,
        (m + TILE_SIZE - 1) / TILE_SIZE,
        batch_size
    );

    batched_bmm_kernel<<<grid, block>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        batch_size,
        m,
        k,
        n
    );

    return C;
}
"""

bmm_cpp_wrapper = "torch::Tensor batched_bmm_cuda(torch::Tensor A, torch::Tensor B);"

# Load the custom CUDA kernel
batched_bmm = load_inline(
    name='batched_bmm',
    cpp_sources=bmm_cpp_wrapper,
    cuda_sources=bmm_kernel_source,
    functions=['batched_bmm_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return batched_bmm.batched_bmm_cuda(A, B)
```

Kernel 2 (runtime: 99.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

bmm_kernel_code = """
#define TILE_SIZE 32
#define TILE_SIZE_K 32

#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void batched_matmul_kernel(const float* A, const float* B, float* C, int batch_size, int m, int k, int n) {
    extern __shared__ float shared_mem[];
    float* A_shared = shared_mem;
    float* B_shared = &shared_mem[TILE_SIZE * TILE_SIZE_K];

    int batch = blockIdx.x;
    int m_tile = blockIdx.y;
    int n_tile = blockIdx.z;

    int row = threadIdx.y;
    int col = threadIdx.x;

    int m_idx = m_tile * TILE_SIZE + row;
    int n_idx = n_tile * TILE_SIZE + col;

    float sum = 0.0f;

    for (int k_tile = 0; k_tile < (k + TILE_SIZE_K - 1) / TILE_SIZE_K; ++k_tile) {
        int k_idx = k_tile * TILE_SIZE_K;

        // Load A tile
        int A_row = m_idx;
        int A_col = k_idx + col;
        if (A_row < m && A_col < k) {
            A_shared[row * TILE_SIZE_K + col] = A[batch * m * k + A_row * k + A_col];
        } else {
            A_shared[row * TILE_SIZE_K + col] = 0.0f;
        }

        // Load B tile
        int B_row = k_idx + row;
        int B_col = n_idx;
        if (B_row < k && B_col < n) {
            B_shared[row * TILE_SIZE + col] = B[batch * k * n + B_row * n + B_col];
        } else {
            B_shared[row * TILE_SIZE + col] = 0.0f;
        }

        __syncthreads();

        // Compute partial sum
        for (int i = 0; i < TILE_SIZE_K; ++i) {
            sum += A_shared[row * TILE_SIZE_K + i] * B_shared[i * TILE_SIZE + col];
        }

        __syncthreads();
    }

    if (m_idx < m && n_idx < n) {
        C[batch * m * n + m_idx * n + n_idx] = sum;
    }
}

torch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 3, "A must be 3D");
    TORCH_CHECK(B.dim() == 3, "B must be 3D");
    int batch_size = A.size(0);
    int m = A.size(1);
    int k = A.size(2);
    TORCH_CHECK(B.size(0) == batch_size, "Batch size mismatch");
    TORCH_CHECK(B.size(1) == k, "Matrix dimensions mismatch");
    int n = B.size(2);

    auto C = torch::zeros({batch_size, m, n}, A.options());

    dim3 block(TILE_SIZE, TILE_SIZE);
    int m_tiles = (m + TILE_SIZE - 1) / TILE_SIZE;
    int n_tiles = (n + TILE_SIZE - 1) / TILE_SIZE;
    dim3 grid(batch_size, m_tiles, n_tiles);

    size_t shared_mem_size = (TILE_SIZE * TILE_SIZE_K + TILE_SIZE_K * TILE_SIZE) * sizeof(float);
    batched_matmul_kernel<<<grid, block, shared_mem_size>>>(
        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(),
        batch_size, m, k, n
    );

    return C;
}
"""

bmm_cpp_code = "torch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B);"

bmm_module = load_inline(
    name='bmm_module',
    cpp_sources=bmm_cpp_code,
    cuda_sources=bmm_kernel_code,
    functions=['batched_matmul_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.bmm_op = bmm_module.batched_matmul_cuda

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.bmm_op(A, B)
```
