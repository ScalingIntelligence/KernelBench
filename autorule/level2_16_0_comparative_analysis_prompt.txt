You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 28.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused activation CUDA kernel (Mish + Add + Hardtanh + Scale)
fused_activation_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_activation_scale_kernel(const float* input, float* output, int num_elements, float add_value, float scale) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float x = input[idx];
        // Mish: x * tanh(softplus(x))
        float softplus_val = log1pf(expf(x));
        float mish = x * tanhf(softplus_val);
        // Add value
        float after_add = mish + add_value;
        // Hardtanh with min=-1, max=1
        float hardtanh = fmaxf(-1.0f, fminf(after_add, 1.0f));
        // Scale output
        output[idx] = hardtanh * scale;
    }
}

torch::Tensor fused_activation_scale_cuda(torch::Tensor input, float add_value, float scale) {
    auto output = torch::empty_like(input);
    int num_elements = input.numel();
    const int block_size = 256;
    int grid_size = (num_elements + block_size - 1) / block_size;
    
    fused_activation_scale_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        add_value,
        scale
    );
    
    return output;
}
"""

fused_activation_cpp_source = "torch::Tensor fused_activation_scale_cuda(torch::Tensor input, float add_value, float scale);"

# Compile the inline CUDA code
fused_activation = load_inline(
    name='fused_activation',
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=['fused_activation_scale_cuda'],
    verbose=True,
    extra_cflags=['-O2'],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    """
    Optimized model using fused CUDA kernel for post-conv operations
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride, padding, output_padding
        )
        self.add_value = add_value
        self.scale = scale

    def forward(self, x):
        x = self.conv_transpose(x)
        # Use fused kernel for Mish + Add + Hardtanh + Scale
        x = fused_activation.fused_activation_scale_cuda(x, self.add_value, self.scale)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width, device='cuda')]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale]
```

Kernel 2 (runtime: 28.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_mish_add_hardtanh_scale_kernel(
    const float* input,
    float* output,
    int num_elements,
    float add_value,
    float scale
) {
    const int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= num_elements) return;

    float x = input[index];

    // Compute Mish activation
    float sp_x = log1pf(expf(x));
    float mish_val = x * tanhf(sp_x);

    // Apply add_value
    mish_val += add_value;

    // Clamp between -1 and 1 (Hardtanh)
    mish_val = fmaxf(-1.0f, fminf(mish_val, 1.0f));

    // Scale the result
    output[index] = mish_val * scale;
}

torch::Tensor fused_ops_cuda(
    torch::Tensor input,
    float add_value,
    float scale
) {
    auto output = torch::empty_like(input);
    int num_elements = input.numel();

    const int block_size = 256;
    int grid_size = (num_elements + block_size - 1) / block_size;

    fused_mish_add_hardtanh_scale_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        add_value,
        scale
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_ops_cuda(torch::Tensor input, float add_value, float scale);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_ops_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size, stride, padding, output_padding
        )
        self.add_value = add_value
        self.scale = scale

    def forward(self, x):
        x = self.conv_transpose(x)
        return fused_ops.fused_ops_cuda(x, self.add_value, self.scale)
```
