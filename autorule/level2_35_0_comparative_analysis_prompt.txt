You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 14.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused Subtract + HardSwish CUDA kernel
subtract_hardswish_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void subtract_hardswish_kernel(const float* input, float* output, 
                                         float subtract_value, int num_elements) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        const float val = input[idx] - subtract_value;
        const float relu6 = fminf(fmaxf(val + 3.0f, 0.0f), 6.0f);
        output[idx] = val * relu6 * 0.16666667f;  // 1/6
    }
}

torch::Tensor subtract_hardswish_cuda(torch::Tensor input, float subtract_value) {
    auto output = torch::empty_like(input);
    const int num_elements = input.numel();
    constexpr int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;
    
    subtract_hardswish_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        subtract_value,
        num_elements
    );
    return output;
}
"""

subtract_hardswish_cpp = "torch::Tensor subtract_hardswish_cuda(torch::Tensor input, float subtract_value);"

# Fused MaxPool2d + Mish CUDA kernel
maxpool_mish_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>
#include <ATen/cuda/CUDAContext.h>

__global__ void maxpool_mish_kernel(const float* input, float* output,
                                   const int batch_size, const int channels,
                                   const int input_h, const int input_w,
                                   const int pool_size, const int output_h, const int output_w) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int num_elements = batch_size * channels * output_h * output_w;
    if (idx >= num_elements) return;

    // Calculate output indices
    const int n = idx / (channels * output_h * output_w);
    const int c = (idx / (output_h * output_w)) % channels;
    const int h = (idx / output_w) % output_h;
    const int w = idx % output_w;

    // Pooling window boundaries
    const int start_h = h * pool_size;
    const int end_h = min(start_h + pool_size, input_h);
    const int start_w = w * pool_size;
    const int end_w = min(start_w + pool_size, input_w);

    // Find max value in window
    float max_val = -INFINITY;
    for (int i = start_h; i < end_h; ++i) {
        for (int j = start_w; j < end_w; ++j) {
            const int input_idx = ((n * channels + c) * input_h + i) * input_w + j;
            max_val = fmaxf(max_val, input[input_idx]);
        }
    }

    // Mish activation
    const float softplus = log1pf(expf(max_val));
    output[idx] = max_val * tanhf(softplus);
}

torch::Tensor maxpool_mish_cuda(torch::Tensor input, int pool_size) {
    const auto sizes = input.sizes();
    const int batch_size = sizes[0];
    const int channels = sizes[1];
    const int input_h = sizes[2];
    const int input_w = sizes[3];
    
    const int output_h = input_h / pool_size;
    const int output_w = input_w / pool_size;
    
    auto output = torch::empty({batch_size, channels, output_h, output_w}, input.options());
    const int num_elements = output.numel();
    constexpr int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;
    
    maxpool_mish_kernel<<<grid_size, block_size, 0, at::cuda::getCurrentCUDAStream()>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        input_h,
        input_w,
        pool_size,
        output_h,
        output_w
    );
    
    return output;
}
"""

maxpool_mish_cpp = "torch::Tensor maxpool_mish_cuda(torch::Tensor input, int pool_size);"

# Load CUDA extensions with fast math optimizations
subtract_hardswish = load_inline(
    name='subtract_hardswish',
    cpp_sources=subtract_hardswish_cpp,
    cuda_sources=subtract_hardswish_source,
    functions=['subtract_hardswish_cuda'],
    extra_cuda_cflags=['--use_fast_math'],
    verbose=True
)

maxpool_mish = load_inline(
    name='maxpool_mish',
    cpp_sources=maxpool_mish_cpp,
    cuda_sources=maxpool_mish_source,
    functions=['maxpool_mish_cuda'],
    extra_cuda_cflags=['--use_fast_math'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value = subtract_value
        self.pool_kernel_size = pool_kernel_size

    def forward(self, x):
        x = self.conv(x)
        x = subtract_hardswish.subtract_hardswish_cuda(x, self.subtract_value)
        x = maxpool_mish.maxpool_mish_cuda(x, self.pool_kernel_size)
        return x
```

Kernel 2 (runtime: 14.6 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

// Optimized Subtract + HardSwish with vectorized loads and ternary operations
__global__ void subtract_hardswish_kernel(const float* input, float subtract_val, float* output, int num_elements) {
    int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x * 4;
    if (idx + 3 >= num_elements) {
        for (int i = 0; i < 4 && idx + i < num_elements; ++i) {
            float x = input[idx + i] - subtract_val;
            float y = x + 3.0f;
            y = y < 0.0f ? 0.0f : (y > 6.0f ? 6.0f : y);
            output[idx + i] = x * y * (1.0f / 6.0f);
        }
        return;
    }

    float4 in = reinterpret_cast<const float4*>(input + idx)[0];
    float4 out;

    #pragma unroll
    for(int i = 0; i < 4; ++i) {
        float x = (&in.x)[i] - subtract_val;
        float y = x + 3.0f;
        y = y < 0.0f ? 0.0f : (y > 6.0f ? 6.0f : y);
        (&out.x)[i] = x * y * (1.0f / 6.0f);
    }

    reinterpret_cast<float4*>(output + idx)[0] = out;
}

torch::Tensor subtract_hardswish_cuda(torch::Tensor input, float subtract_val) {
    auto output = torch::empty_like(input);
    int num_elements = input.numel();
    const int block_size = 512;  // Increased block size for better occupancy
    int grid_size = (num_elements + block_size * 4 - 1) / (block_size * 4);
    subtract_hardswish_kernel<<<grid_size, block_size>>>(input.data_ptr<float>(), subtract_val, output.data_ptr<float>(), num_elements);
    return output;
}

// Optimized MaxPool2d + Mish with vectorized loads and fast math
__global__ void maxpool_mish_kernel(const float* input, float* output, 
                                  int batch_size, int channels, 
                                  int height, int width) {
    extern __shared__ float smem[];
    
    const int TILE_SIZE = 32;
    const int OUT_TILE = TILE_SIZE / 2;
    
    int n = blockIdx.z / channels;
    int c = blockIdx.z % channels;
    int tile_y = blockIdx.y * TILE_SIZE;
    int tile_x = blockIdx.x * TILE_SIZE;

    int ty = threadIdx.y;
    int tx = threadIdx.x;
    
    // Vectorized load using float2
    for(int dy = 0; dy < 2; ++dy) {
        int y = tile_y + ty * 2 + dy;
        if (y >= height) {
            for(int dx = 0; dx < 2; ++dx) {
                smem[(ty * 2 + dy) * (TILE_SIZE + 1) + (tx * 2 + dx)] = -INFINITY;
            }
            continue;
        }
        
        int x_base = tile_x + tx * 2;
        if (x_base + 1 < width) {
            float2 val = *reinterpret_cast<const float2*>(
                &input[n * channels * height * width + c * height * width + y * width + x_base]
            );
            smem[(ty * 2 + dy) * (TILE_SIZE + 1) + tx * 2] = val.x;
            smem[(ty * 2 + dy) * (TILE_SIZE + 1) + tx * 2 + 1] = val.y;
        } else {
            for(int dx = 0; dx < 2; ++dx) {
                int x = x_base + dx;
                if (x < width) {
                    smem[(ty * 2 + dy) * (TILE_SIZE + 1) + tx * 2 + dx] = 
                        input[n * channels * height * width + c * height * width + y * width + x];
                } else {
                    smem[(ty * 2 + dy) * (TILE_SIZE + 1) + tx * 2 + dx] = -INFINITY;
                }
            }
        }
    }
    __syncthreads();

    // Compute max pooling
    float max_val = -INFINITY;
    for(int dy = 0; dy < 2; ++dy) {
        for(int dx = 0; dx < 2; ++dx) {
            max_val = fmaxf(max_val, smem[(ty * 2 + dy) * (TILE_SIZE + 1) + (tx * 2 + dx)]);
        }
    }

    // Fast Mish approximation using built-in functions
    float sp = log1pf(__expf(max_val));
    float mish_val = max_val * tanhf(sp);

    // Write output
    int out_y = tile_y / 2 + ty;
    int out_x = tile_x / 2 + tx;
    if(out_y < height/2 && out_x < width/2) {
        output[n * channels * (height/2) * (width/2) + c * (height/2) * (width/2) + out_y * (width/2) + out_x] = mish_val;
    }
}

torch::Tensor maxpool_mish_cuda(torch::Tensor input) {
    auto sizes = input.sizes();
    auto output = torch::zeros({sizes[0], sizes[1], sizes[2]/2, sizes[3]/2}, input.options());
    
    const int TILE_SIZE = 32;
    dim3 block(16, 16);
    dim3 grid(
        (sizes[3] + TILE_SIZE - 1) / TILE_SIZE,
        (sizes[2] + TILE_SIZE - 1) / TILE_SIZE,
        sizes[0] * sizes[1]
    );
    
    size_t smem_size = (TILE_SIZE + 1) * (TILE_SIZE + 1) * sizeof(float);
    maxpool_mish_kernel<<<grid, block, smem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        sizes[0], sizes[1], sizes[2], sizes[3]
    );
    
    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor subtract_hardswish_cuda(torch::Tensor input, float subtract_val);
torch::Tensor maxpool_mish_cuda(torch::Tensor input);
"""

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=["subtract_hardswish_cuda", "maxpool_mish_cuda"],
    extra_cuda_cflags=["-O3", "--use_fast_math"],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value = subtract_value
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = x.contiguous()  # Ensure memory layout for optimal kernel performance
        x = self.fused_ops.subtract_hardswish_cuda(x, self.subtract_value)
        x = x.contiguous()  # Maintain contiguous layout between operations
        x = self.fused_ops.maxpool_mish_cuda(x)
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height = width = 128
kernel_size = 3
subtract_value = 0.5
pool_kernel_size = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size]
```
