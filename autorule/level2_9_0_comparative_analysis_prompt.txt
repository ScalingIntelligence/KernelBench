You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.11 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom CUDA kernel for fused subtract-multiply-relu operation
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_sub_mul_relu_kernel(const float* input, float* output, float sub_val, float mul_val, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = (input[idx] - sub_val) * mul_val;
        output[idx] = fmaxf(val, 0.0f);
    }
}

torch::Tensor fused_sub_mul_relu_cuda(torch::Tensor input, float sub_val, float mul_val) {
    auto output = torch::empty_like(input);
    int num_elements = input.numel();
    const int block_size = 256;
    int grid_size = (num_elements + block_size - 1) / block_size;

    fused_sub_mul_relu_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        sub_val,
        mul_val,
        num_elements
    );

    return output;
}
"""

fused_ops_cpp_source = "torch::Tensor fused_sub_mul_relu_cuda(torch::Tensor input, float sub_val, float mul_val);"

# Compile the inline CUDA code
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=['fused_sub_mul_relu_cuda'],
    verbose=True,
    extra_cflags=[],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, subtract_value, multiply_value):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.subtract_value = subtract_value
        self.multiply_value = multiply_value

    def forward(self, x):
        x = self.linear(x)
        x = fused_ops.fused_sub_mul_relu_cuda(x, self.subtract_value, self.multiply_value)
        return x
```

Kernel 2 (runtime: 7.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused subtract-multiply-relu CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_op_kernel(const float* input, float* output, float sub_val, float mul_val, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = (input[idx] - sub_val) * mul_val;
        output[idx] = fmaxf(val, 0.0f);
    }
}

torch::Tensor fused_op_cuda(torch::Tensor input, float sub_val, float mul_val) {
    auto output = torch::empty_like(input);
    int num_elements = input.numel();
    
    const int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;
    
    fused_op_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        sub_val,
        mul_val,
        num_elements
    );
    
    return output;
}
"""

fused_cpp_source = "torch::Tensor fused_op_cuda(torch::Tensor input, float sub_val, float mul_val);"

# Load the custom CUDA operation
fused_op = load_inline(
    name="fused_op",
    cpp_sources=fused_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=["fused_op_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=["-lcudart"]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, subtract_value, multiply_value):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.subtract_value = subtract_value
        self.multiply_value = multiply_value
        self.fused_op = fused_op

    def forward(self, x):
        x = self.linear(x)
        x = self.fused_op.fused_op_cuda(x, self.subtract_value, self.multiply_value)
        return x

def get_init_inputs():
    return [8192, 8192, 2.0, 1.5]
```
