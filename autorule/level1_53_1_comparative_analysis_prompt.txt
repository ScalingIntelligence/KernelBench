You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 21.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

min_reduction_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cfloat>

inline __device__ float warpReduceMin(float val) {
    for (int offset = 16; offset > 0; offset /= 2) 
        val = fminf(val, __shfl_down_sync(0xffffffff, val, offset));
    return val;
}

__global__ void min_reduction_kernel(const float* input, float* output, 
                                    int B, int reduce_size, int other_size,
                                    int input_stride0, int input_stride_reduce,
                                    int input_stride_pos, int output_stride) {
    extern __shared__ float sdata[];

    const int output_idx = blockIdx.x;
    const int b = output_idx / other_size;
    const int pos = output_idx % other_size;
    
    if (b >= B || pos >= other_size) return;

    const float* input_ptr = input + b * input_stride0 + pos * input_stride_pos;
    const int tid = threadIdx.x;
    const int warp_size = 32;
    const int warp_id = tid / warp_size;
    const int lane_id = tid % warp_size;

    float local_min = FLT_MAX;
    for(int i = tid; i < reduce_size; i += blockDim.x) {
        float val = input_ptr[i * input_stride_reduce];
        local_min = fminf(local_min, val);
    }

    // Warp-level reduction
    local_min = warpReduceMin(local_min);

    // Store warp results to shared memory
    if (lane_id == 0) {
        sdata[warp_id] = local_min;
    }
    __syncthreads();

    // Final reduction using first warp
    if (warp_id == 0) {
        float final_min = (tid < blockDim.x / warp_size) ? sdata[lane_id] : FLT_MAX;
        final_min = warpReduceMin(final_min);
        if (tid == 0) {
            output[b * output_stride + pos] = final_min;
        }
    }
}

torch::Tensor min_reduction_cuda(torch::Tensor input, int dim) {
    const int B = input.size(0);
    const int reduce_size = input.size(dim);
    const int other_size = input.numel() / (B * reduce_size);
    
    auto output_shape = input.sizes().vec();
    output_shape.erase(output_shape.begin() + dim);
    auto output = torch::empty(output_shape, input.options());
    
    const int block_size = 256;
    const int num_blocks = B * other_size;
    const size_t shared_mem_size = (block_size / 32) * sizeof(float);
    
    int input_stride0, input_stride_reduce, input_stride_pos;
    if (dim == 1) {
        input_stride0 = input.stride(0);
        input_stride_reduce = input.stride(1);
        input_stride_pos = input.stride(2);
    } else {
        input_stride0 = input.stride(0);
        input_stride_reduce = input.stride(2);
        input_stride_pos = input.stride(1);
    }
    
    min_reduction_kernel<<<num_blocks, block_size, shared_mem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B,
        reduce_size,
        other_size,
        input_stride0,
        input_stride_reduce,
        input_stride_pos,
        output.stride(0)
    );
    
    return output;
}
"""

min_reduction_cpp_source = "torch::Tensor min_reduction_cuda(torch::Tensor input, int dim);"

min_reduction = load_inline(
    name="min_reduction",
    cpp_sources=min_reduction_cpp_source,
    cuda_sources=min_reduction_source,
    functions=["min_reduction_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.min_reduction = min_reduction

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if x.is_cuda:
            return self.min_reduction.min_reduction_cuda(x, self.dim)
        else:
            return torch.min(x, dim=self.dim)[0]
```

Kernel 2 (runtime: 14.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

min_reduction_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cfloat>

__global__ void min_reduction_kernel(
    const float* input,
    float* output,
    int reduction_dim,
    int dim0, int dim1, int dim2,
    int stride0, int stride1, int stride2
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int output_size;

    if (reduction_dim == 0) {
        output_size = dim1 * dim2;
        if (idx >= output_size) return;
        int dim1_idx = idx / dim2;
        int dim2_idx = idx % dim2;
        float min_val = FLT_MAX;
        for (int b = 0; b < dim0; ++b) {
            int input_idx = b * stride0 + dim1_idx * stride1 + dim2_idx * stride2;
            min_val = fminf(min_val, input[input_idx]);
        }
        output[idx] = min_val;
    } else if (reduction_dim == 1) {
        output_size = dim0 * dim2;
        if (idx >= output_size) return;
        int dim0_idx = idx / dim2;
        int dim2_idx = idx % dim2;
        float min_val = FLT_MAX;
        for (int m = 0; m < dim1; ++m) {
            int input_idx = dim0_idx * stride0 + m * stride1 + dim2_idx * stride2;
            min_val = fminf(min_val, input[input_idx]);
        }
        output[idx] = min_val;
    } else if (reduction_dim == 2) {
        output_size = dim0 * dim1;
        if (idx >= output_size) return;
        int dim0_idx = idx / dim1;
        int dim1_idx = idx % dim1;
        float min_val = FLT_MAX;
        for (int n = 0; n < dim2; ++n) {
            int input_idx = dim0_idx * stride0 + dim1_idx * stride1 + n * stride2;
            min_val = fminf(min_val, input[input_idx]);
        }
        output[idx] = min_val;
    }
}

torch::Tensor min_reduction_cuda(torch::Tensor x, int64_t dim) {
    TORCH_CHECK(x.dim() == 3, "Input tensor must be 3D");
    auto sizes = x.sizes().vec();
    int64_t reduction_dim = dim;
    TORCH_CHECK(reduction_dim >=0 && reduction_dim < 3, "dim must be 0, 1, or 2");

    // Compute output sizes
    sizes.erase(sizes.begin() + reduction_dim);
    auto out = torch::empty(sizes, x.options());

    int dim0 = x.size(0);
    int dim1 = x.size(1);
    int dim2 = x.size(2);

    auto strides = x.strides().vec();
    int stride0 = strides[0];
    int stride1 = strides[1];
    int stride2 = strides[2];

    int output_size = out.numel();

    const int block_size = 256;
    const int num_blocks = (output_size + block_size - 1) / block_size;

    min_reduction_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        (int)reduction_dim,
        dim0, dim1, dim2,
        stride0, stride1, stride2
    );

    return out;
}
"""

min_reduction_cpp_source = "torch::Tensor min_reduction_cuda(torch::Tensor x, int64_t dim);"

min_reduction = load_inline(
    name="min_reduction",
    cpp_sources=min_reduction_cpp_source,
    cuda_sources=min_reduction_source,
    functions=["min_reduction_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super(ModelNew, self).__init__()
        self.dim = dim
        self.min_reduction = min_reduction

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.min_reduction.min_reduction_cuda(x, self.dim)
```
