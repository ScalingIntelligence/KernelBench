You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 6.71 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized fused bias addition and summation kernel with shared memory reduction
fused_bias_sum_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_bias_sum_kernel(const float* x, float sum_bias, float* out, int N, int C) {
    const int batch_idx = blockIdx.x;
    if (batch_idx >= N) return;

    __shared__ float smem[16];
    const int tid = threadIdx.x;
    
    // Coalesced global memory load
    if (tid < C) {
        smem[tid] = x[batch_idx * C + tid];
    } else {
        smem[tid] = 0.0f;
    }
    __syncthreads();

    // Parallel reduction with sequential addressing
    for (int s = 8; s > 0; s >>= 1) {
        if (tid < s) {
            smem[tid] += smem[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        out[batch_idx] = smem[0] + sum_bias;
    }
}

torch::Tensor fused_bias_sum_cuda(torch::Tensor x, torch::Tensor sum_bias) {
    int N = x.size(0);
    int C = x.size(1);
    auto out = torch::zeros({N, 1, 1, 1}, x.options());

    const int threads_per_block = 16;
    fused_bias_sum_kernel<<<N, threads_per_block>>>(
        x.data_ptr<float>(),
        sum_bias.item<float>(),
        out.data_ptr<float>(),
        N,
        C
    );

    return out;
}
"""

fused_bias_sum_cpp_source = "torch::Tensor fused_bias_sum_cuda(torch::Tensor x, torch::Tensor sum_bias);"

# Compile the inline CUDA code
fused_bias_sum = load_inline(
    name="fused_bias_sum",
    cpp_sources=fused_bias_sum_cpp_source,
    cuda_sources=fused_bias_sum_source,
    functions=["fused_bias_sum_cuda"],
    verbose=True,
)

# Custom autograd function to handle gradient computation
class FusedBiasSumFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, sum_bias):
        ctx.save_for_backward(sum_bias)
        ctx.x_shape = x.shape
        output = fused_bias_sum.fused_bias_sum_cuda(x, sum_bias)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        sum_bias, = ctx.saved_tensors
        grad_x = grad_output.view(-1, 1, 1, 1, 1).expand(ctx.x_shape)
        grad_sum_bias = grad_output.sum()
        return grad_x, grad_sum_bias

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):
        super(ModelNew, self).__init__()
        # Initialize convolution with absorbed division
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        with torch.no_grad():
            self.conv.weight.div_(divisor)
            if self.conv.bias is not None:
                self.conv.bias.div_(divisor)
        
        self.max_pool = nn.MaxPool3d(pool_size)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.sum_dim = sum_dim

    def forward(self, x):
        x = self.conv(x)
        x = self.max_pool(x)
        x = self.global_avg_pool(x)  # Shape: [N, C, 1, 1, 1]
        sum_bias = torch.sum(self.bias)
        x = FusedBiasSumFunction.apply(x, sum_bias)
        return x
```

Kernel 2 (runtime: 6.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized fused kernel with vectorized loads and warp shuffle reduction
fused_bias_sum_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_bias_sum_kernel(const float* x, float sum_bias, float* out, int N, int C) {
    const int batch_idx = blockIdx.x;
    if (batch_idx >= N) return;

    const int tid = threadIdx.x;
    const int num_threads = blockDim.x;
    float val = 0.0f;

    // Vectorized load parameters
    const int vec_size = 4;
    const int max_c_vector = (C / vec_size) * vec_size;

    // Process vectorized elements
    for (int c = tid * vec_size; c < max_c_vector; c += num_threads * vec_size) {
        float4 vec = *reinterpret_cast<const float4*>(x + batch_idx * C + c);
        val += vec.x + vec.y + vec.z + vec.w;
    }

    // Process remaining elements
    for (int c = max_c_vector + tid; c < C; c += num_threads) {
        val += x[batch_idx * C + c];
    }

    // Warp-level reduction using shuffle instructions
    #pragma unroll
    for (int offset = 16; offset > 0; offset >>= 1) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }

    // First thread in warp writes result
    if (tid == 0) {
        out[batch_idx] = val + sum_bias;
    }
}

torch::Tensor fused_bias_sum_cuda(torch::Tensor x, torch::Tensor sum_bias) {
    int N = x.size(0);
    int C = x.size(1);
    auto out = torch::zeros({N, 1, 1, 1}, x.options());

    // Use 32 threads per block (1 warp) for optimal occupancy
    const int threads_per_block = 32;
    fused_bias_sum_kernel<<<N, threads_per_block>>>(
        x.data_ptr<float>(),
        sum_bias.item<float>(),
        out.data_ptr<float>(),
        N,
        C
    );

    return out;
}
"""

fused_bias_sum_cpp_source = "torch::Tensor fused_bias_sum_cuda(torch::Tensor x, torch::Tensor sum_bias);"

# Compile the inline CUDA code
fused_bias_sum = load_inline(
    name="fused_bias_sum",
    cpp_sources=fused_bias_sum_cpp_source,
    cuda_sources=fused_bias_sum_source,
    functions=["fused_bias_sum_cuda"],
    verbose=True,
)

# Custom autograd function with optimized backward
class FusedBiasSumFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, sum_bias):
        ctx.save_for_backward(sum_bias)
        ctx.x_shape = x.shape
        return fused_bias_sum.fused_bias_sum_cuda(x, sum_bias)

    @staticmethod
    def backward(ctx, grad_output):
        sum_bias, = ctx.saved_tensors
        grad_x = grad_output.view(-1, 1, 1, 1, 1).expand(ctx.x_shape)
        grad_sum_bias = grad_output.sum()
        return grad_x, grad_sum_bias

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):
        super(ModelNew, self).__init__()
        # Initialize convolution with absorbed division
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        with torch.no_grad():
            self.conv.weight.div_(divisor)
            if self.conv.bias is not None:
                self.conv.bias.div_(divisor)
        
        self.max_pool = nn.MaxPool3d(pool_size)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.sum_dim = sum_dim

    def forward(self, x):
        x = self.conv(x)
        x = self.max_pool(x)
        x = self.global_avg_pool(x)  # Shape: [N, C, 1, 1, 1]
        sum_bias = torch.sum(self.bias)
        x = FusedBiasSumFunction.apply(x.contiguous(), sum_bias)
        return x
```
