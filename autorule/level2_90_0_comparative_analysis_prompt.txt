You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 24.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom CUDA kernel for fused LeakyReLU, Add, Clamp, GELU operations
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_ops_kernel(const float* input, const float* sum_tensor, float* output, 
                                int batch_size, int channels, int depth, int height, int width) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total_elements = batch_size * channels * depth * height * width;
    if (index >= total_elements) return;

    // Calculate channel index from linear index
    int channel_idx = (index / (depth * height * width)) % channels;
    
    float val = input[index];
    
    // LeakyReLU (alpha=0.2)
    val = fmaxf(0.2f * val, val);
    
    // Add channel-specific bias
    val += sum_tensor[channel_idx];
    
    // Clamp between -1 and 1
    val = fmaxf(-1.0f, fminf(val, 1.0f));
    
    // Approximate GELU (x * sigmoid(1.702x))
    float gelu_input = val;
    float sigmoid = 1.0f / (1.0f + expf(-1.702f * gelu_input));
    val = gelu_input * sigmoid;
    
    output[index] = val;
}

torch::Tensor fused_ops_cuda(torch::Tensor input, torch::Tensor sum_tensor) {
    TORCH_CHECK(input.dim() == 5, "Input must be 5D tensor");
    TORCH_CHECK(sum_tensor.dim() == 4, "Sum tensor must be 4D (channels, 1, 1, 1)");
    
    auto output = torch::empty_like(input);
    int total_elements = input.numel();
    
    const int block_size = 256;
    int num_blocks = (total_elements + block_size - 1) / block_size;
    
    fused_ops_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        sum_tensor.data_ptr<float>(),
        output.data_ptr<float>(),
        input.size(0),  // batch_size
        input.size(1),  // channels
        input.size(2),  // depth
        input.size(3),  // height
        input.size(4)   // width
    );
    
    return output;
}
"""

fused_ops_cpp_source = "torch::Tensor fused_ops_cuda(torch::Tensor input, torch::Tensor sum_tensor);"

# Compile the inline CUDA code
fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=["fused_ops_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, sum_tensor_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.sum_tensor = nn.Parameter(torch.randn(sum_tensor_shape))
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_ops.fused_ops_cuda(x, self.sum_tensor)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, sum_tensor_shape]

# Constants from original implementation
batch_size = 128
in_channels = 8
out_channels = 64
depth, height, width = 16, 64, 64
kernel_size = 3
sum_tensor_shape = (out_channels, 1, 1, 1)
```

Kernel 2 (runtime: 24.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_operations_kernel(
    const float* x,
    const float* sum,
    float* output,
    int total_elements,
    int channels,
    int hw_d
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;

    int c = (idx / hw_d) % channels; // Correctly calculate channel index
    float val = x[idx];

    // Apply LeakyReLU
    val = val > 0 ? val : val * 0.2f;

    // Add sum for current channel
    val += sum[c];

    // Clamp between -1 and 1
    val = fmaxf(fminf(val, 1.0f), -1.0f);

    // Compute GELU approximation
    float y = val * 0.5f * (1.0f + tanhf(0.7978845608f * (val + 0.044715f * val * val * val)));
    output[idx] = y;
}

torch::Tensor fused_operations_cuda(torch::Tensor x, torch::Tensor sum) {
    auto output = torch::empty_like(x);
    int total_elements = x.numel();
    int channels = x.size(1);
    int hw_d = x.size(2) * x.size(3) * x.size(4);

    const int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;

    fused_operations_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        sum.data_ptr<float>(),
        output.data_ptr<float>(),
        total_elements,
        channels,
        hw_d
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_operations_cuda(torch::Tensor x, torch::Tensor sum);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_operations_cuda'],
    verbose=True,
    extra_cflags=[],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, sum_tensor_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.sum_tensor = nn.Parameter(torch.randn(sum_tensor_shape))
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_ops.fused_operations_cuda(x, self.sum_tensor)
        return x
```
