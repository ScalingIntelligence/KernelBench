You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.22 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused bias addition, division, and Swish activation
bias_divide_swish_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void bias_divide_swish_kernel(const float* input, float* output, const float* bias, float divide_value, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = input[idx];
        val += *bias;
        val /= divide_value;
        val *= 1.0f / (1.0f + expf(-val));  // Sigmoid
        output[idx] = val;
    }
}

torch::Tensor bias_divide_swish_cuda(torch::Tensor input, torch::Tensor bias, float divide_value) {
    auto output = torch::empty_like(input);
    int num_elements = input.numel();
    const int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;
    const float* bias_ptr = bias.data_ptr<float>();
    bias_divide_swish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), output.data_ptr<float>(), bias_ptr, divide_value, num_elements
    );
    return output;
}
"""

bias_divide_swish_cpp_source = "torch::Tensor bias_divide_swish_cuda(torch::Tensor input, torch::Tensor bias, float divide_value);"

bias_divide_swish = load_inline(
    name='bias_divide_swish',
    cpp_sources=bias_divide_swish_cpp_source,
    cuda_sources=bias_divide_swish_source,
    functions=['bias_divide_swish_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.divide_value = divide_value
        self.custom_op = bias_divide_swish

    def forward(self, x):
        x = self.matmul(x)
        x = self.bn(x)
        x = self.custom_op.bias_divide_swish_cuda(x, self.bias, self.divide_value)
        return x
```

Kernel 2 (runtime: 7.39 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel for fused bias add, divide, and Swish
elementwise_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void elementwise_ops_kernel(const float* x, float bias, float inv_divide_value, float* out, int size) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        // Fused operations with fast math
        const float val = (x[idx] + bias) * inv_divide_value;
        const float sigmoid_val = 1.0f / (1.0f + __expf(-val));
        out[idx] = __fmul_rn(val, sigmoid_val);
    }
}

torch::Tensor elementwise_ops_cuda(torch::Tensor x, torch::Tensor bias, float divide_value) {
    auto out = torch::empty_like(x);
    const int size = x.numel();
    constexpr int block_size = 512;  // Optimal block size for modern GPUs
    const int num_blocks = (size + block_size - 1) / block_size;

    const float bias_val = bias.item<float>();
    const float inv_divide_value = 1.0f / divide_value;

    elementwise_ops_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias_val,
        inv_divide_value,
        out.data_ptr<float>(),
        size
    );

    return out;
}
"""

elementwise_ops_cpp_source = "torch::Tensor elementwise_ops_cuda(torch::Tensor x, torch::Tensor bias, float divide_value);"

# Load the optimized kernel with fast math
elementwise_ops = load_inline(
    name='elementwise_ops',
    cpp_sources=elementwise_ops_cpp_source,
    cuda_sources=elementwise_ops_source,
    functions=['elementwise_ops_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):
        super().__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.divide_value = divide_value
        self.elementwise_ops = elementwise_ops

    def forward(self, x):
        x = self.matmul(x)
        x = self.bn(x)
        x = self.elementwise_ops.elementwise_ops_cuda(x, self.bias, self.divide_value)
        return x
```
