You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 65.4 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for Frobenius norm normalization
frob_norm_cuda_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 256
#define THREADS_PER_BLOCK 256

__global__ void sum_squares_reduction(const float* input, float* partial_sums, int num_elements) {
    extern __shared__ float shared_mem[];
    int tid = threadIdx.x;
    int global_idx = blockIdx.x * blockDim.x + tid;

    float val = 0.0f;
    if (global_idx < num_elements) {
        val = input[global_idx] * input[global_idx];
    }

    shared_mem[tid] = val;
    __syncthreads();

    for(int stride = blockDim.x/2; stride > 0; stride >>=1) {
        if(tid < stride) {
            shared_mem[tid] += shared_mem[tid+stride];
        }
        __syncthreads();
    }

    if(tid == 0) {
        partial_sums[blockIdx.x] = shared_mem[0];
    }
}

__global__ void sum_partial_sums(float* partial_sums, float* total_sum, int num_blocks) {
    extern __shared__ float shared_mem[];
    int tid = threadIdx.x;
    int block_id = blockIdx.x;

    if(block_id >= num_blocks) return;

    float val = partial_sums[block_id];
    shared_mem[tid] = val;
    __syncthreads();

    for(int stride = blockDim.x/2; stride > 0; stride >>=1) {
        if(tid < stride) {
            shared_mem[tid] += shared_mem[tid+stride];
        }
        __syncthreads();
    }

    if(tid == 0) {
        atomicAdd(total_sum, shared_mem[0]);
    }
}

__global__ void normalize_kernel(const float* input, float* output, float norm_inv, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if(idx < num_elements) {
        output[idx] = input[idx] * norm_inv;
    }
}

torch::Tensor frob_norm_normalize_cuda(torch::Tensor input) {
    TORCH_CHECK(input.is_cuda(), "Input must be a CUDA tensor");
    int64_t num_elements = input.numel();
    
    auto partial_sums = torch::empty((num_elements + THREADS_PER_BLOCK - 1)/THREADS_PER_BLOCK, input.options());
    auto total_sum = torch::zeros({}, input.options());

    int block_size = THREADS_PER_BLOCK;
    int grid_size = (num_elements + block_size - 1)/block_size;
    
    sum_squares_reduction<<<grid_size, block_size, block_size*sizeof(float)>>>(
        input.data_ptr<float>(),
        partial_sums.data_ptr<float>(),
        num_elements
    );
    cudaDeviceSynchronize();

    int partial_grid_size = partial_sums.size(0);
    sum_partial_sums<<<partial_grid_size, block_size, block_size*sizeof(float)>>>(
        partial_sums.data_ptr<float>(),
        total_sum.data_ptr<float>(),
        partial_grid_size
    );
    cudaDeviceSynchronize();

    float sum_val = total_sum.item<float>();
    float norm = sqrtf(sum_val);
    float norm_inv = 1.0f / norm;

    int normalize_grid_size = (num_elements + block_size - 1)/block_size;
    normalize_kernel<<<normalize_grid_size, block_size>>>(
        input.data_ptr<float>(),
        input.data_ptr<float>(),
        norm_inv,
        num_elements
    );

    return input;
}
"""

frob_norm_cpp_wrapper = """
torch::Tensor frob_norm_normalize_cuda(torch::Tensor input);
"""

# Load the custom CUDA extension
frob_norm_ext = load_inline(
    name='frob_norm',
    cpp_sources=frob_norm_cpp_wrapper,
    cuda_sources=frob_norm_cuda_code,
    functions=['frob_norm_normalize_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.frob_norm_cuda = frob_norm_ext

    def forward(self, x):
        return self.frob_norm_cuda.frob_norm_normalize_cuda(x.contiguous())
```

Kernel 2 (runtime: 79.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void compute_frobenius_norm(float* input, float* output, int num_elements) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int bid = blockIdx.x;
    int stride = blockDim.x * gridDim.x;
    double sum_sq = 0.0;

    for (int i = tid; i < num_elements; i += stride) {
        float val = input[i];
        sum_sq += static_cast<double>(val) * val;
    }

    sdata[tid] = static_cast<float>(sum_sq);
    __syncthreads();

    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(output, sdata[0]);
    }
}

__global__ void apply_division(float* input, float norm_inv, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        input[idx] *= norm_inv;
    }
}

torch::Tensor fused_forward_cuda(torch::Tensor x) {
    auto device = x.device().type();
    TORCH_CHECK(device == torch::kCUDA, "Input must be CUDA tensor");

    auto ones = torch::ones({}, x.options());
    auto output = torch::empty({}, x.options());

    int num_elements = x.numel();
    const int threads_per_block = 1024;
    const int blocks_per_grid = (threads_per_block * ((num_elements + threads_per_block - 1) / threads_per_block)) / threads_per_block;

    compute_frobenius_norm<<<blocks_per_grid, threads_per_block, threads_per_block * sizeof(float)>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    cudaDeviceSynchronize();

    float norm = output.item<float>();
    float norm_inv = 1.0f / norm;

    const int blocks_div = (num_elements + threads_per_block - 1) / threads_per_block;
    apply_division<<<blocks_div, threads_per_block>>>(
        x.data_ptr<float>(),
        norm_inv,
        num_elements
    );

    return x;
}
"""

cpp_wrapper = "torch::Tensor fused_forward_cuda(torch::Tensor x);"

fused_op = load_inline(
    name='fused_op',
    cpp_sources=cpp_wrapper,
    cuda_sources=fused_kernel_code,
    functions=['fused_forward_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.fused_forward = fused_op.fused_forward_cuda

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.fused_forward(x.contiguous())
```
