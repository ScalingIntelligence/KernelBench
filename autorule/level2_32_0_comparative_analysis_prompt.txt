You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 21.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused scaling and channel-wise minimum
scale_min_kernel = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cfloat>

__global__ void scale_min_kernel(const float* input, float* output, 
                               int channels, int height, int width, 
                               int batch_size, float scale_factor) {
    // Calculate spatial position and batch index
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * height * width) return;
    
    const int b = idx / (height * width);
    const int hw = idx % (height * width);
    const int h = hw / width;
    const int w = hw % width;

    float min_val = FLT_MAX;
    for (int c = 0; c < channels; ++c) {
        // Input index calculation [N, C, H, W]
        const int input_idx = b * channels * height * width + 
                             c * height * width + 
                             h * width + w;
        float val = input[input_idx] * scale_factor;
        min_val = fminf(min_val, val);
    }
    
    // Output index calculation [N, 1, H, W]
    const int output_idx = b * height * width + h * width + w;
    output[output_idx] = min_val;
}

torch::Tensor scale_min_cuda(torch::Tensor input, float scale_factor) {
    auto sizes = input.sizes();
    int batch_size = sizes[0];
    int channels = sizes[1];
    int height = sizes[2];
    int width = sizes[3];
    
    auto output = torch::empty({batch_size, 1, height, width}, input.options());
    
    int total_elements = batch_size * height * width;
    const int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    scale_min_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        channels,
        height,
        width,
        batch_size,
        scale_factor
    );
    
    return output;
}
"""

scale_min_cpp = "torch::Tensor scale_min_cuda(torch::Tensor input, float scale_factor);"

# Load the custom CUDA operation
scale_min_op = load_inline(
    name='scale_min_op',
    cpp_sources=scale_min_cpp,
    cuda_sources=scale_min_kernel,
    functions=['scale_min_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    """
    Optimized model using fused scale+min CUDA kernel
    """
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor

    def forward(self, x):
        x = self.conv(x)
        x = scale_min_op.scale_min_cuda(x.contiguous(), self.scale_factor)
        return x
```

Kernel 2 (runtime: 21.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel with coalesced memory access and fast math
scale_min_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void scale_min_kernel(const float* input, float* output, float scale_factor, 
                                int batch_size, int channels, int height, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int spatial_size = height * width;
    int total_positions = batch_size * spatial_size;
    
    if (idx < total_positions) {
        int b = idx / spatial_size;
        int hw = idx % spatial_size;
        int h = hw / width;
        int w = hw % width;
        
        float min_val = INFINITY;
        for (int c = 0; c < channels; ++c) {
            // Coalesced memory access pattern using NCHW layout
            int input_idx = ((b * channels + c) * height + h) * width + w;
            float val = __ldg(&input[input_idx]) * scale_factor;
            min_val = fminf(min_val, val);
        }
        
        output[b * spatial_size + h * width + w] = min_val;
    }
}

torch::Tensor scale_min_cuda(torch::Tensor input, float scale_factor) {
    auto sizes = input.sizes();
    int batch_size = sizes[0];
    int channels = sizes[1];
    int height = sizes[2];
    int width = sizes[3];
    
    auto output = torch::empty({batch_size, 1, height, width}, input.options());
    
    int total_elements = batch_size * height * width;
    const int block_size = 256;  // Optimal for memory-bound kernels
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    scale_min_kernel<<<grid_size, block_size>>>(
        input.contiguous().data_ptr<float>(),
        output.data_ptr<float>(),
        scale_factor,
        batch_size,
        channels,
        height,
        width
    );
    
    return output;
}
"""

scale_min_cpp = "torch::Tensor scale_min_cuda(torch::Tensor input, float scale_factor);"

scale_min = load_inline(
    name="scale_min",
    cpp_sources=scale_min_cpp,
    cuda_sources=scale_min_source,
    functions=["scale_min_cuda"],
    verbose=True,
    extra_cflags=["-use_fast_math"],  # Enable aggressive math optimizations
    extra_ldflags=[""]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        # Maintain original convolution parameters (no padding)
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor

    def forward(self, x):
        x = self.conv(x)
        x = scale_min.scale_min_cuda(x, self.scale_factor)
        return x
```
