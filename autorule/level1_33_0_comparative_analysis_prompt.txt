You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 18.8 ms):
```
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for optimized batch normalization inference
cuda_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename scalar_t>
__global__ void batchnorm_inference_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ scale,
    const scalar_t* __restrict__ shift,
    scalar_t* __restrict__ output,
    int N, int C, int H, int W) {
    
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int total_elements = N * C * H * W;
    if (idx >= total_elements) return;
    
    const int spatial_size = H * W;
    const int c = (idx / spatial_size) % C;
    
    output[idx] = input[idx] * scale[c] + shift[c];
}

torch::Tensor batchnorm_inference_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor shift) {
    
    auto output = torch::empty_like(input);
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);
    
    const int total_elements = N * C * H * W;
    const int block_size = 256;
    const int num_blocks = (total_elements + block_size - 1) / block_size;
    
    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "batchnorm_inference", ([&] {
        batchnorm_inference_kernel<scalar_t><<<num_blocks, block_size>>>(
            input.data_ptr<scalar_t>(),
            scale.data_ptr<scalar_t>(),
            shift.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            N, C, H, W
        );
    }));
    
    return output;
}
"""

cpp_code = "torch::Tensor batchnorm_inference_cuda(torch::Tensor input, torch::Tensor scale, torch::Tensor shift);"

# Load the custom CUDA kernel
custom_bn = load_inline(
    name='custom_bn',
    cpp_sources=cpp_code,
    cuda_sources=cuda_code,
    functions=['batchnorm_inference_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, num_features: int):
        super(ModelNew, self).__init__()
        self.num_features = num_features
        self.eps = 1e-5
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.training:
            # Fallback to PyTorch for training
            return F.batch_norm(
                x, self.running_mean, self.running_var, 
                self.gamma, self.beta, training=True, 
                momentum=0.1, eps=self.eps
            )
        else:
            # Use optimized inference kernel with coalesced memory access
            scale = self.gamma / torch.sqrt(self.running_var + self.eps)
            shift = self.beta - scale * self.running_mean
            return custom_bn.batchnorm_inference_cuda(x, scale, shift)
```

Kernel 2 (runtime: 18.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernels for BatchNorm with vectorized memory access and warp shuffles
batchnorm_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__inline__ __device__ float warp_reduce(float val) {
    for (int offset = 16; offset > 0; offset /= 2) 
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void compute_sum_sq_kernel(
    const float* input,
    float* sum,
    float* sum_sq,
    int batch_size,
    int num_features,
    int h,
    int w
) {
    const int c = blockIdx.x;
    const int tid = threadIdx.x;
    const int block_size = blockDim.x;
    const int elements_per_channel = batch_size * h * w;

    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;

    // Vectorized processing (4 elements per load)
    const int vec_size = 4;
    const int num_vectors = elements_per_channel / vec_size;
    const int remainder_start = num_vectors * vec_size;

    for (int i = tid; i < num_vectors; i += block_size) {
        const int base_idx = i * vec_size;
        const int n = base_idx / (h * w);
        const int spatial_idx = base_idx % (h * w);
        const int hi = spatial_idx / w;
        const int wi = spatial_idx % w;
        const int input_idx = n * num_features * h * w + c * h * w + hi * w + wi;

        const float4 vals = *reinterpret_cast<const float4*>(input + input_idx);
        local_sum += vals.x + vals.y + vals.z + vals.w;
        local_sum_sq += vals.x*vals.x + vals.y*vals.y + vals.z*vals.z + vals.w*vals.w;
    }

    // Process remaining elements
    for (int idx = remainder_start + tid; idx < elements_per_channel; idx += block_size) {
        const int n = idx / (h * w);
        const int spatial_idx = idx % (h * w);
        const int hi = spatial_idx / w;
        const int wi = spatial_idx % w;
        const int input_idx = n * num_features * h * w + c * h * w + hi * w + wi;
        
        const float val = input[input_idx];
        local_sum += val;
        local_sum_sq += val * val;
    }

    // Warp-level reduction
    float warp_sum = warp_reduce(local_sum);
    float warp_sum_sq = warp_reduce(local_sum_sq);

    if (tid % 32 == 0) {
        atomicAdd(&sum[c], warp_sum);
        atomicAdd(&sum_sq[c], warp_sum_sq);
    }
}

__global__ void batchnorm_forward_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    const float* mean,
    const float* var,
    float* output,
    float eps,
    int num_features,
    int batch_size,
    int h,
    int w
) {
    const int n = blockIdx.x;
    const int c = blockIdx.y;
    const int tid = threadIdx.x;
    const int spatial_size = h * w;

    if (n >= batch_size || c >= num_features) return;

    const float channel_mean = mean[c];
    const float inv_std = 1.0f / sqrtf(var[c] + eps);
    const float scale = weight[c] * inv_std;
    const float shift = bias[c] - scale * channel_mean;

    // Vectorized processing (4 elements per thread)
    const int vec_size = 4;
    const int num_vectors = spatial_size / vec_size;
    const int remainder_start = num_vectors * vec_size;

    for (int i = tid; i < num_vectors; i += blockDim.x) {
        const int spatial_idx = i * vec_size;
        const int hi = spatial_idx / w;
        const int wi = spatial_idx % w;
        const int idx = n * num_features * h * w + c * h * w + hi * w + wi;

        float4 in_val = *reinterpret_cast<const float4*>(input + idx);
        float4 out_val;
        out_val.x = in_val.x * scale + shift;
        out_val.y = in_val.y * scale + shift;
        out_val.z = in_val.z * scale + shift;
        out_val.w = in_val.w * scale + shift;
        *reinterpret_cast<float4*>(output + idx) = out_val;
    }

    // Process remaining elements
    for (int i = remainder_start + tid; i < spatial_size; i += blockDim.x) {
        const int hi = i / w;
        const int wi = i % w;
        const int idx = n * num_features * h * w + c * h * w + hi * w + wi;
        output[idx] = input[idx] * scale + shift;
    }
}

torch::Tensor batchnorm_forward_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor mean,
    torch::Tensor var,
    float eps,
    int batch_size,
    int num_features,
    int h,
    int w
) {
    auto output = torch::empty_like(input);
    dim3 grid(batch_size, num_features);
    const int threads_per_block = 256;
    
    batchnorm_forward_kernel<<<grid, threads_per_block>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        mean.data_ptr<float>(),
        var.data_ptr<float>(),
        output.data_ptr<float>(),
        eps,
        num_features,
        batch_size,
        h,
        w
    );
    
    return output;
}

void compute_sum_sq_cuda(
    torch::Tensor input,
    torch::Tensor sum,
    torch::Tensor sum_sq,
    int batch_size,
    int num_features,
    int h,
    int w
) {
    const int threads_per_block = 256;
    compute_sum_sq_kernel<<<num_features, threads_per_block>>>(
        input.data_ptr<float>(),
        sum.data_ptr<float>(),
        sum_sq.data_ptr<float>(),
        batch_size,
        num_features,
        h,
        w
    );
}
"""

batchnorm_cpp_source = """
torch::Tensor batchnorm_forward_cuda(
    torch::Tensor input, 
    torch::Tensor weight, 
    torch::Tensor bias,
    torch::Tensor mean,
    torch::Tensor var,
    float eps,
    int batch_size,
    int num_features,
    int h,
    int w
);

void compute_sum_sq_cuda(
    torch::Tensor input,
    torch::Tensor sum,
    torch::Tensor sum_sq,
    int batch_size,
    int num_features,
    int h,
    int w
);
"""

# Load the optimized CUDA extension
batchnorm_ext = load_inline(
    name='batchnorm_ext',
    cpp_sources=batchnorm_cpp_source,
    cuda_sources=batchnorm_source,
    functions=['batchnorm_forward_cuda', 'compute_sum_sq_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O2', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, num_features: int):
        super(ModelNew, self).__init__()
        self.num_features = num_features
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.eps = 1e-5
        self.momentum = 0.1

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.contiguous()
        batch_size, num_channels, h, w = x.size()
        
        if self.training:
            # Compute batch statistics with optimized kernel
            sum_ = torch.zeros(self.num_features, device=x.device, dtype=x.dtype)
            sum_sq = torch.zeros_like(sum_)
            batchnorm_ext.compute_sum_sq_cuda(x, sum_, sum_sq, batch_size, self.num_features, h, w)
            
            num_elements = batch_size * h * w
            batch_mean = sum_ / num_elements
            batch_var = (sum_sq / num_elements) - batch_mean.pow(2)
            
            # Update running stats with Bessel's correction
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.detach()
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.detach() * (num_elements / (num_elements - 1))
            
            mean = batch_mean
            var = batch_var
        else:
            mean = self.running_mean
            var = self.running_var
        
        # Apply optimized batchnorm kernel with 2D grid
        return batchnorm_ext.batchnorm_forward_cuda(
            x,
            self.weight,
            self.bias,
            mean,
            var,
            self.eps,
            batch_size,
            self.num_features,
            h,
            w
        )
```
