You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 14.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <math.h>

// Fused operations: subtract1, tanh, subtract2
__global__ void fused_operations(
    const float* input, float* output,
    float s1, float s2,
    int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = input[idx];
        val -= s1;
        val = tanhf(val);
        val -= s2;
        output[idx] = val;
    }
}

torch::Tensor fused_operations_cuda(
    torch::Tensor input,
    float s1,
    float s2) {
    auto output = torch::empty_like(input);
    int numel = input.numel();
    const int block_size = 256;
    int grid_size = (numel + block_size - 1) / block_size;
    fused_operations<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        s1,
        s2,
        numel);
    return output;
}

// Custom average pooling kernel for 2x2 kernel
__global__ void avg_pool_2x2(
    const float* input,
    float* output,
    int batch_size,
    int channels,
    int input_height,
    int input_width,
    int output_height,
    int output_width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * output_height * output_width)
        return;
    
    int w_out = idx % output_width;
    int h_out = (idx / output_width) % output_height;
    int c = (idx / (output_width * output_height)) % channels;
    int n = idx / (channels * output_width * output_height);
    
    int h_start = h_out * 2;
    int w_start = w_out * 2;
    
    float sum = 0.0f;
    for(int i=0; i<2; ++i) {
        for(int j=0; j<2; ++j) {
            int h = h_start + i;
            int w = w_start + j;
            if(h < input_height && w < input_width) {
                int input_idx = n * channels * input_height * input_width +
                                c * input_height * input_width +
                                h * input_width + w;
                sum += input[input_idx];
            }
        }
    }
    output[idx] = sum / 4.0f;
}

torch::Tensor avg_pool_2x2_cuda(
    torch::Tensor input) {
    int batch_size = input.size(0);
    int channels = input.size(1);
    int input_h = input.size(2);
    int input_w = input.size(3);
    int output_h = input_h / 2;
    int output_w = input_w / 2;
    auto output = torch::empty({batch_size, channels, output_h, output_w}, input.options());
    
    int numel = output.numel();
    const int block_size = 256;
    int grid_size = (numel + block_size - 1) / block_size;
    
    avg_pool_2x2<<<grid_size, block_size>>>(
        input.contiguous().data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        input_h,
        input_w,
        output_h,
        output_w);
    
    return output;
}
"""

fused_ops_cpp = """
torch::Tensor fused_operations_cuda(torch::Tensor input, float s1, float s2);
torch::Tensor avg_pool_2x2_cuda(torch::Tensor input);
"""

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_operations_cuda', 'avg_pool_2x2_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract1_value = subtract1_value
        self.subtract2_value = subtract2_value
        
    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_operations_cuda(x, self.subtract1_value, self.subtract2_value)
        x = fused_ops.avg_pool_2x2_cuda(x)
        return x
```

Kernel 2 (runtime: 14.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_sub_tanh_subtract_code = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_sub_tanh_subtract(
    const float* input,
    float* output,
    float sub1,
    float sub2,
    int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = input[idx] - sub1;
        val = tanhf(val);
        val -= sub2;
        output[idx] = val;
    }
}

torch::Tensor fused_sub_tanh_subtract_cuda(
    torch::Tensor input,
    float sub1,
    float sub2) {
    auto output = torch::empty_like(input);
    int numel = input.numel();
    const int threads_per_block = 256;
    int blocks_per_grid = (numel + threads_per_block - 1) / threads_per_block;
    fused_sub_tanh_subtract<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        sub1,
        sub2,
        numel);
    return output;
}
"""

fused_sub_tanh_subtract = load_inline(
    name='fused_sub_tanh_subtract',
    cpp_sources="torch::Tensor fused_sub_tanh_subtract_cuda(torch::Tensor input, float sub1, float sub2);",
    cuda_sources=fused_sub_tanh_subtract_code,
    functions=['fused_sub_tanh_subtract_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract1_value = subtract1_value
        self.subtract2_value = subtract2_value
        self.avgpool = nn.AvgPool2d(kernel_size_pool)
        self.fused_sub_tanh_subtract = fused_sub_tanh_subtract

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_sub_tanh_subtract.fused_sub_tanh_subtract_cuda(x, self.subtract1_value, self.subtract2_value)
        x = self.avgpool(x)
        return x
```
