You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 1170.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernels for fused operations
cuda_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Fused GEMM + Sigmoid kernel
__global__ void gemm_sigmoid_kernel(
    const float* input, const float* weight, const float* bias,
    float* output, int M, int N, int K) {
    
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < M && col < N) {
        float sum = 0.0f;
        for(int k = 0; k < K; ++k) {
            sum += input[row*K + k] * weight[col*K + k];
        }
        sum += bias[col];
        output[row*N + col] = 1.0f / (1.0f + expf(-sum));
    }
}

torch::Tensor gemm_sigmoid_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {
    int M = input.size(0);
    int K = input.size(1);
    int N = weight.size(0);
    
    auto output = torch::empty({M, N}, torch::device(torch::kCUDA).dtype(torch::kFloat32));
    
    dim3 threads(16, 16);
    dim3 blocks((N + threads.x - 1)/threads.x, (M + threads.y - 1)/threads.y);
    
    gemm_sigmoid_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        M, N, K
    );
    
    return output;
}

// Fused GEMM + LogSumExp kernel
__global__ void gemm_logsumexp_kernel(
    const float* input, const float* weight, const float* bias,
    float* output, int M, int N, int K) {
    
    int sample_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if(sample_idx >= M) return;
    
    float max_val = -INFINITY;
    float sum_exp = 0.0f;
    
    for(int n = 0; n < N; ++n) {
        float val = 0.0f;
        for(int k = 0; k < K; ++k) {
            val += input[sample_idx*K + k] * weight[n*K + k];
        }
        val += bias[n];
        
        if(val > max_val) {
            sum_exp *= expf(max_val - val);
            max_val = val;
            sum_exp += 1.0f;
        } else {
            sum_exp += expf(val - max_val);
        }
    }
    
    output[sample_idx] = max_val + logf(sum_exp);
}

torch::Tensor gemm_logsumexp_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {
    int M = input.size(0);
    int K = input.size(1);
    int N = weight.size(0);
    
    auto output = torch::empty({M}, torch::device(torch::kCUDA).dtype(torch::kFloat32));
    
    int threads = 256;
    int blocks = (M + threads - 1) / threads;
    
    gemm_logsumexp_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        M, N, K
    );
    
    return output;
}
"""

cpp_code = """
torch::Tensor gemm_sigmoid_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias);
torch::Tensor gemm_logsumexp_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias);
"""

# Load custom CUDA operations
custom_ops = load_inline(
    name='custom_ops',
    cpp_sources=cpp_code,
    cuda_sources=cuda_code,
    functions=['gemm_sigmoid_cuda', 'gemm_logsumexp_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        # Maintain linear layers for parameter storage
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Fused GEMM + Sigmoid
        x = custom_ops.gemm_sigmoid_cuda(x, self.linear1.weight, self.linear1.bias)
        # Fused GEMM + LogSumExp
        x = custom_ops.gemm_logsumexp_cuda(x, self.linear2.weight, self.linear2.bias)
        return x
```

Kernel 2 (runtime: 606.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------- Fused GEMM + Sigmoid Kernel ----------------------
gemm_sigmoid_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_SIZE 32

__global__ void gemm_sigmoid_kernel(
    const float* input, const float* weights, const float* bias,
    float* output,
    int batch_size, int input_size, int hidden_size
) {
    __shared__ float input_tile[TILE_SIZE][TILE_SIZE];
    __shared__ float weight_tile[TILE_SIZE][TILE_SIZE];

    int row = blockIdx.y * TILE_SIZE + threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + threadIdx.x;

    float sum = 0.0f;

    for (int t = 0; t < (input_size + TILE_SIZE - 1) / TILE_SIZE; ++t) {
        // Load input tile
        int input_col = t * TILE_SIZE + threadIdx.x;
        if (row < batch_size && input_col < input_size) {
            input_tile[threadIdx.y][threadIdx.x] = input[row * input_size + input_col];
        } else {
            input_tile[threadIdx.y][threadIdx.x] = 0.0f;
        }

        // Load weight tile (transposed for coalesced access)
        int weight_row = t * TILE_SIZE + threadIdx.y;
        if (weight_row < input_size && col < hidden_size) {
            weight_tile[threadIdx.y][threadIdx.x] = weights[col * input_size + weight_row];
        } else {
            weight_tile[threadIdx.y][threadIdx.x] = 0.0f;
        }

        __syncthreads();

        // Compute partial sum
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += input_tile[threadIdx.y][k] * weight_tile[k][threadIdx.x];
        }

        __syncthreads();
    }

    if (row < batch_size && col < hidden_size) {
        sum += bias[col];
        output[row * hidden_size + col] = 1.0f / (1.0f + expf(-sum));
    }
}

torch::Tensor gemm_sigmoid_cuda(
    torch::Tensor input, torch::Tensor weights, torch::Tensor bias
) {
    auto batch_size = input.size(0);
    auto input_size = input.size(1);
    auto hidden_size = weights.size(0);
    
    auto output = torch::zeros({batch_size, hidden_size}, 
        torch::device(torch::kCUDA).dtype(torch::kFloat32));
    
    dim3 block(TILE_SIZE, TILE_SIZE);
    dim3 grid(
        (hidden_size + TILE_SIZE - 1) / TILE_SIZE,
        (batch_size + TILE_SIZE - 1) / TILE_SIZE
    );
    
    gemm_sigmoid_kernel<<<grid, block>>>(
        input.data_ptr<float>(),
        weights.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, input_size, hidden_size
    );
    
    return output;
}
"""

# ---------------------- Fused GEMM + LogSumExp Kernel ----------------------
gemm_logsumexp_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__device__ float warp_reduce_max(float val) {
    for (int offset = 16; offset > 0; offset >>= 1) 
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    return val;
}

__device__ float warp_reduce_sum(float val) {
    for (int offset = 16; offset > 0; offset >>= 1) 
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void gemm_logsumexp_kernel(
    const float* input, const float* weights, const float* bias,
    float* output,
    int batch_size, int hidden_size, int output_size
) {
    extern __shared__ float smem[];
    float* input_row = smem;
    const int tid = threadIdx.x;
    const int lane_id = tid % 32;
    const int warp_id = tid / 32;
    const int row = blockIdx.x;

    // Load input row into shared memory
    for (int i = tid; i < hidden_size; i += blockDim.x) {
        input_row[i] = input[row * hidden_size + i];
    }
    __syncthreads();

    float max_val = -INFINITY;
    float sum_exp = 0.0f;

    // Each thread computes multiple output elements
    for (int j = tid; j < output_size; j += blockDim.x) {
        float val = 0.0f;
        for (int k = 0; k < hidden_size; ++k) {
            val += input_row[k] * weights[j * hidden_size + k];
        }
        val += bias[j];
        
        // Update max value
        max_val = fmaxf(max_val, val);
        
        // Store computed value for later exp sum
        smem[hidden_size + j] = val;  // Reuse shared memory
    }

    // Block-wide max reduction
    float block_max = warp_reduce_max(max_val);
    if (lane_id == 0) {
        smem[warp_id] = block_max;
    }
    __syncthreads();
    
    if (tid == 0) {
        block_max = -INFINITY;
        for (int i = 0; i < (blockDim.x + 31) / 32; ++i) {
            block_max = fmaxf(block_max, smem[i]);
        }
        smem[0] = block_max;
    }
    __syncthreads();
    block_max = smem[0];

    // Compute sum of exponentials
    for (int j = tid; j < output_size; j += blockDim.x) {
        sum_exp += expf(smem[hidden_size + j] - block_max);
    }

    // Block-wide sum reduction
    float block_sum = warp_reduce_sum(sum_exp);
    if (lane_id == 0) {
        smem[warp_id] = block_sum;
    }
    __syncthreads();
    
    if (tid == 0) {
        block_sum = 0.0f;
        for (int i = 0; i < (blockDim.x + 31) / 32; ++i) {
            block_sum += smem[i];
        }
        output[row] = logf(block_sum) + block_max;
    }
}

torch::Tensor gemm_logsumexp_cuda(
    torch::Tensor input, torch::Tensor weights, torch::Tensor bias
) {
    auto batch_size = input.size(0);
    auto hidden_size = input.size(1);
    auto output_size = weights.size(0);
    
    auto output = torch::zeros({batch_size}, 
        torch::device(torch::kCUDA).dtype(torch::kFloat32));
    
    int threads = 256;
    int shared_mem = (hidden_size + output_size + 32) * sizeof(float);
    
    gemm_logsumexp_kernel<<<batch_size, threads, shared_mem>>>(
        input.data_ptr<float>(),
        weights.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, hidden_size, output_size
    );
    
    return output;
}
"""

# Load both kernels
custom_ops = load_inline(
    name='custom_ops',
    cpp_sources=[
        "torch::Tensor gemm_sigmoid_cuda(torch::Tensor input, torch::Tensor weights, torch::Tensor bias);",
        "torch::Tensor gemm_logsumexp_cuda(torch::Tensor input, torch::Tensor weights, torch::Tensor bias);"
    ],
    cuda_sources=[gemm_sigmoid_source, gemm_logsumexp_source],
    functions=['gemm_sigmoid_cuda', 'gemm_logsumexp_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # Fused GEMM + Sigmoid
        x = custom_ops.gemm_sigmoid_cuda(
            x, 
            self.linear1.weight,
            self.linear1.bias
        )
        
        # Fused GEMM + LogSumExp
        x = custom_ops.gemm_logsumexp_cuda(
            x,
            self.linear2.weight,
            self.linear2.bias
        )
        
        return x
```
