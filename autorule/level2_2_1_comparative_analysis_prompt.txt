You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 28.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_ops_kernel(
    const float* input,
    const float* bias,
    float* output,
    int num_elements,
    int channels,
    int H, int W,
    float scaling_factor
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= num_elements) return;

    // Compute channel index
    int n = idx / (channels * H * W);
    int remainder = idx % (channels * H * W);
    int c = remainder / (H * W);
    int hw = remainder % (H * W);
    int h = hw / W;
    int w = hw % W;

    // Apply bias
    float val = input[idx] + bias[c];

    // First clamp between 0 and 1
    val = fmaxf(fminf(val, 1.0f), 0.0f);

    // Multiply by scaling factor
    val *= scaling_factor;

    // Second clamp between 0 and 1
    val = fmaxf(fminf(val, 1.0f), 0.0f);

    // Divide by scaling factor
    val /= scaling_factor;

    output[idx] = val;
}

torch::Tensor fused_ops_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling_factor
) {
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");
    TORCH_CHECK(bias.is_contiguous(), "Bias must be contiguous");
    TORCH_CHECK(input.dim() == 4, "Input must be 4D (N,C,H,W)");

    auto output = torch::empty_like(input);
    int num_elements = input.numel();
    int channels = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    dim3 threadsPerBlock(256);
    dim3 numBlocks((num_elements + threadsPerBlock.x - 1) / threadsPerBlock.x);

    fused_ops_kernel<<<numBlocks, threadsPerBlock>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        channels,
        H, W,
        scaling_factor
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_ops_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_ops_cuda'],
    verbose=True,
    extra_cflags=[],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_ops_cuda(x, self.bias, self.scaling_factor)
        return x
```

Kernel 2 (runtime: 28.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel with mathematical simplification and correct remainder handling
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_ops_kernel(
    const float* __restrict__ conv_output,
    const float* __restrict__ bias,
    float* __restrict__ output,
    float inv_scale,
    int N, int C, int H, int W
) {
    __shared__ float s_bias;
    const int tid = threadIdx.x;
    const int n = blockIdx.x;
    const int c = blockIdx.y;
    
    // Load bias once per block using shared memory
    if (tid == 0) s_bias = __ldg(bias + c);
    __syncthreads();
    
    const int hw = H * W;
    const int vec_size = 4;
    const int base = n * C * H * W + c * H * W;
    const float upper_limit = fminf(1.0f, inv_scale);
    
    const float4* base_ptr = reinterpret_cast<const float4*>(conv_output + base);
    float4* out_ptr = reinterpret_cast<float4*>(output + base);
    
    const int vec_count = hw / vec_size;
    const int vec_processed = vec_count * vec_size;
    
    // Vectorized processing of full 4-element chunks
    for (int i = tid; i < vec_count; i += blockDim.x) {
        float4 conv_val = __ldg(base_ptr + i);
        float4 out_val;
        
        #pragma unroll
        for (int v = 0; v < vec_size; ++v) {
            float val = (&conv_val.x)[v] + s_bias;
            val = fmaxf(0.0f, fminf(val, upper_limit));
            (&out_val.x)[v] = val;
        }
        
        out_ptr[i] = out_val;
    }
    
    // Process remaining elements using scalar operations
    const int remaining = hw - vec_processed;
    for (int i = tid; i < remaining; i += blockDim.x) {
        const int linear_idx = base + vec_processed + i;
        float val = __ldg(conv_output + linear_idx) + s_bias;
        val = fmaxf(0.0f, fminf(val, upper_limit));
        output[linear_idx] = val;
    }
}

torch::Tensor fused_ops_cuda(
    torch::Tensor conv_output,
    torch::Tensor bias,
    float scaling_factor
) {
    auto output = torch::empty_like(conv_output);
    const int N = conv_output.size(0), C = conv_output.size(1);
    const float inv_scale = 1.0f / scaling_factor;
    
    dim3 grid(N, C);
    const int block_size = 256;
    
    fused_ops_kernel<<<grid, block_size>>>(
        conv_output.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        inv_scale,
        N, C, conv_output.size(2), conv_output.size(3)
    );
    
    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_ops_cuda(torch::Tensor conv_output, torch::Tensor bias, float scaling_factor);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=["fused_ops_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_ops_cuda(x, self.bias, self.scaling_factor)
        return x
```
