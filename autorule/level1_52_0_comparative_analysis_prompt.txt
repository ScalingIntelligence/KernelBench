You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 14.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

argmin_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

__global__ void argmin_kernel(const float* input, int64_t* output, int batch_size, int dim1, int dim2) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_elements = batch_size * dim2;
    
    if (idx < total_elements) {
        int batch = idx / dim2;
        int d2 = idx % dim2;
        
        float min_val = INFINITY;
        int min_index = 0;
        
        for (int d1 = 0; d1 < dim1; ++d1) {
            float val = input[batch * dim1 * dim2 + d1 * dim2 + d2];
            if (val < min_val) {
                min_val = val;
                min_index = d1;
            }
        }
        
        output[idx] = min_index;
    }
}

torch::Tensor argmin_cuda(torch::Tensor input) {
    auto sizes = input.sizes();
    int batch_size = sizes[0];
    int dim1 = sizes[1];
    int dim2 = sizes[2];
    
    auto output = torch::empty({batch_size, dim2}, torch::dtype(torch::kLong).device(input.device()));
    
    int total_elements = batch_size * dim2;
    const int block_size = 256;
    int num_blocks = (total_elements + block_size - 1) / block_size;
    
    argmin_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<int64_t>(),
        batch_size,
        dim1,
        dim2
    );
    
    return output;
}
"""

argmin_cpp_source = "torch::Tensor argmin_cuda(torch::Tensor input);"

argmin_extension = load_inline(
    name='argmin',
    cpp_sources=argmin_cpp_source,
    cuda_sources=argmin_source,
    functions=['argmin_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        assert self.dim == 1, "Custom kernel only supports dim=1"
        self.argmin_cuda = argmin_extension.argmin_cuda

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.argmin_cuda(x.contiguous())
```

Kernel 2 (runtime: 14.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for optimized argmin along dim=1
argmin_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

__global__ void argmin_kernel(const float* input, int64_t* output, int batch_size, int dim1, int dim2) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * dim2) return;

    int batch = idx / dim2;
    int dim2_pos = idx % dim2;

    float min_val = INFINITY;
    int min_idx = 0;

    for(int dim1_idx = 0; dim1_idx < dim1; dim1_idx++) {
        float val = input[batch * dim1 * dim2 + dim1_idx * dim2 + dim2_pos];
        if(val < min_val) {
            min_val = val;
            min_idx = dim1_idx;
        }
    }
    
    output[idx] = min_idx;
}

torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    TORCH_CHECK(dim == 1, "This kernel only supports dim=1");
    auto sizes = input.sizes();
    auto output = torch::empty({sizes[0], sizes[2]}, 
                             torch::TensorOptions().dtype(torch::kLong).device(input.device()));

    int total_threads = sizes[0] * sizes[2];
    const int block_size = 256;
    dim3 grid((total_threads + block_size - 1) / block_size);

    argmin_kernel<<<grid, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<int64_t>(),
        sizes[0], sizes[1], sizes[2]
    );

    return output;
}
"""

argmin_cpp = "torch::Tensor argmin_cuda(torch::Tensor input, int dim);"

# Load the custom CUDA extension
argmin_ext = load_inline(
    name='argmin_ext',
    cpp_sources=argmin_cpp,
    cuda_sources=argmin_source,
    functions=['argmin_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.dim == 1 and x.is_cuda:
            return argmin_ext.argmin_cuda(x, self.dim)
        return torch.argmin(x, dim=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095
dim = 1

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2).cuda()
    return [x]

def get_init_inputs():
    return [dim]
```
