You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.24 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused clamp, logsumexp, Mish, and multiplication
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_ops_kernel(const float* input, float* output, 
                               int hidden_size, float clamp_min, 
                               float clamp_max) {
    int batch_idx = blockIdx.x;
    int tid = threadIdx.x;
    
    extern __shared__ float shared_expsum[];
    
    float expsum = 0.0f;
    const int elements_per_thread = (hidden_size + blockDim.x - 1) / blockDim.x;
    
    for (int i = 0; i < elements_per_thread; ++i) {
        int idx = tid + i * blockDim.x;
        if (idx < hidden_size) {
            float val = input[batch_idx * hidden_size + idx];
            val = fmaxf(fminf(val, clamp_max), clamp_min);
            expsum += expf(val);
        }
    }
    
    shared_expsum[tid] = expsum;
    __syncthreads();
    
    // Parallel reduction
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_expsum[tid] += shared_expsum[tid + s];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        float sum_exp = shared_expsum[0];
        float logsumexp = logf(sum_exp + 1e-10f);
        
        // Compute Mish: x * tanh(softplus(x))
        float softplus_val = log1pf(expf(logsumexp));
        float tanh_sp = tanhf(softplus_val);
        float mish = logsumexp * tanh_sp;
        
        output[batch_idx] = logsumexp * mish;
    }
}

torch::Tensor fused_ops_cuda(torch::Tensor input, float clamp_min, float clamp_max) {
    int batch_size = input.size(0);
    int hidden_size = input.size(1);
    
    auto output = torch::empty({batch_size, 1}, input.options());
    
    const int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);
    
    fused_ops_kernel<<<grid, block, block_size * sizeof(float)>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        hidden_size,
        clamp_min,
        clamp_max
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_ops_cuda(torch::Tensor input, float clamp_min, float clamp_max);"

# Compile the CUDA kernel
fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=["fused_ops_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, scale_factor, clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        # Pre-scale weights and bias to absorb scaling and addition operations
        self.matmul = nn.Linear(input_size, hidden_size)
        with torch.no_grad():
            self.matmul.weight.data *= (scale_factor * 2)  # Combine scaling and x+x
            self.matmul.bias.data *= (scale_factor * 2)
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        x = self.matmul(x)
        x = fused_ops.fused_ops_cuda(x, self.clamp_min, self.clamp_max)
        return x
```

Kernel 2 (runtime: 7.48 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused Scale, Add (x + x), and Clamp kernel
scale_add_clamp_cuda = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scale_add_clamp_kernel(const float* input, float* output, float scale, float clamp_min, float clamp_max, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx] * scale;
        val = fminf(fmaxf(val, clamp_min), clamp_max);
        output[idx] = val;
    }
}

torch::Tensor scale_add_clamp(torch::Tensor input, float scale, float clamp_min, float clamp_max) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    scale_add_clamp_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), scale, clamp_min, clamp_max, size);
    return output;
}
"""
scale_add_clamp = load_inline(
    name='scale_add_clamp',
    cpp_sources="torch::Tensor scale_add_clamp(torch::Tensor input, float scale, float clamp_min, float clamp_max);",
    cuda_sources=scale_add_clamp_cuda,
    functions=['scale_add_clamp'],
    verbose=True
)

# Optimized LogSumExp kernel with online computation
logsumexp_cuda = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void logsumexp_kernel(const float* input, float* output, int rows, int cols) {
    extern __shared__ float smem[];
    
    int row = blockIdx.x;
    int tid = threadIdx.x;
    const float* row_ptr = input + row * cols;
    
    // Compute max reduction
    float max_val = -INFINITY;
    for(int i = tid; i < cols; i += blockDim.x) {
        max_val = fmaxf(max_val, row_ptr[i]);
    }
    smem[tid] = max_val;
    __syncthreads();
    
    for(int s = blockDim.x/2; s > 0; s >>= 1) {
        if(tid < s) smem[tid] = fmaxf(smem[tid], smem[tid+s]);
        __syncthreads();
    }
    
    float row_max = smem[0];
    __syncthreads();
    
    // Compute sum reduction
    float sum_exp = 0.0f;
    for(int i = tid; i < cols; i += blockDim.x) {
        sum_exp += expf(row_ptr[i] - row_max);
    }
    smem[tid] = sum_exp;
    __syncthreads();
    
    for(int s = blockDim.x/2; s > 0; s >>= 1) {
        if(tid < s) smem[tid] += smem[tid+s];
        __syncthreads();
    }
    
    if(tid == 0) output[row] = logf(smem[0]) + row_max;
}

torch::Tensor logsumexp(torch::Tensor input) {
    auto output = torch::empty({input.size(0), 1}, input.options());
    int block_size = 256;
    size_t smem = block_size * sizeof(float);
    logsumexp_kernel<<<input.size(0), block_size, smem>>>(
        input.data_ptr<float>(), output.data_ptr<float>(), 
        input.size(0), input.size(1)
    );
    return output;
}
"""
logsumexp = load_inline(
    name='logsumexp',
    cpp_sources="torch::Tensor logsumexp(torch::Tensor input);",
    cuda_sources=logsumexp_cuda,
    functions=['logsumexp'],
    verbose=True
)

# Fused Mish activation kernel
mish_cuda = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void mish_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if(idx < size) {
        float x = input[idx];
        float sp = log1pf(expf(x));
        float tanh_sp = tanhf(sp);
        output[idx] = x * x * tanh_sp;  // x * mish(x)
    }
}

torch::Tensor mish_activation(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    mish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
    return output;
}
"""
mish_activation = load_inline(
    name='mish_activation',
    cpp_sources="torch::Tensor mish_activation(torch::Tensor input);",
    cuda_sources=mish_cuda,
    functions=['mish_activation'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, scale_factor, clamp_min, clamp_max):
        super().__init__()
        self.matmul = nn.Linear(input_size, hidden_size)
        self.combined_scale = scale_factor * 2.0  # Fuse scaling and x+x
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        x = self.matmul(x)
        x = scale_add_clamp.scale_add_clamp(x, self.combined_scale, self.clamp_min, self.clamp_max)
        x = logsumexp.logsumexp(x)
        x = mish_activation.mish_activation(x)
        return x

def get_inputs():
    return [torch.rand(1024, 8192)]

def get_init_inputs():
    return [8192, 8192, 2.0, -10.0, 10.0]
```
