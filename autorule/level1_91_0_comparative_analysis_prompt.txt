You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 128.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

reverse_cumsum_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void reverse_cumsum_kernel(const float* input, float* output, int outer_size, int size_dim, int stride_dim) {
    int outer_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (outer_idx >= outer_size) return;

    int base = (outer_idx / stride_dim) * (size_dim * stride_dim) + (outer_idx % stride_dim);

    float sum = 0.0f;
    for (int i = size_dim - 1; i >= 0; --i) {
        int idx = base + i * stride_dim;
        sum += input[idx];
        output[idx] = sum;
    }
}

torch::Tensor reverse_cumsum_cuda(torch::Tensor input, int64_t dim) {
    input = input.contiguous();
    auto output = torch::zeros_like(input);

    int64_t size_dim = input.size(dim);
    int64_t stride_dim = input.stride(dim);
    int64_t outer_size = input.numel() / size_dim;

    const int block_size = 256;
    int num_blocks = (outer_size + block_size - 1) / block_size;

    reverse_cumsum_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        outer_size,
        size_dim,
        stride_dim
    );

    return output;
}
"""

reverse_cumsum_cpp_source = "torch::Tensor reverse_cumsum_cuda(torch::Tensor input, int64_t dim);"

reverse_cumsum = load_inline(
    name="reverse_cumsum",
    cpp_sources=reverse_cumsum_cpp_source,
    cuda_sources=reverse_cumsum_source,
    functions=["reverse_cumsum_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.reverse_cumsum = reverse_cumsum

    def forward(self, x):
        return self.reverse_cumsum.reverse_cumsum_cuda(x, self.dim)
```

Kernel 2 (runtime: 121.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

reverse_cumsum_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void reverse_cumsum_kernel(const float* input, float* output, int dim_size, int num_rows) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row >= num_rows) return;

    float sum = 0.0f;
    // Traverse backward through the dimension
    for(int j = dim_size-1; j >= 0; j--) {
        int idx = row * dim_size + j;
        sum += input[idx];
        output[idx] = sum;
    }
}

torch::Tensor reverse_cumsum_cuda(torch::Tensor input, int dim) {
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");
    TORCH_CHECK(dim == 1, "Only dim=1 supported for this implementation");

    int dim_size = input.size(dim);
    int num_rows = input.size(0);

    auto output = torch::empty_like(input);

    const int threads_per_block = 256;
    int num_blocks = (num_rows + threads_per_block - 1) / threads_per_block;

    reverse_cumsum_kernel<<<num_blocks, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim_size,
        num_rows
    );

    return output;
}
"""

reverse_cumsum_cpp_source = "torch::Tensor reverse_cumsum_cuda(torch::Tensor input, int dim);"

reverse_cumsum_ext = load_inline(
    name="reverse_cumsum",
    cpp_sources=reverse_cumsum_cpp_source,
    cuda_sources=reverse_cumsum_cuda_source,
    functions=["reverse_cumsum_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        
    def forward(self, x):
        # Ensure contiguous memory access
        x = x.contiguous()
        return reverse_cumsum_ext.reverse_cumsum_cuda(x, self.dim)
```
