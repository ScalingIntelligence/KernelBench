You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 16.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused kernel with correct pair handling and efficient reduction
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_maxpool_sum_scale_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int batch_size,
    int out_features,
    float scale_factor
) {
    const int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;

    const float* batch_row = input + batch_idx * out_features;
    const int tid = threadIdx.x;
    const int num_pairs = out_features / 2;
    const int elements_per_thread = (num_pairs + blockDim.x - 1) / blockDim.x;

    float thread_sum = 0.0f;

    // Process pairs with coalesced accesses and vectorized loads
    for (int i = 0; i < elements_per_thread; ++i) {
        const int pair_idx = tid + i * blockDim.x;
        if (pair_idx >= num_pairs) break;
        
        const int offset = pair_idx * 2;
        float2 pair = *reinterpret_cast<const float2*>(batch_row + offset);
        thread_sum += fmaxf(pair.x, pair.y);
    }

    // Warp-level reduction using shuffle instructions
    for (int offset = 16; offset > 0; offset >>= 1) {
        thread_sum += __shfl_down_sync(0xFFFFFFFF, thread_sum, offset);
    }

    // Shared memory for inter-warp reduction
    extern __shared__ float warp_sums[];
    if (tid % 32 == 0) {
        warp_sums[tid / 32] = thread_sum;
    }
    __syncthreads();

    // Final reduction across warps using first warp
    if (tid < 32) {
        float total = 0.0f;
        const int num_warps = (blockDim.x + 31) / 32;
        
        for (int i = tid; i < num_warps; i += 32) {
            total += warp_sums[i];
        }
        
        // Warp-wide reduction of partial sums
        for (int offset = 16; offset > 0; offset >>= 1) {
            total += __shfl_down_sync(0xFFFFFFFF, total, offset);
        }

        if (tid == 0) {
            output[batch_idx] = total * scale_factor;
        }
    }
}

torch::Tensor fused_ops_cuda(
    torch::Tensor input,
    float scale_factor
) {
    const int batch_size = input.size(0);
    const int out_features = input.size(1);
    auto output = torch::zeros({batch_size}, input.options());

    const int block_size = 256;  // Optimal for modern GPUs
    const dim3 grid(batch_size);
    const dim3 block(block_size);
    const size_t shared_mem = ((block_size + 31)/32) * sizeof(float);

    fused_maxpool_sum_scale_kernel<<<grid, block, shared_mem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features,
        scale_factor
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_ops_cuda(torch::Tensor input, float scale_factor);"

# Load the optimized CUDA kernel with compiler optimizations
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_ops_cuda'],
    verbose=True,
    extra_cuda_cflags=["-O3", "--use_fast_math"]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.scale_factor = scale_factor
        assert kernel_size == 2, "Kernel size must be 2 for fused implementation"
        assert out_features % 2 == 0, "out_features must be even for vectorized loads"

    def forward(self, x):
        x = self.matmul(x)
        x = fused_ops.fused_ops_cuda(x, self.scale_factor)
        return x
```

Kernel 2 (runtime: 16.4 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused kernel with vectorized loads and warp shuffle reduction
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_maxpool_sum_scale_kernel(const float* __restrict__ input, 
                                             float* __restrict__ output,
                                             int out_features, 
                                             float scale_factor) {
    extern __shared__ float warp_sums[];
    const int batch_idx = blockIdx.x;
    const int tid = threadIdx.x;
    const int warp_id = tid / 32;
    const int lane_id = tid % 32;
    const int num_pairs = out_features / 2;
    
    const float* row = input + batch_idx * out_features;
    
    // Vectorized access using float2 for coalesced memory loads
    float thread_sum = 0.0f;
    for(int i = tid; i < num_pairs; i += blockDim.x) {
        float2 pair = reinterpret_cast<const float2*>(row)[i];
        thread_sum += fmaxf(pair.x, pair.y);
    }
    
    // Warp-level reduction with shuffle instructions
    for(int offset = 16; offset > 0; offset >>= 1) {
        thread_sum += __shfl_down_sync(0xffffffff, thread_sum, offset);
    }
    
    // Store warp sum to shared memory
    if(lane_id == 0) {
        warp_sums[warp_id] = thread_sum;
    }
    __syncthreads();
    
    // First warp reduces all warp sums
    if(warp_id == 0) {
        float block_sum = lane_id < (blockDim.x / 32) ? warp_sums[lane_id] : 0.0f;
        
        // Final warp reduction
        for(int offset = 16; offset > 0; offset >>= 1) {
            block_sum += __shfl_down_sync(0xffffffff, block_sum, offset);
        }
        
        if(lane_id == 0) {
            output[batch_idx] = block_sum * scale_factor;
        }
    }
}

torch::Tensor fused_maxpool_sum_scale_cuda(torch::Tensor input, float scale_factor) {
    const auto batch_size = input.size(0);
    const int out_features = input.size(1);
    auto output = torch::empty({batch_size}, input.options());
    
    const int block_size = 256;  // Optimal for modern GPU architectures
    const int num_warps = block_size / 32;
    const size_t shared_mem = num_warps * sizeof(float);
    
    fused_maxpool_sum_scale_kernel<<<batch_size, block_size, shared_mem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        out_features,
        scale_factor
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_maxpool_sum_scale_cuda(torch::Tensor input, float scale_factor);"

# Load custom CUDA operations
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_maxpool_sum_scale_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.scale_factor = scale_factor
        
        # Validate kernel configuration
        assert kernel_size == 2, "Optimization requires kernel_size=2"
        assert out_features % 2 == 0, "out_features must be even"
        assert out_features >= 32, "out_features should be >=32 for good GPU utilization"

    def forward(self, x):
        x = self.matmul(x)  # (batch_size, out_features)
        x = fused_ops.fused_maxpool_sum_scale_cuda(x, self.scale_factor)
        return x
```
