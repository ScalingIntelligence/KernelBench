You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused activation kernel with vectorized loads/stores and optimized GELU
fused_activation_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_activation_kernel(
    const float* __restrict__ input,
    const float* __restrict__ add_value,
    float* __restrict__ output,
    int batch_size,
    int out_features
) {
    const int global_idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
    const int total_elements = batch_size * out_features;

    if (global_idx >= total_elements) return;

    // Vectorized load for input and add_value
    float4 in = *reinterpret_cast<const float4*>(input + global_idx);
    const int feature_idx = global_idx % out_features;
    float4 add = *reinterpret_cast<const float4*>(add_value + feature_idx);

    float4 res;
    // Element-wise addition
    res.x = in.x + add.x;
    res.y = in.y + add.y;
    res.z = in.z + add.z;
    res.w = in.w + add.w;

    // Optimized Swish implementation
    auto swish = [](float x) { return x * (1.0f / (1.0f + __expf(-x))); };
    res.x = swish(res.x);
    res.y = swish(res.y);
    res.z = swish(res.z);
    res.w = swish(res.w);

    // Fast Tanh
    res.x = tanhf(res.x);
    res.y = tanhf(res.y);
    res.z = tanhf(res.z);
    res.w = tanhf(res.w);

    // Approximate GELU for better performance
    const float sqrt_2_over_pi = 0.7978845608028654f;
    const float gelu_coeff = 0.044715f;
    auto gelu = [&](float x) {
        return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * x * (1.0f + gelu_coeff * x * x)));
    };
    res.x = gelu(res.x);
    res.y = gelu(res.y);
    res.z = gelu(res.z);
    res.w = gelu(res.w);

    // Hardtanh with bounds checking
    auto clamp = [](float x) { return fmaxf(-1.0f, fminf(1.0f, x)); };
    res.x = clamp(res.x);
    res.y = clamp(res.y);
    res.z = clamp(res.z);
    res.w = clamp(res.w);

    // Vectorized store
    *reinterpret_cast<float4*>(output + global_idx) = res;
}

torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor add_value) {
    auto output = torch::empty_like(input);
    const int total_elements = input.numel();
    const int out_features = add_value.size(0);
    const int batch_size = total_elements / out_features;

    const int threads = 256;
    const int elements_per_block = threads * 4;
    const int blocks = (total_elements + elements_per_block - 1) / elements_per_block;

    fused_activation_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        add_value.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features
    );

    return output;
}
"""

fused_activation_cpp_source = "torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor add_value);"

# Compile the optimized fused kernel
fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, add_value_shape):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.add_value = nn.Parameter(torch.randn(add_value_shape))
        self.fused_activation = fused_activation

    def forward(self, x):
        x = self.matmul(x)
        x = self.fused_activation.fused_activation_cuda(x, self.add_value)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, add_value_shape]

# Preserve original configuration values
batch_size = 1024
in_features = 8192
out_features = 8192
add_value_shape = (out_features,)
```

Kernel 2 (runtime: 7.26 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom fused activation CUDA kernel
fused_activation_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_activation_kernel(
    const float* input,
    const float* add_value,
    float* output,
    int batch_size,
    int out_features
) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int total_elements = batch_size * out_features;
    
    if (idx < total_elements) {
        const int feature_idx = idx % out_features;
        float x = input[idx] + add_value[feature_idx];
        
        // Swish: x * sigmoid(x)
        const float sigmoid_x = 1.0f / (1.0f + expf(-x));
        x *= sigmoid_x;
        
        // Tanh
        x = tanhf(x);
        
        // GELU (exact implementation)
        x = 0.5f * x * (1.0f + erff(x / 1.41421356237f));
        
        // Hardtanh
        x = fmaxf(fminf(x, 1.0f), -1.0f);
        
        output[idx] = x;
    }
}

torch::Tensor fused_activation_cuda(
    torch::Tensor matmul_result,
    torch::Tensor add_value
) {
    TORCH_CHECK(matmul_result.dim() == 2, "Input must be 2D tensor");
    TORCH_CHECK(add_value.dim() == 1, "Add value must be 1D tensor");
    
    const int batch_size = matmul_result.size(0);
    const int out_features = matmul_result.size(1);
    
    auto output = torch::empty_like(matmul_result);
    const int total_elements = batch_size * out_features;
    const int block_size = 256;
    const int grid_size = (total_elements + block_size - 1) / block_size;
    
    fused_activation_kernel<<<grid_size, block_size>>>(
        matmul_result.data_ptr<float>(),
        add_value.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features
    );
    
    return output;
}
"""

fused_activation_cpp = "torch::Tensor fused_activation_cuda(torch::Tensor, torch::Tensor);"

fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, add_value_shape):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.add_value = nn.Parameter(torch.randn(add_value_shape))
        
    def forward(self, x):
        x = self.matmul(x)
        x = fused_activation.fused_activation_cuda(x, self.add_value)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
add_value_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, add_value_shape]
```
