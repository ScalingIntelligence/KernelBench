You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 107.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized CUDA kernel with vectorized B accesses and 4 k's per thread
tensor_matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void tensor_matmul_kernel(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ C,
    int B_dim, int I_dim, int J_dim, int K_dim, int L_dim
) {
    const int total_groups = (K_dim + 3) / 4;
    const int bij_total = B_dim * I_dim * J_dim;
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx >= bij_total * total_groups) return;

    const int bij = idx / total_groups;
    const int group = idx % total_groups;

    const int b = bij / (I_dim * J_dim);
    const int ij_remainder = bij % (I_dim * J_dim);
    const int i = ij_remainder / J_dim;
    const int j = ij_remainder % J_dim;
    const int k_group = group * 4;
    const int remaining_ks = K_dim - k_group;
    const bool valid_group = remaining_ks >= 4;

    float sum0 = 0.0f, sum1 = 0.0f, sum2 = 0.0f, sum3 = 0.0f;

    const int a_base = b * I_dim * J_dim * L_dim 
                     + i * J_dim * L_dim 
                     + j * L_dim;

    // Main vectorized processing
    for (int l = 0; l < L_dim; ++l) {
        const float a_val = A[a_base + l];
        const int b_base = l * K_dim + k_group;
        
        if (valid_group) {
            const float4 b4 = *reinterpret_cast<const float4*>(B + b_base);
            sum0 += a_val * b4.x;
            sum1 += a_val * b4.y;
            sum2 += a_val * b4.z;
            sum3 += a_val * b4.w;
        } else {
            // Handle remaining elements with bounds checking
            for(int k_offset = 0; k_offset < min(4, remaining_ks); ++k_offset) {
                const float b_val = B[b_base + k_offset];
                switch(k_offset) {
                    case 0: sum0 += a_val * b_val; break;
                    case 1: sum1 += a_val * b_val; break;
                    case 2: sum2 += a_val * b_val; break;
                    case 3: sum3 += a_val * b_val; break;
                }
            }
        }
    }

    // Write results with coalesced writes
    const int c_base = b * I_dim * J_dim * K_dim 
                     + i * J_dim * K_dim 
                     + j * K_dim 
                     + k_group;

    if (valid_group) {
        *reinterpret_cast<float4*>(C + c_base) = make_float4(sum0, sum1, sum2, sum3);
    } else {
        for(int k_offset = 0; k_offset < min(4, remaining_ks); ++k_offset) {
            C[c_base + k_offset] = (&sum0)[k_offset];
        }
    }
}

torch::Tensor tensor_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 4, "A must be a 4D tensor");
    TORCH_CHECK(B.dim() == 2, "B must be a 2D matrix");
    
    const int B_dim = A.size(0);
    const int I_dim = A.size(1);
    const int J_dim = A.size(2);
    const int L_dim = A.size(3);
    const int K_dim = B.size(1);
    
    TORCH_CHECK(B.size(0) == L_dim, 
        "B's first dimension must match A's last dimension");

    auto C = torch::zeros({B_dim, I_dim, J_dim, K_dim}, A.options());
    
    const int total_groups = (K_dim + 3) / 4;
    const int total_threads = B_dim * I_dim * J_dim * total_groups;
    const int block_size = 256;
    const int num_blocks = (total_threads + block_size - 1) / block_size;
    
    tensor_matmul_kernel<<<num_blocks, block_size>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        B_dim, I_dim, J_dim, K_dim, L_dim
    );
    
    return C;
}
"""

tensor_matmul_cpp_source = "torch::Tensor tensor_matmul_cuda(torch::Tensor A, torch::Tensor B);"

# Compile the inline CUDA code
tensor_matmul = load_inline(
    name="tensor_matmul",
    cpp_sources=tensor_matmul_cpp_source,
    cuda_sources=tensor_matmul_source,
    functions=["tensor_matmul_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[""]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.tensor_matmul = tensor_matmul

    def forward(self, A, B):
        return self.tensor_matmul.tensor_matmul_cuda(A, B)
```

Kernel 2 (runtime: 26.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel using cuBLAS for optimized 4D tensor-matrix multiplication
custom_gemm_source = """
#include <torch/extension.h>
#include <cublas_v2.h>

torch::Tensor custom_gemm_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 4, "A must be a 4D tensor");
    TORCH_CHECK(B.dim() == 2, "B must be a 2D tensor");
    TORCH_CHECK(A.size(3) == B.size(0), "Mismatch in dimension l");

    int b = A.size(0);
    int i = A.size(1);
    int j = A.size(2);
    int l = A.size(3);
    int k = B.size(1);
    int m = b * i * j;

    // Reshape A to 2D (b*i*j, l) without data copy
    auto A_2d = A.view({m, l});
    auto C = torch::zeros({m, k}, A.options());

    cublasHandle_t handle;
    cublasCreate(&handle);

    float alpha = 1.0f;
    float beta = 0.0f;

    // cuBLAS SGEMM: C = A_2d * B with optimal row-major layout handling
    cublasSgemm(handle,
                CUBLAS_OP_N, CUBLAS_OP_N,
                k, m, l,
                &alpha,
                B.data_ptr<float>(), k,    // B is (l, k) in row-major
                A_2d.data_ptr<float>(), l, // A_2d is (m, l) in row-major
                &beta,
                C.data_ptr<float>(), k);

    cublasDestroy(handle);

    return C.view({b, i, j, k});
}
"""

custom_gemm_cpp = "torch::Tensor custom_gemm_cuda(torch::Tensor A, torch::Tensor B);"

# Load the custom CUDA kernel with cuBLAS linkage
custom_gemm = load_inline(
    name='custom_gemm',
    cpp_sources=custom_gemm_cpp,
    cuda_sources=custom_gemm_source,
    functions=['custom_gemm_cuda'],
    extra_ldflags=['-lcublas'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, A, B):
        return custom_gemm.custom_gemm_cuda(A, B)
```
