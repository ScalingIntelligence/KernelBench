You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 3.38 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel for fused GroupNorm + Mean with coalesced memory access
fused_gnm_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename T>
__inline__ __device__ T warpReduceSum(T val) {
    for (int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

template<typename T>
__inline__ __device__ T blockReduceSum(T val) {
    static __shared__ T shared[32];
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    val = warpReduceSum(val);
    if (lane == 0) shared[wid] = val;
    __syncthreads();

    val = (threadIdx.x < (blockDim.x + 31)/32) ? shared[lane] : 0;
    return wid == 0 ? warpReduceSum(val) : val;
}

__global__ void fused_gnm_kernel(
    const float* __restrict__ input,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ output,
    int B, int C, int D, int H, int W,
    int G, float eps, int elements_per_group,
    int C_per_group, int DHW
) {
    extern __shared__ float s_params[];
    float* s_gamma = s_params;
    float* s_beta = s_params + C_per_group;

    int batch = blockIdx.x / G;
    int group = blockIdx.x % G;
    int start_c = group * C_per_group;
    
    if (batch >= B) return;

    // Load group parameters into shared memory
    for (int i = threadIdx.x; i < C_per_group; i += blockDim.x) {
        s_gamma[i] = gamma[start_c + i];
        s_beta[i] = beta[start_c + i];
    }
    __syncthreads();

    // Phase 1: Compute group statistics with optimized memory pattern
    float sum = 0.0f, sum_sq = 0.0f;
    const int HxW = H * W;
    
    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        int s = i / C_per_group;  // Spatial index
        int c = i % C_per_group;  // Channel index within group
        
        int d = s / HxW;
        int hw = s % HxW;
        int h = hw / W;
        int w = hw % W;
        
        int idx = (((batch*C + start_c + c)*D + d)*H + h)*W + w;
        float val = input[idx];
        sum += val;
        sum_sq += val*val;
    }

    // Block reduction for statistics
    float block_sum = blockReduceSum(sum);
    float block_sum_sq = blockReduceSum(sum_sq);
    
    __shared__ float mean, inv_std;
    if (threadIdx.x == 0) {
        mean = block_sum / elements_per_group;
        inv_std = rsqrtf(block_sum_sq/elements_per_group - mean*mean + eps);
    }
    __syncthreads();

    // Phase 2: Compute normalized sum with coalesced accesses
    float group_sum = 0.0f;
    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        int s = i / C_per_group;
        int c = i % C_per_group;
        
        int d = s / HxW;
        int hw = s % HxW;
        int h = hw / W;
        int w = hw % W;
        
        int idx = (((batch*C + start_c + c)*D + d)*H + h)*W + w;
        float val = (input[idx] - mean) * inv_std;
        group_sum += s_gamma[c] * val + s_beta[c];
    }

    // Final reduction and atomic add with integrated mean
    float block_sum_out = blockReduceSum(group_sum);
    if (threadIdx.x == 0) {
        atomicAdd(&output[batch], block_sum_out / (C*D*H*W));
    }
}

torch::Tensor fused_gnm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps
) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);
    int G = num_groups;
    int C_per_group = C/G;
    int DHW = D * H * W;
    int elements_per_group = C_per_group * DHW;
    
    auto output = torch::zeros(B, input.options());
    size_t shared_mem = 2 * C_per_group * sizeof(float);
    
    dim3 blocks(B * G);
    dim3 threads(256);  // Optimized for memory coalescing
    
    fused_gnm_kernel<<<blocks, threads, shared_mem>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, D, H, W,
        G, eps, elements_per_group,
        C_per_group, DHW
    );
    
    return output;
}
"""

fused_gnm_cpp = """
torch::Tensor fused_gnm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps
);
"""

fused_gnm = load_inline(
    name='fused_gnm',
    cpp_sources=fused_gnm_cpp,
    cuda_sources=fused_gnm_source,
    functions=['fused_gnm_forward'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.num_groups = num_groups
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        self.eps = 1e-5

    def forward(self, x):
        x = self.conv(x)
        return fused_gnm.fused_gnm_forward(
            x, self.gamma, self.beta,
            self.num_groups, self.eps
        )
```

Kernel 2 (runtime: 3.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused GroupNorm + Mean with optimized memory access
fused_gnm_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename T>
__inline__ __device__ T warpReduceSum(T val) {
    for (int offset = 16; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

template<typename T>
__inline__ __device__ T blockReduceSum(T val) {
    static __shared__ T shared[32];
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;

    val = warpReduceSum(val);
    if (lane == 0) shared[wid] = val;
    __syncthreads();

    val = (threadIdx.x < (blockDim.x + 31)/32) ? shared[lane] : 0;
    return wid == 0 ? warpReduceSum(val) : val;
}

__global__ void fused_gnm_kernel(
    const float* __restrict__ input,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ output,
    int B, int C, int D, int H, int W,
    int G, float eps, int elements_per_group,
    int C_per_group
) {
    extern __shared__ float s_gn[];
    float* s_gamma = s_gn;
    float* s_beta = s_gn + C_per_group;

    int batch = blockIdx.x / G;
    int group = blockIdx.x % G;
    int start_c = group * C_per_group;
    
    if (batch >= B) return;

    // Load group's gamma/beta into shared memory
    for (int i = threadIdx.x; i < C_per_group; i += blockDim.x) {
        s_gamma[i] = gamma[start_c + i];
        s_beta[i] = beta[start_c + i];
    }
    __syncthreads();

    // Phase 1: Compute group statistics with coalesced access
    float sum = 0.0f, sum_sq = 0.0f;
    const int HxW = H*W;
    
    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        int spatial_idx = i / C_per_group;
        int c_idx = i % C_per_group;
        
        int d = spatial_idx / HxW;
        int hw = spatial_idx % HxW;
        int h = hw / W;
        int w = hw % W;
        
        int idx = (((batch*C + (start_c + c_idx))*D + d)*H + h)*W + w;
        float val = input[idx];
        sum += val;
        sum_sq += val*val;
    }

    // Block reduction for statistics
    float block_sum = blockReduceSum(sum);
    float block_sum_sq = blockReduceSum(sum_sq);
    
    __shared__ float mean, inv_std;
    if (threadIdx.x == 0) {
        mean = block_sum / elements_per_group;
        inv_std = rsqrtf(block_sum_sq/elements_per_group - mean*mean + eps);
    }
    __syncthreads();

    // Phase 2: Compute normalized sum with shared params
    float group_sum = 0.0f;
    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        int spatial_idx = i / C_per_group;
        int c_idx = i % C_per_group;
        
        int d = spatial_idx / HxW;
        int hw = spatial_idx % HxW;
        int h = hw / W;
        int w = hw % W;
        
        int idx = (((batch*C + (start_c + c_idx))*D + d)*H + h)*W + w;
        float val = (input[idx] - mean) * inv_std;
        group_sum += s_gamma[c_idx] * val + s_beta[c_idx];
    }

    // Atomic add to output and final mean
    float block_sum_out = blockReduceSum(group_sum);
    if (threadIdx.x == 0) {
        atomicAdd(&output[batch], block_sum_out / (C*D*H*W));
    }
}

torch::Tensor fused_gnm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps
) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);
    int G = num_groups;
    int C_per_group = C/G;
    int elements_per_group = C_per_group * D * H * W;
    
    auto output = torch::zeros(B, input.options());
    size_t shared_mem = 2 * C_per_group * sizeof(float);
    
    dim3 blocks(B * G);
    dim3 threads(512);  // Optimized block size
    
    fused_gnm_kernel<<<blocks, threads, shared_mem>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, D, H, W,
        G, eps, elements_per_group,
        C_per_group
    );
    
    return output;
}
"""

fused_gnm_cpp = """
torch::Tensor fused_gnm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps
);
"""

fused_gnm = load_inline(
    name='fused_gnm',
    cpp_sources=fused_gnm_cpp,
    cuda_sources=fused_gnm_source,
    functions=['fused_gnm_forward'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.num_groups = num_groups
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        self.eps = 1e-5

    def forward(self, x):
        x = self.conv(x)
        return fused_gnm.fused_gnm_forward(
            x, self.gamma, self.beta,
            self.num_groups, self.eps
        )
```
