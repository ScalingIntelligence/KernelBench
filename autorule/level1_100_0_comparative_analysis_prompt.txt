You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 6.07 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel with vectorized loads and warp-level reduction
hinge_loss_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void hinge_loss_kernel(const float* pred, const float* target, float* sum_ptr, int batch_size, int d) {
    extern __shared__ float sdata[];
    float* shared_target = sdata;

    int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;

    int tid = threadIdx.x;
    const int warp_size = 32;
    const int elements_per_vector = 4;

    // Load target once per block
    if (tid == 0) {
        shared_target[0] = target[batch_idx];
    }
    __syncthreads();

    float t = shared_target[0];
    float sum = 0.0f;
    int offset = batch_idx * d;

    // Vectorized processing with float4
    const int vector_stride = blockDim.x * elements_per_vector;
    for (int i = tid * elements_per_vector; i < d; i += vector_stride) {
        float4 vec = *reinterpret_cast<const float4*>(pred + offset + i);
        
        // Branch prediction friendly since all threads in block use same t
        if (t == 1.0f) {
            sum += fmaxf(0.0f, 1.0f - vec.x) 
                 + fmaxf(0.0f, 1.0f - vec.y)
                 + fmaxf(0.0f, 1.0f - vec.z)
                 + fmaxf(0.0f, 1.0f - vec.w);
        } else {
            sum += fmaxf(0.0f, 1.0f + vec.x)
                 + fmaxf(0.0f, 1.0f + vec.y)
                 + fmaxf(0.0f, 1.0f + vec.z)
                 + fmaxf(0.0f, 1.0f + vec.w);
        }
    }

    // Warp-level reduction using shuffle instructions
    for (int offset = warp_size/2; offset > 0; offset >>= 1) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
    }

    // First thread in warp stores partial sum
    if (tid % warp_size == 0) {
        sdata[tid / warp_size] = sum;
    }
    __syncthreads();

    // Final reduction using first warp
    if (tid < warp_size) {
        float val = (tid < blockDim.x/warp_size) ? sdata[tid] : 0.0f;
        for (int offset = warp_size/2; offset > 0; offset >>= 1) {
            val += __shfl_down_sync(0xffffffff, val, offset);
        }
        if (tid == 0) {
            atomicAdd(sum_ptr, val);
        }
    }
}

torch::Tensor hinge_loss_cuda(torch::Tensor predictions, torch::Tensor targets) {
    TORCH_CHECK(predictions.dim() == 2, "Predictions must be 2D tensor");
    TORCH_CHECK(targets.dim() == 1, "Targets must be 1D tensor");
    TORCH_CHECK(predictions.size(0) == targets.size(0), "Batch size mismatch");
    
    int batch_size = predictions.size(0);
    int d = predictions.size(1);
    TORCH_CHECK(d % 4 == 0, "Feature dimension must be multiple of 4 for vectorization");
    
    auto sum_tensor = torch::zeros(1, torch::kFloat32).to(predictions.device());
    
    const int block_size = 1024;
    int grid_size = batch_size;
    
    // Shared memory: 1 float for target + space for warp reductions
    size_t shared_mem_size = sizeof(float) + (block_size / 32) * sizeof(float);
    
    hinge_loss_kernel<<<grid_size, block_size, shared_mem_size>>>(
        predictions.data_ptr<float>(),
        targets.data_ptr<float>(),
        sum_tensor.data_ptr<float>(),
        batch_size,
        d
    );
    
    return sum_tensor.squeeze() / (batch_size * d);
}
"""

hinge_loss_cpp_source = "torch::Tensor hinge_loss_cuda(torch::Tensor predictions, torch::Tensor targets);"

# Compile the optimized CUDA extension
hinge_loss = load_inline(
    name='hinge_loss',
    cpp_sources=hinge_loss_cpp_source,
    cuda_sources=hinge_loss_source,
    functions=['hinge_loss_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.hinge_loss = hinge_loss

    def forward(self, predictions, targets):
        # Ensure 4-byte alignment for vectorized loads
        predictions = predictions.contiguous()
        targets = targets.contiguous().to(predictions.dtype)
        return self.hinge_loss.hinge_loss_cuda(predictions, targets)
```

Kernel 2 (runtime: 6.04 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized Hinge Loss CUDA kernel with vectorized memory access
hinge_loss_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void hinge_loss_kernel(const float* predictions, const float* targets, float* output, int N, int M) {
    const int tid = threadIdx.x;
    const int bid = blockIdx.x;
    const int bdim = blockDim.x;
    
    extern __shared__ float sdata[];
    float sum = 0.0f;

    // Vectorized processing using CUDA's built-in float4
    const int vec_size = 4;
    const int vec_idx = (bid * bdim + tid);
    const int total_elements = N * M;
    const int total_vec = (total_elements + vec_size - 1) / vec_size;

    // Grid-stride loop for vectorized access
    for (int i = vec_idx; i < total_vec; i += gridDim.x * blockDim.x) {
        const int base_idx = i * vec_size;
        if (base_idx + 3 < total_elements) {
            float4 pred_vec = *reinterpret_cast<const float4*>(&predictions[base_idx]);
            const int batch_idx = base_idx / M;
            const float target_val = targets[batch_idx];

            sum += fmaxf(1.0f - pred_vec.x * target_val, 0.0f);
            sum += fmaxf(1.0f - pred_vec.y * target_val, 0.0f);
            sum += fmaxf(1.0f - pred_vec.z * target_val, 0.0f);
            sum += fmaxf(1.0f - pred_vec.w * target_val, 0.0f);
        }
    }

    // Block reduction
    sdata[tid] = sum;
    __syncthreads();

    for (int s = blockDim.x/2; s > 0; s >>=1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    // Atomic add to global output
    if (tid == 0) {
        atomicAdd(output, sdata[0]);
    }
}

torch::Tensor hinge_loss_cuda(torch::Tensor predictions, torch::Tensor targets) {
    TORCH_CHECK(predictions.is_contiguous(), "Predictions must be contiguous");
    TORCH_CHECK(predictions.size(0) == targets.size(0), "Batch dimension mismatch");
    
    const int N = predictions.size(0);
    const int M = predictions.size(1);
    auto output = torch::zeros({}, predictions.options());  // Scalar tensor
    
    const int block_size = 256;
    const int grid_size = (N * M + block_size * 4 - 1) / (block_size * 4);
    
    hinge_loss_kernel<<<grid_size, block_size, block_size * sizeof(float)>>>(
        predictions.data_ptr<float>(),
        targets.data_ptr<float>(),
        output.data_ptr<float>(),
        N,
        M
    );
    
    return output / (N * M);
}
"""

hinge_loss_cpp_source = "torch::Tensor hinge_loss_cuda(torch::Tensor predictions, torch::Tensor targets);"

# Load the optimized CUDA extension
hinge_loss = load_inline(
    name='hinge_loss',
    cpp_sources=hinge_loss_cpp_source,
    cuda_sources=hinge_loss_source,
    functions=['hinge_loss_cuda'],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-use_fast_math"]
)

# Optimized model with direct scalar output
class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.hinge_loss = hinge_loss

    def forward(self, predictions, targets):
        return self.hinge_loss.hinge_loss_cuda(predictions, targets)

# Helper functions remain unchanged
batch_size = 32768
input_shape = (32768,)

def get_inputs():
    return [torch.rand(batch_size, *input_shape).cuda(), 
            (torch.randint(0, 2, (batch_size,)).float() * 2 - 1).cuda()]

def get_init_inputs():
    return []
```
