You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 28.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused Swish+GroupNorm+HardSwish CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_swish_groupnorm_hardswish_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int num_channels,
    int groups,
    int depth,
    int height,
    int width,
    float eps) {
    
    const int sample_group = blockIdx.x;
    const int sample = sample_group / groups;
    const int group = sample_group % groups;
    const int channels_per_group = num_channels / groups;
    
    extern __shared__ float shared[];
    float* sum_shared = shared;
    float* sum_sq_shared = &shared[blockDim.x];
    
    const int tid = threadIdx.x;
    float sum = 0.0f, sum_sq = 0.0f;
    const int elements_per_channel = depth * height * width;
    const int elements_per_group = channels_per_group * elements_per_channel;
    
    // First pass: Compute sum and sum of squares with Swish activation
    for (int i = tid; i < elements_per_group; i += blockDim.x) {
        const int c = group * channels_per_group + i / elements_per_channel;
        const int spatial_idx = i % elements_per_channel;
        const int d = spatial_idx / (height * width);
        const int hw = spatial_idx % (height * width);
        const int h = hw / width;
        const int w = hw % width;
        
        const int idx = ((sample * num_channels + c) * depth + d) * height * width + h * width + w;
        const float x = input[idx];
        const float swish = x * (1.0f / (1.0f + expf(-x)));
        
        sum += swish;
        sum_sq += swish * swish;
    }
    
    sum_shared[tid] = sum;
    sum_sq_shared[tid] = sum_sq;
    __syncthreads();
    
    // Parallel reduction
    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sum_shared[tid] += sum_shared[tid + stride];
            sum_sq_shared[tid] += sum_sq_shared[tid + stride];
        }
        __syncthreads();
    }
    
    // Compute statistics
    const float mean = sum_shared[0] / elements_per_group;
    const float var = (sum_sq_shared[0] / elements_per_group) - (mean * mean);
    const float inv_std = rsqrtf(var + eps);
    
    // Second pass: Normalize and apply activations
    for (int i = tid; i < elements_per_group; i += blockDim.x) {
        const int c = group * channels_per_group + i / elements_per_channel;
        const int spatial_idx = i % elements_per_channel;
        const int d = spatial_idx / (height * width);
        const int hw = spatial_idx % (height * width);
        const int h = hw / width;
        const int w = hw % width;
        
        const int idx = ((sample * num_channels + c) * depth + d) * height * width + h * width + w;
        const float x = input[idx];
        const float swish = x * (1.0f / (1.0f + expf(-x)));
        
        // Normalize
        float normalized = (swish - mean) * inv_std;
        // Scale and shift
        normalized = normalized * weight[c] + bias[c];
        // HardSwish
        output[idx] = normalized * fminf(fmaxf(normalized + 3.0f, 0.0f), 6.0f) / 6.0f;
    }
}

torch::Tensor fused_op_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int groups,
    float eps) {
    
    auto output = torch::empty_like(input);
    const int N = input.size(0);
    const int C = input.size(1);
    const int D = input.size(2);
    const int H = input.size(3);
    const int W = input.size(4);
    
    const int threads = 256;
    const int blocks = N * groups;
    const size_t shared_mem = 2 * threads * sizeof(float);
    
    fused_swish_groupnorm_hardswish_kernel<<<blocks, threads, shared_mem>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        C,
        groups,
        D,
        H,
        W,
        eps);
        
    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_op_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int groups, float eps);
"""

# Load CUDA extension
fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_op_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, bias=bias
        )
        self.groups = groups
        self.eps = eps
        self.group_norm_weight = nn.Parameter(torch.ones(out_channels))
        self.group_norm_bias = nn.Parameter(torch.zeros(out_channels))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_op.fused_op_cuda(
            x.contiguous(),
            self.group_norm_weight,
            self.group_norm_bias,
            self.groups,
            self.eps
        )
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups, eps]
```

Kernel 2 (runtime: 29.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused Swish + GroupNorm + HardSwish
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_swish_groupnorm_hardswish_kernel(
    const float* input,
    const float* gamma,
    const float* beta,
    float* output,
    int N, int C, int D, int H, int W, int G,
    int group_size,
    float eps) {
    
    int n = blockIdx.x;
    int g = blockIdx.y;
    int tid = threadIdx.x;
    
    int c_start = g * group_size;
    int num_elements = group_size * D * H * W;
    
    extern __shared__ float smem[];
    float* sum = smem;
    float* sum_sq = &smem[blockDim.x];
    
    // First pass: compute sum and sum_sq with strided access
    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;
    for(int i = tid; i < num_elements; i += blockDim.x) {
        int c = c_start + i / (D * H * W);
        int spatial_idx = i % (D * H * W);
        int d = spatial_idx / (H * W);
        int h = (spatial_idx % (H * W)) / W;
        int w = spatial_idx % W;
        
        int idx = n * C * D * H * W + c * D * H * W + d * H * W + h * W + w;
        float x = input[idx];
        float swish_x = x * (1.0f / (1.0f + __expf(-x)));
        local_sum += swish_x;
        local_sum_sq += swish_x * swish_x;
    }
    
    sum[tid] = local_sum;
    sum_sq[tid] = local_sum_sq;
    __syncthreads();
    
    // Block reduction for sum and sum_sq
    for(int s = blockDim.x/2; s > 0; s >>= 1) {
        if(tid < s) {
            sum[tid] += sum[tid + s];
            sum_sq[tid] += sum_sq[tid + s];
        }
        __syncthreads();
    }
    
    float total_sum = sum[0];
    float total_sum_sq = sum_sq[0];
    float mean = total_sum / num_elements;
    float var = (total_sum_sq / num_elements) - (mean * mean);
    float inv_std = rsqrtf(var + eps);
    
    // Second pass: apply normalization and activations
    for(int i = tid; i < num_elements; i += blockDim.x) {
        int c = c_start + i / (D * H * W);
        int spatial_idx = i % (D * H * W);
        int d = spatial_idx / (H * W);
        int h = (spatial_idx % (H * W)) / W;
        int w = spatial_idx % W;
        
        int idx = n * C * D * H * W + c * D * H * W + d * H * W + h * W + w;
        float x = input[idx];
        float swish_x = x * (1.0f / (1.0f + __expf(-x)));
        
        float normalized = (swish_x - mean) * inv_std * gamma[c] + beta[c];
        float hardswish = normalized * fminf(fmaxf(normalized + 3.0f, 0.0f), 6.0f) / 6.0f;
        output[idx] = hardswish;
    }
}

torch::Tensor fused_swish_groupnorm_hardswish_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int groups,
    float eps) {
    
    auto N = input.size(0);
    auto C = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);
    int group_size = C / groups;
    
    auto output = torch::empty_like(input);
    
    dim3 grid(N, groups);
    int block = 256;
    size_t shared_mem = 2 * block * sizeof(float);
    
    fused_swish_groupnorm_hardswish_kernel<<<grid, block, shared_mem>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, D, H, W, groups,
        group_size,
        eps);
    
    return output;
}
"""

fused_kernel_cpp_source = """
torch::Tensor fused_swish_groupnorm_hardswish_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int groups, float eps);
"""

# Compile CUDA extension
fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_kernel_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=["fused_swish_groupnorm_hardswish_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding, bias=bias
        )
        self.groups = groups
        self.eps = eps
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        
        if out_channels % groups != 0:
            raise ValueError("out_channels must be divisible by groups")

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_swish_groupnorm_hardswish_cuda(
            x, self.gamma, self.beta, self.groups, self.eps
        )
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups, eps]
```
