You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 6.56 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

avg_pool1d_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void avg_pool1d_kernel(
    const float* input,
    float* output,
    int batch_size,
    int in_channels,
    int input_length,
    int output_length,
    int kernel_size,
    int stride,
    int padding
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * in_channels * output_length) return;

    int b = idx / (in_channels * output_length);
    int remaining = idx % (in_channels * output_length);
    int c = remaining / output_length;
    int o = remaining % output_length;

    int start = o * stride - padding;
    float sum = 0.0f;

    #pragma unroll
    for (int i = 0; i < kernel_size; ++i) {
        int pos = start + i;
        if (pos >= 0 && pos < input_length) {
            sum += input[b * in_channels * input_length + c * input_length + pos];
        }
    }

    output[b * in_channels * output_length + c * output_length + o] = sum / kernel_size;
}

torch::Tensor avg_pool1d_cuda(torch::Tensor input, int kernel_size, int stride, int padding) {
    TORCH_CHECK(input.dim() == 3, "Input must be 3D (batch, channels, length)");
    auto input_contig = input.contiguous();
    
    int batch_size = input_contig.size(0);
    int in_channels = input_contig.size(1);
    int input_length = input_contig.size(2);
    int output_length = (input_length + 2 * padding - kernel_size) / stride + 1;

    auto output = torch::empty({batch_size, in_channels, output_length}, input_contig.options());
    int total_elements = batch_size * in_channels * output_length;

    const int block_size = 256;
    int num_blocks = (total_elements + block_size - 1) / block_size;

    avg_pool1d_kernel<<<num_blocks, block_size>>>(
        input_contig.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        input_length,
        output_length,
        kernel_size,
        stride,
        padding
    );

    return output;
}
"""

avg_pool1d_cpp_source = "torch::Tensor avg_pool1d_cuda(torch::Tensor input, int kernel_size, int stride, int padding);"

avg_pool1d = load_inline(
    name="avg_pool1d",
    cpp_sources=avg_pool1d_cpp_source,
    cuda_sources=avg_pool1d_source,
    functions=["avg_pool1d_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return avg_pool1d.avg_pool1d_cuda(x.contiguous(), self.kernel_size, self.stride, self.padding)
```

Kernel 2 (runtime: 6.46 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

avg_pool1d_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void avg_pool1d_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int batch_size,
    int in_channels,
    int input_length,
    int output_length,
    int kernel_size,
    int stride,
    int padding
) {
    int global_idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_elements = batch_size * in_channels * output_length;
    if (global_idx >= total_elements) return;

    // Optimized memory access pattern: [batch][channel][output_pos]
    int j = global_idx % output_length;
    int c = (global_idx / output_length) % in_channels;
    int b = global_idx / (output_length * in_channels);

    int start = j * stride - padding;
    float sum = 0.0f;
    const float inv_kernel_size = 1.0f / kernel_size;

    // Precompute base address for this (b, c) pair
    const int base = (b * in_channels + c) * input_length;

    // Calculate valid window boundaries
    const int valid_start = max(start, 0);
    const int valid_end = min(start + kernel_size, input_length);
    const int valid_count = valid_end - valid_start;

    if (valid_count == kernel_size) {  // Common full-window case
        #pragma unroll(4)  // Increased unroll factor for better ILP
        for (int k = 0; k < kernel_size; ++k) {
            sum += input[base + start + k];
        }
    } else if (valid_count > 0) {  // Partial window case
        #pragma unroll(2)  // Partial unroll for better performance
        for (int k = 0; k < valid_count; ++k) {
            sum += input[base + valid_start + k];
        }
    }

    output[global_idx] = sum * inv_kernel_size;
}

torch::Tensor avg_pool1d_cuda(torch::Tensor input, int kernel_size, int stride, int padding) {
    input = input.contiguous();
    const int batch_size = input.size(0);
    const int in_channels = input.size(1);
    const int input_length = input.size(2);

    const int output_length = (input_length + 2 * padding - kernel_size) / stride + 1;
    auto output = torch::empty({batch_size, in_channels, output_length}, input.options());

    const int total_elements = batch_size * in_channels * output_length;
    const int threads_per_block = 256;
    const int num_blocks = (total_elements + threads_per_block - 1) / threads_per_block;

    avg_pool1d_kernel<<<num_blocks, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        input_length,
        output_length,
        kernel_size,
        stride,
        padding
    );

    return output;
}
"""

avg_pool1d_cpp_source = "torch::Tensor avg_pool1d_cuda(torch::Tensor input, int kernel_size, int stride, int padding);"

avg_pool1d = load_inline(
    name='avg_pool1d',
    cpp_sources=avg_pool1d_cpp_source,
    cuda_sources=avg_pool1d_source,
    functions=['avg_pool1d_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return avg_pool1d.avg_pool1d_cuda(x, self.kernel_size, self.stride, self.padding)
```
