You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 13.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused CUDA kernel for Add+Scale+Sigmoid operations
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_scale_sigmoid_kernel(
    const float* conv_out,
    const float* bias,
    const float* scale,
    float* out,
    int num_elements,
    int C, int H, int W) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= num_elements) return;
    
    int c = (idx / (H * W)) % C;
    float val = conv_out[idx] + bias[c];
    val *= scale[c];
    out[idx] = 1.0f / (1.0f + expf(-val));
}

torch::Tensor fused_add_scale_sigmoid_cuda(
    torch::Tensor conv_out,
    torch::Tensor bias,
    torch::Tensor scale) {
    
    auto out = torch::empty_like(conv_out);
    int num_elements = conv_out.numel();
    int C = conv_out.size(1);
    int H = conv_out.size(2);
    int W = conv_out.size(3);

    const int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_add_scale_sigmoid_kernel<<<num_blocks, block_size>>>(
        conv_out.data_ptr<float>(),
        bias.data_ptr<float>(),
        scale.data_ptr<float>(),
        out.data_ptr<float>(),
        num_elements,
        C, H, W
    );

    return out;
}
"""

fused_ops_cpp = "torch::Tensor fused_add_scale_sigmoid_cuda(torch::Tensor conv_out, torch::Tensor bias, torch::Tensor scale);"

# Load the fused operations CUDA extension
fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=["fused_add_scale_sigmoid_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=["-lcudart"],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_channels)
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_ops.fused_add_scale_sigmoid_cuda(x, self.bias, self.scale)
        x = self.group_norm(x)
        return x
```

Kernel 2 (runtime: 13.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define optimized fused kernel with shared memory for bias/scale
fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_scale_sigmoid_kernel(
    const float* __restrict__ input,
    const float* __restrict__ bias,
    const float* __restrict__ scale,
    float* __restrict__ output,
    int channels,
    int hw,
    int total_elements
) {
    extern __shared__ float s_bias_scale[];
    float* s_bias = s_bias_scale;
    float* s_scale = s_bias_scale + channels;

    // Load bias and scale into shared memory
    int tid = threadIdx.x;
    if (tid < channels) {
        s_bias[tid] = __ldg(bias + tid);
        s_scale[tid] = __ldg(scale + tid);
    }
    __syncthreads();

    // Grid-stride loop for better occupancy
    int idx = blockIdx.x * blockDim.x + tid;
    int stride = blockDim.x * gridDim.x;
    
    while (idx < total_elements) {
        // Vectorized memory access for input
        const float in_val = __ldg(input + idx);
        
        // Calculate channel index using fused dimensions
        const int c = (idx / hw) % channels;
        
        // Fused operations with shared memory access
        const float val = (in_val + s_bias[c]) * s_scale[c];
        output[idx] = 1.0f / (1.0f + __expf(-val));
        
        idx += stride;
    }
}

torch::Tensor fused_add_scale_sigmoid_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    torch::Tensor scale
) {
    auto output = torch::empty_like(input);
    const int total_elements = input.numel();
    const int channels = input.size(1);
    const int hw = input.size(2) * input.size(3);

    // Optimized launch configuration
    const int block_size = 512;
    int num_blocks = (total_elements + block_size - 1) / block_size;
    
    // Calculate shared memory requirements
    const size_t shared_mem = 2 * channels * sizeof(float);
    
    fused_add_scale_sigmoid_kernel<<<num_blocks, block_size, shared_mem>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scale.data_ptr<float>(),
        output.data_ptr<float>(),
        channels,
        hw,
        total_elements
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_add_scale_sigmoid_cuda(torch::Tensor, torch::Tensor, torch::Tensor);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=["fused_add_scale_sigmoid_cuda"],
    verbose=True,
    extra_cuda_cflags=["-use_fast_math"]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):
        super(ModelNew, self).__init__()
        # Maintain original convolution configuration (no padding)
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_channels)
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        # Ensure contiguous memory layout and optimal access pattern
        x = x.contiguous()
        x = self.fused_ops.fused_add_scale_sigmoid_cuda(x, self.bias, self.scale)
        x = self.group_norm(x.contiguous())  # GroupNorm prefers contiguous inputs
        return x
```
