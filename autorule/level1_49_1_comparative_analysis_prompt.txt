You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

max_reduce_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_reduce_dim2_kernel(const float* input, float* output, int batch_size, int dim1, int dim2) {
    int batch = blockIdx.x;
    int m = blockIdx.y;
    int tid = threadIdx.x;

    int input_start = batch * dim1 * dim2 + m * dim2;
    const float* input_ptr = input + input_start;

    float local_max = -FLT_MAX;
    for (int i = tid; i < dim2; i += blockDim.x) {
        float val = input_ptr[i];
        if (val > local_max) {
            local_max = val;
        }
    }

    __shared__ float shared_max[256];
    shared_max[tid] = local_max;
    __syncthreads();

    for (int s = blockDim.x/2; s>0; s>>=1) {
        if (tid < s) {
            if (shared_max[tid + s] > shared_max[tid]) {
                shared_max[tid] = shared_max[tid + s];
            }
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[batch * dim1 + m] = shared_max[0];
    }
}

torch::Tensor max_reduce_dim2_cuda(torch::Tensor input) {
    int batch_size = input.size(0);
    int dim1 = input.size(1);
    int dim2 = input.size(2);

    auto output = torch::empty({batch_size, dim1}, input.options());

    dim3 grid(batch_size, dim1);
    int block_size = 256;

    max_reduce_dim2_kernel<<<grid, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        dim1,
        dim2
    );

    return output;
}
"""

max_reduce_cpp_source = "torch::Tensor max_reduce_dim2_cuda(torch::Tensor input);"

max_reduce_ext = load_inline(
    name="max_reduce",
    cpp_sources=max_reduce_cpp_source,
    cuda_sources=max_reduce_source,
    functions=["max_reduce_dim2_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super(ModelNew, self).__init__()
        self.dim = dim
        self.max_reduce_dim2 = max_reduce_ext.max_reduce_dim2_cuda

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.dim == 2:
            return self.max_reduce_dim2(x)
        else:
            return torch.max(x, dim=self.dim)[0]
```

Kernel 2 (runtime: 14.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

max_reduce_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cfloat>

__global__ void max_reduce_kernel(const float* input, float* output, int reduce_dim,
                                  int size0, int size1, int size2,
                                  int stride0, int stride1, int stride2,
                                  int out_size0, int out_size1,
                                  int out_stride0, int out_stride1) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= out_size0 * out_size1) return;

    int i = idx / out_size1;
    int j = idx % out_size1;

    float max_val = -FLT_MAX;
    if (reduce_dim == 0) {
        for (int k = 0; k < size0; ++k) {
            int input_idx = k*stride0 + i*stride1 + j*stride2;
            max_val = fmaxf(max_val, input[input_idx]);
        }
    }
    else if (reduce_dim == 1) {
        for (int k = 0; k < size1; ++k) {
            int input_idx = i*stride0 + k*stride1 + j*stride2;
            max_val = fmaxf(max_val, input[input_idx]);
        }
    }
    else if (reduce_dim == 2) {
        for (int k = 0; k < size2; ++k) {
            int input_idx = i*stride0 + j*stride1 + k*stride2;
            max_val = fmaxf(max_val, input[input_idx]);
        }
    }

    output[i*out_stride0 + j*out_stride1] = max_val;
}

torch::Tensor max_reduce_cuda(torch::Tensor input, int dim) {
    auto sizes = input.sizes().vec();
    int64_t reduce_size = sizes[dim];
    sizes.erase(sizes.begin() + dim);
    auto output = torch::empty(sizes, input.options());
    
    if (output.numel() == 0) return output;

    int num_output_elements = output.numel();
    dim3 input_sizes = {1, 1, 1};
    dim3 input_strides = {0, 0, 0};

    if (input.dim() == 3) {
        input_sizes.x = input.size(0);
        input_sizes.y = input.size(1);
        input_sizes.z = input.size(2);
        input_strides.x = input.stride(0);
        input_strides.y = input.stride(1);
        input_strides.z = input.stride(2);
    }

    dim3 output_sizes = {1, 1};
    dim3 output_strides = {0, 0};
    if (output.dim() == 2) {
        output_sizes.x = output.size(0);
        output_sizes.y = output.size(1);
        output_strides.x = output.stride(0);
        output_strides.y = output.stride(1);
    }

    const int block_size = 256;
    int num_blocks = (num_output_elements + block_size - 1) / block_size;

    max_reduce_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim,
        input_sizes.x, input_sizes.y, input_sizes.z,
        input_strides.x, input_strides.y, input_strides.z,
        output_sizes.x, output_sizes.y,
        output_strides.x, output_strides.y
    );

    return output;
}
"""

max_reduce_cpp_source = "torch::Tensor max_reduce_cuda(torch::Tensor input, int dim);"

max_reduce = load_inline(
    name="max_reduce",
    cpp_sources=max_reduce_cpp_source,
    cuda_sources=max_reduce_source,
    functions=["max_reduce_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.max_reduce = max_reduce

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.max_reduce.max_reduce_cuda(x, self.dim)
```
