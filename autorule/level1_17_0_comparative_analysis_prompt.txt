You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 54.4 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename scalar_t>
__global__ void matmul_kernel(const scalar_t* __restrict__ A, const scalar_t* __restrict__ B, scalar_t* __restrict__ C,
                             int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row >= M || col >= N) return;
    
    scalar_t c = static_cast<scalar_t>(0);
    
    constexpr int TILE_SIZE = 16;
    __shared__ scalar_t As[TILE_SIZE][TILE_SIZE];
    __shared__ scalar_t Bs[TILE_SIZE][TILE_SIZE];

    for (int k_block = 0; k_block < (K + TILE_SIZE - 1) / TILE_SIZE; ++k_block) {
        int row_A = row;
        int col_A = k_block * TILE_SIZE + threadIdx.x;
        
        int row_B = k_block * TILE_SIZE + threadIdx.y;
        int col_B = col;
        
        // Load A into shared memory
        if (col_A < K) {
            As[threadIdx.y][threadIdx.x] = A[row_A * K + col_A];
        } else {
            As[threadIdx.y][threadIdx.x] = static_cast<scalar_t>(0);
        }

        // Load B into shared memory
        if (row_B < K && col_B < N) {
            Bs[threadIdx.y][threadIdx.x] = B[col_B * K + row_B];
        } else {
            Bs[threadIdx.y][threadIdx.x] = static_cast<scalar_t>(0);
        }

        __syncthreads();

        for (int i = 0; i < TILE_SIZE; ++i) {
            c += As[threadIdx.y][i] * Bs[i][threadIdx.x];
        }

        __syncthreads();
    }

    C[row * N + col] = c;
}

torch::Tensor custom_matmul(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2, "Input A must be 2D");
    TORCH_CHECK(B.dim() == 2, "Input B must be 2D");
    int M = A.size(0), K = A.size(1);
    int N = B.size(0);
    TORCH_CHECK(B.size(1) == K, "Dimension mismatch between A and B");

    auto options = A.options();
    auto C = torch::empty({M, N}, options);

    dim3 block(16, 16);
    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);

    AT_DISPATCH_FLOATING_TYPES(A.scalar_type(), "custom_matmul", ([&] {
        matmul_kernel<scalar_t><<<grid, block>>>(
            A.data_ptr<scalar_t>(),
            B.data_ptr<scalar_t>(),
            C.data_ptr<scalar_t>(),
            M, N, K
        );
    }));

    return C;
}
"""

matmul_wrapper = """
torch::Tensor custom_matmul(torch::Tensor A, torch::Tensor B);
"""

matmul_ext = load_inline(
    name='matmul_ext',
    cpp_sources=matmul_wrapper,
    cuda_sources=matmul_kernel_code,
    functions=['custom_matmul'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return matmul_ext.custom_matmul(A, B)
```

Kernel 2 (runtime: 13.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define BM 64  // Tile size for M dimension
#define BN 64  // Tile size for N dimension
#define BK 16  // Tile size for K dimension

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int N, int K) {
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;

    __shared__ float As[BM][BK];
    __shared__ float Bs[BK][BN];

    float Cres[4][4] = {{0.0f}};

    for (int k_idx = 0; k_idx < K; k_idx += BK) {
        // Load A tile with coalesced access
        #pragma unroll
        for (int i = 0; i < 4; ++i) {
            int row = by * BM + ty * 4 + i;
            int col = k_idx + tx;
            if (row < M && col < K) {
                As[ty * 4 + i][tx] = A[row * K + col];
            } else {
                As[ty * 4 + i][tx] = 0.0f;
            }
        }

        // Load and transpose B tile with bounds checking
        #pragma unroll
        for (int j = 0; j < 4; ++j) {
            int B_row = bx * BN + tx * 4 + j;
            int B_col = k_idx + ty;
            if (B_row < N && B_col < K) {
                Bs[ty][tx * 4 + j] = B[B_row * K + B_col];
            } else {
                Bs[ty][tx * 4 + j] = 0.0f;
            }
        }

        __syncthreads();

        // Compute partial sums with register blocking
        #pragma unroll
        for (int k = 0; k < BK; ++k) {
            float Areg[4] = {As[ty*4][k], As[ty*4+1][k], As[ty*4+2][k], As[ty*4+3][k]};
            float Breg[4] = {Bs[k][tx*4], Bs[k][tx*4+1], Bs[k][tx*4+2], Bs[k][tx*4+3]};
            
            #pragma unroll
            for (int i = 0; i < 4; ++i) {
                #pragma unroll
                for (int j = 0; j < 4; ++j) {
                    Cres[i][j] += Areg[i] * Breg[j];
                }
            }
        }

        __syncthreads();
    }

    // Store results with bounds checking
    #pragma unroll
    for (int i = 0; i < 4; ++i) {
        #pragma unroll
        for (int j = 0; j < 4; ++j) {
            int row = by * BM + ty * 4 + i;
            int col = bx * BN + tx * 4 + j;
            if (row < M && col < N) {
                C[row * N + col] = Cres[i][j];
            }
        }
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Inputs must be 2D tensors");
    
    int M = A.size(0);
    int K = A.size(1);
    int N = B.size(0);
    
    auto C = torch::zeros({M, N}, A.options());

    dim3 grid((N + BN - 1) / BN, (M + BM - 1) / BM);
    dim3 block(16, 16);  // 256 threads per block

    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, N, K);
    
    return C;
}
"""

matmul_cpp_source = "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul = load_inline(
    name="matmul",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return matmul.matmul_cuda(A, B)
```
