You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_relu_bias_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_kernel(const float* conv_out, const float* bias, float* output, int total_elements, int channels, int hw) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;
    
    int c = (idx / hw) % channels; // Correctly map idx to channel
    float val = conv_out[idx];
    val = fmaxf(val, 0.f);
    output[idx] = val + bias[c];
}

torch::Tensor fused_relu_bias_cuda(torch::Tensor conv_out, torch::Tensor bias) {
    TORCH_CHECK(conv_out.dim() == 4, "conv_out must be 4D");
    TORCH_CHECK(bias.dim() == 1, "Bias must be 1D");
    
    int N = conv_out.size(0);
    int C = conv_out.size(1);
    int H = conv_out.size(2);
    int W = conv_out.size(3);
    int total_elements = N * C * H * W;
    
    auto output = torch::empty_like(conv_out);
    
    const int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    fused_relu_bias_kernel<<<grid_size, block_size>>>(
        conv_out.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        total_elements,
        C,
        H * W
    );
    
    return output;
}
"""

fused_relu_bias_cpp = "torch::Tensor fused_relu_bias_cuda(torch::Tensor conv_out, torch::Tensor bias);"

fused_relu_bias = load_inline(
    name='fused_relu_bias',
    cpp_sources=fused_relu_bias_cpp,
    cuda_sources=fused_relu_bias_source,
    functions=['fused_relu_bias_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_op = fused_relu_bias

    def forward(self, x):
        conv_out = self.conv(x)
        bias_flat = self.bias.view(-1)
        return self.fused_op.fused_relu_bias_cuda(conv_out, bias_flat)
```

Kernel 2 (runtime: 12.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused ReLU + Bias Add CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_kernel(
    const float* input,
    const float* bias,
    float* output,
    int num_elements,
    int channels,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= num_elements) return;

    int ch = (idx / (height * width)) % channels;
    float val = input[idx];
    val = fmaxf(val, 0.0f) + bias[ch];
    output[idx] = val;
}

torch::Tensor fused_relu_bias_cuda(torch::Tensor x, torch::Tensor bias) {
    auto output = torch::empty_like(x);
    int num_elements = x.numel();
    auto sizes = x.sizes();
    
    const int channels = sizes[1];
    const int height = sizes[2];
    const int width = sizes[3];
    
    const int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;

    fused_relu_bias_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        channels,
        height,
        width
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_relu_bias_cuda(torch::Tensor x, torch::Tensor bias);"

fused_relu_bias = load_inline(
    name='fused_relu_bias',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_relu_bias_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_relu_bias = fused_relu_bias

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_relu_bias.fused_relu_bias_cuda(x, self.bias)
        return x
```
