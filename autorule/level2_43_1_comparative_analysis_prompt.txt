You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 8.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_logsumexp_relu_kernel(const float* input, float* output,
                                          int B, int C, int D, int H, int W) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;
    
    for(int idx = tid; idx < B*D*H*W; idx += stride) {
        const int b = idx / (D*H*W);
        const int rem = idx % (D*H*W);
        const int d = rem / (H*W);
        const int rem_wh = rem % (H*W);
        const int h = rem_wh / W;
        const int w = rem_wh % W;
        
        float max_val = -INFINITY;
        float sum_exp = 0.0f;
        
        // Compute max value across channels
        for(int c = 0; c < C; ++c) {
            const int offset = ((b*C + c)*D + d)*H*W + h*W + w;
            const float val = input[offset];
            if(val > max_val) max_val = val;
        }
        
        // Compute sum of exponentials after subtracting max
        for(int c = 0; c < C; ++c) {
            const int offset = ((b*C + c)*D + d)*H*W + h*W + w;
            const float val = input[offset];
            sum_exp += expf(val - max_val);
        }
        
        // Apply log-sum-exp and ReLU
        const float result = fmaxf(logf(sum_exp) + max_val, 0.0f);
        output[idx] = result;
    }
}

torch::Tensor fused_logsumexp_relu_cuda(torch::Tensor input) {
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");
    TORCH_CHECK(input.dim() == 5, "Input must be 5D (B, C, D, H, W)");
    
    const int B = input.size(0);
    const int C = input.size(1);
    const int D = input.size(2);
    const int H = input.size(3);
    const int W = input.size(4);
    
    auto output = torch::empty({B, 1, D, H, W}, input.options());
    
    const int threads = 256;
    const int blocks = (B*D*H*W + threads - 1)/threads;
    
    fused_logsumexp_relu_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, D, H, W
    );
    
    return output;
}
"""

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources="torch::Tensor fused_logsumexp_relu_cuda(torch::Tensor input);",
    cuda_sources=fused_ops_source,
    functions=['fused_logsumexp_relu_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size,
                             stride=stride, padding=padding)
        self.max_pool = nn.MaxPool3d(kernel_size=2, stride=2)
        self.fused_op = fused_ops.fused_logsumexp_relu_cuda

    def forward(self, x):
        x = self.conv(x)
        x = self.max_pool(x)
        x = self.fused_op(x)
        return x
```

Kernel 2 (runtime: 8.83 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_logsumexp_relu_kernel(const float* input, float* output,
                                          int B, int C, int D, int H, int W) {
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;
    
    for(int idx = tid; idx < B*D*H*W; idx += stride) {
        const int b = idx / (D*H*W);
        const int rem = idx % (D*H*W);
        const int d = rem / (H*W);
        const int rem_wh = rem % (H*W);
        const int h = rem_wh / W;
        const int w = rem_wh % W;
        
        float max_val = -INFINITY;
        float sum_exp = 0.0f;
        
        for(int c = 0; c < C; ++c) {
            const int offset = ((b*C + c)*D + d)*H*W + h*W + w;
            const float val = input[offset];
            
            if(val > max_val) max_val = val;
        }
        
        sum_exp = 0.0f;
        for(int c = 0; c < C; ++c) {
            const int offset = ((b*C + c)*D + d)*H*W + h*W + w;
            const float val = input[offset];
            sum_exp += expf(val - max_val);
        }
        
        const float result = logf(sum_exp) + max_val;
        output[idx] = fmaxf(result, 0.0f);
    }
}

torch::Tensor fused_logsumexp_relu_cuda(torch::Tensor input) {
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");
    TORCH_CHECK(input.dim() == 5, "Input must be 5D (B, C, D, H, W)");
    
    const int B = input.size(0);
    const int C = input.size(1);
    const int D = input.size(2);
    const int H = input.size(3);
    const int W = input.size(4);
    
    auto output = torch::empty({B, 1, D, H, W}, input.options());
    
    const int threads = 256;
    const int blocks = (B*D*H*W + threads - 1)/threads;
    
    fused_logsumexp_relu_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, D, H, W
    );
    
    return output;
}
"""

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources="torch::Tensor fused_logsumexp_relu_cuda(torch::Tensor input);",
    cuda_sources=fused_ops_source,
    functions=['fused_logsumexp_relu_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size,
                             stride=stride, padding=padding)
        self.max_pool = nn.MaxPool3d(kernel_size=2, stride=2)
        self.fused_op = fused_ops.fused_logsumexp_relu_cuda

    def forward(self, x):
        x = self.conv(x)
        x = self.max_pool(x)
        x = self.fused_op(x)
        return x
```
