You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused ReLU+Bias CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_kernel(
    const float* input,
    const float* bias,
    float* output,
    int batch_size,
    int channels,
    int height,
    int width
) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total_elements = batch_size * channels * height * width;
    
    if (index >= total_elements) return;
    
    int c = (index / (height * width)) % channels;
    float val = input[index];
    output[index] = fmaxf(val, 0.0f) + bias[c];
}

torch::Tensor fused_relu_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    TORCH_CHECK(input.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(bias.is_contiguous(), "Bias tensor must be contiguous");
    
    auto output = torch::empty_like(input);
    int total_elements = input.numel();
    
    const int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    fused_relu_bias_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        input.size(0),
        input.size(1),
        input.size(2),
        input.size(3)
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_relu_bias_cuda(torch::Tensor input, torch::Tensor bias);"

# Load the fused kernel
fused_relu_bias = load_inline(
    name='fused_relu_bias',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_relu_bias_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        
    def forward(self, x):
        x = self.conv(x)
        # Flatten bias to 1D for efficient access in kernel
        return fused_relu_bias.fused_relu_bias_cuda(x, self.bias.view(-1))
```

Kernel 2 (runtime: 12.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused ReLU + bias add kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_kernel(const float* input, const float* bias, float* output, 
                                      int N, int C, int H, int W, int total_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < total_elements) {
        int c = (idx / (H * W)) % C;  // Calculate channel index
        float val = input[idx];
        val = fmaxf(val, 0.0f);      // ReLU
        output[idx] = val + bias[c]; // Add channel-specific bias
    }
}

torch::Tensor fused_relu_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    auto sizes = input.sizes();
    int N = sizes[0], C = sizes[1], H = sizes[2], W = sizes[3];
    auto output = torch::empty_like(input);
    
    int total = N * C * H * W;
    const int block_size = 256;
    int blocks = (total + block_size - 1) / block_size;
    
    fused_relu_bias_kernel<<<blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W, total
    );
    
    return output;
}
"""

cpp_source = "torch::Tensor fused_relu_bias_cuda(torch::Tensor input, torch::Tensor bias);"

# Load the custom CUDA kernel
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_relu_bias_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_relu_bias_cuda(x, self.bias)
        return x
```
