You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 48.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

sgemm_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_SIZE 16

__global__ void sgemm_kernel(const float* A, const float* B, float* C, int M, int N, int K) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;

    float sum = 0.0f;

    for (int i = 0; i < (K + TILE_SIZE - 1) / TILE_SIZE; ++i) {
        // Load tile from A
        if (row < M && (i * TILE_SIZE + tx) < K) {
            As[ty][tx] = A[row * K + (i * TILE_SIZE + tx)];
        } else {
            As[ty][tx] = 0.0f;
        }

        // Load tile from B
        if ((i * TILE_SIZE + ty) < K && col < N) {
            Bs[ty][tx] = B[(i * TILE_SIZE + ty) * N + col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        // Compute partial sums using shared memory tiles
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += As[ty][k] * Bs[k][tx];
        }

        __syncthreads(); // Ensure data is written back before next iteration
    }

    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}

extern "C" {
    __host__ torch::Tensor custom_sgemm(torch::Tensor A, torch::Tensor B) {
        TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Inputs must be 2D");
        int M = A.size(0), K_A = A.size(1);
        int K_B = B.size(0), N = B.size(1);
        TORCH_CHECK(K_A == K_B, "Matrix dimensions must agree");

        auto C = torch::empty({M, N}, A.options());

        dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);
        dim3 threads(TILE_SIZE, TILE_SIZE);

        sgemm_kernel<<<blocks, threads>>>(
            A.contiguous().data_ptr<float>(),
            B.contiguous().data_ptr<float>(),
            C.data_ptr<float>(),
            M, N, K_A
        );

        return C;
    }
}
"""

sgemm_cpp_wrapper = """
extern "C" {
    torch::Tensor custom_sgemm(torch::Tensor A, torch::Tensor B);
}
"""

sgemm_ext = load_inline(
    name='custom_sgemm',
    cpp_sources=sgemm_cpp_wrapper,
    cuda_sources=sgemm_kernel_code,
    functions=['custom_sgemm'],
    verbose=False,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.sgemm = sgemm_ext.custom_sgemm

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.sgemm(A, B)
```

Kernel 2 (runtime: 54.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for optimized matrix multiplication
matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

const int TILE_SIZE = 32;

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int N, int K) {
    // Block indices
    int bx = blockIdx.x;
    int by = blockIdx.y;

    // Thread indices
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    __shared__ float A_tile[TILE_SIZE][64];
    __shared__ float B_tile[64][TILE_SIZE];

    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;

    // Initialize accumulation register
    float sum = 0.0f;

    // Load A tile (TILE_SIZE x 64)
    if (row < M) {
        A_tile[ty][tx] = A[row * K + tx];
        A_tile[ty][tx + 32] = A[row * K + tx + 32];
    } else {
        A_tile[ty][tx] = 0.0f;
        A_tile[ty][tx + 32] = 0.0f;
    }

    // Load B tile (64 x TILE_SIZE)
    if (col < N) {
        B_tile[ty][tx] = B[ty * N + col];
        B_tile[ty + 32][tx] = B[(ty + 32) * N + col];
    } else {
        B_tile[ty][tx] = 0.0f;
        B_tile[ty + 32][tx] = 0.0f;
    }

    __syncthreads();

    // Compute dot product
    #pragma unroll
    for (int k = 0; k < 64; ++k) {
        sum += A_tile[ty][k] * B_tile[k][tx];
    }

    // Write result to global memory
    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K = A.size(1);
    int N = B.size(1);

    auto C = torch::zeros({M, N}, A.options());

    dim3 threads(TILE_SIZE, TILE_SIZE);
    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);

    matmul_kernel<<<blocks, threads>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, N, K
    );

    return C;
}
"""

matmul_cpp_source = "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"

# Compile the inline CUDA code
matmul_cuda = load_inline(
    name='matmul_cuda',
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_cuda_source,
    functions=['matmul_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return matmul_cuda.matmul_cuda(A, B)

M = 16384 * 2
N = 16384 * 2
K = 32 * 2

def get_inputs():
    A = torch.rand(M, K).cuda()
    B = torch.rand(K, N).cuda()
    return [A, B]

def get_init_inputs():
    return []
```
