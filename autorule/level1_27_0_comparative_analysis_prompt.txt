You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 18.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom CUDA kernel for optimized SELU activation
selu_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define ALPHA 1.6732632423543772f
#define SCALE 1.0507009873554805f

__global__ void selu_forward_kernel(const float* __restrict__ input,
                                    float* __restrict__ output,
                                    int num_elements) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        const float x = input[idx];
        output[idx] = SCALE * (x > 0 ? x : ALPHA * (expf(x) - 1));
    }
}

torch::Tensor selu_cuda(torch::Tensor x) {
    auto output = torch::empty_like(x);
    int num_elements = x.numel();
    
    const int block_size = 256;
    int grid_size = (num_elements + block_size - 1) / block_size;
    
    selu_forward_kernel<<<grid_size, block_size>>>(
        x.contiguous().data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    
    return output;
}
"""

selu_cpp_source = "torch::Tensor selu_cuda(torch::Tensor x);"

# Compile the inline CUDA code
selu_extension = load_inline(
    name="selu_extension",
    cpp_sources=selu_cpp_source,
    cuda_sources=selu_source,
    functions=["selu_cuda"],
    verbose=True,
    extra_cuda_cflags=["-O3"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return selu_extension.selu_cuda(x.contiguous())
```

Kernel 2 (runtime: 18.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom SELU CUDA kernel
selu_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void selu_kernel(const float* input, float* output, int num_elements) {
    const float alpha = 1.6732632423543772f;
    const float scale = 1.0507009873554802f;
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float x = input[idx];
        output[idx] = scale * (x > 0 ? x : alpha * (expf(x) - 1));
    }
}

torch::Tensor selu_forward_cuda(torch::Tensor x) {
    auto output = torch::empty_like(x);
    int num_elements = x.numel();
    
    const int threads_per_block = 256;
    int num_blocks = (num_elements + threads_per_block - 1) / threads_per_block;
    
    selu_kernel<<<num_blocks, threads_per_block>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    
    return output;
}
"""

selu_cpp_source = "torch::Tensor selu_forward_cuda(torch::Tensor x);"

# Compile the inline CUDA code
selu_extension = load_inline(
    name="selu_extension",
    cpp_sources=selu_cpp_source,
    cuda_sources=selu_source,
    functions=["selu_forward_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=["-lcudart"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return selu_extension.selu_forward_cuda(x)

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()
    return [x]

def get_init_inputs():
    return []

batch_size = 4096
dim = 393216
```
