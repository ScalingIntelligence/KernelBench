You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

scalar_mult_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scalar_mult_kernel(const float* input, float scalar, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = input[idx] * scalar;
    }
}

extern "C" {
    __host__ torch::Tensor scalar_mult_cuda(torch::Tensor input, float scalar) {
        int64_t size = input.numel();
        torch::Tensor output = torch::empty_like(input);
        
        const int block_size = 512;  // Increased block size
        int num_blocks = (size + block_size - 1) / block_size;
        
        scalar_mult_kernel<<<num_blocks, block_size>>>(
            input.data_ptr<float>(),
            scalar,
            output.data_ptr<float>(),
            size
        );
        
        return output;
    }
}
"""

scalar_mult_cpp_source = "extern \"C\" { torch::Tensor scalar_mult_cuda(torch::Tensor input, float scalar); }"

scalar_mult_ext = load_inline(
    name='scalar_mult',
    cpp_sources=scalar_mult_cpp_source,
    cuda_sources=scalar_mult_source,
    functions=['scalar_mult_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.scalar_mult = scalar_mult_ext

    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:
        return self.scalar_mult.scalar_mult_cuda(A.contiguous(), s)  # Ensure contiguous input
```

Kernel 2 (runtime: 12.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

scalar_mult_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scalar_mult_kernel(const float* input, float scalar, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = input[idx] * scalar;
    }
}

torch::Tensor scalar_mult_cuda(torch::Tensor input, float scalar) {
    int64_t size = input.numel();
    torch::Tensor output = torch::empty_like(input);
    
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    
    scalar_mult_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        scalar,
        output.data_ptr<float>(),
        size
    );
    
    return output;
}
"""

scalar_mult_cpp_source = "torch::Tensor scalar_mult_cuda(torch::Tensor input, float scalar);"

scalar_mult_ext = load_inline(
    name='scalar_mult',
    cpp_sources=scalar_mult_cpp_source,
    cuda_sources=scalar_mult_source,
    functions=['scalar_mult_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.scalar_mult = scalar_mult_ext

    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:
        return self.scalar_mult.scalar_mult_cuda(A, s)
```
