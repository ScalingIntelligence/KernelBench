You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 42.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_SIZE 32
#define SUB_TILE 2

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int N, int K) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    int row = by * TILE_SIZE + ty * SUB_TILE;
    int col = bx * TILE_SIZE + tx * SUB_TILE;

    float sum[SUB_TILE][SUB_TILE] = {{0.0f}};

    for (int k_step = 0; k_step < K; k_step += TILE_SIZE) {
        // Load tiles into shared memory with coalesced access
        #pragma unroll
        for (int i = 0; i < SUB_TILE; ++i) {
            #pragma unroll
            for (int j = 0; j < SUB_TILE; ++j) {
                int a_row = row + i;
                int a_col = k_step + tx * SUB_TILE + j;
                if (a_row < M && a_col < K) {
                    As[ty * SUB_TILE + i][tx * SUB_TILE + j] = A[a_row * K + a_col];
                } else {
                    As[ty * SUB_TILE + i][tx * SUB_TILE + j] = 0.0f;
                }

                int b_row = k_step + ty * SUB_TILE + i;
                int b_col = col + j;
                if (b_row < K && b_col < N) {
                    Bs[ty * SUB_TILE + i][tx * SUB_TILE + j] = B[b_row * N + b_col];
                } else {
                    Bs[ty * SUB_TILE + i][tx * SUB_TILE + j] = 0.0f;
                }
            }
        }

        __syncthreads();

        // Compute partial sums using shared memory tiles
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; ++k) {
            #pragma unroll
            for (int i = 0; i < SUB_TILE; ++i) {
                #pragma unroll
                for (int j = 0; j < SUB_TILE; ++j) {
                    sum[i][j] += As[ty * SUB_TILE + i][k] * Bs[k][tx * SUB_TILE + j];
                }
            }
        }

        __syncthreads();
    }

    // Write results to global memory
    #pragma unroll
    for (int i = 0; i < SUB_TILE; ++i) {
        #pragma unroll
        for (int j = 0; j < SUB_TILE; ++j) {
            int c_row = row + i;
            int c_col = col + j;
            if (c_row < M && c_col < N) {
                C[c_row * N + c_col] = sum[i][j];
            }
        }
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K = A.size(1);
    int N = B.size(1);

    auto C = torch::zeros({M, N}, A.options());

    dim3 block(TILE_SIZE/SUB_TILE, TILE_SIZE/SUB_TILE);
    dim3 grid((N + TILE_SIZE - 1)/TILE_SIZE, (M + TILE_SIZE - 1)/TILE_SIZE);

    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, N, K);

    return C;
}
"""

matmul_cpp_source = "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul_extension = load_inline(
    name='matmul_extension',
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=['matmul_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.matmul = matmul_extension.matmul_cuda

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matmul(A, B)

M = 8205
K = 2949
N = 5921

def get_inputs():
    A = torch.rand(M, K).cuda()
    B = torch.rand(K, N).cuda()
    return [A, B]

def get_init_inputs():
    return []
```

Kernel 2 (runtime: 28.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define TILE_SIZE 64
#define VECTOR_SIZE 4

__global__ void matmul_kernel(const float* __restrict__ A,
                              const float* __restrict__ B,
                              float* __restrict__ C,
                              int M, int N, int K) {
    
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];
    
    const int tile_row = blockIdx.y * TILE_SIZE;
    const int tile_col = blockIdx.x * TILE_SIZE;
    const int thread_row = threadIdx.y * VECTOR_SIZE;
    const int thread_col = threadIdx.x * VECTOR_SIZE;
    
    float sum[VECTOR_SIZE][VECTOR_SIZE] = {0.0f};
    
    for (int t = 0; t < K; t += TILE_SIZE) {
        // Load A tile with vectorization and bounds checking
        #pragma unroll
        for (int i = 0; i < VECTOR_SIZE; i++) {
            const int a_row = tile_row + thread_row + i;
            const int a_col = t + thread_col;
            if (a_row < M) {
                #pragma unroll
                for (int j = 0; j < VECTOR_SIZE; j++) {
                    const int col_idx = a_col + j;
                    As[thread_row+i][thread_col+j] = 
                        (col_idx < K) ? A[a_row*K + col_idx] : 0.0f;
                }
            } else {
                #pragma unroll
                for (int j = 0; j < VECTOR_SIZE; j++) {
                    As[thread_row+i][thread_col+j] = 0.0f;
                }
            }
        }

        // Load B tile with vectorization and bounds checking (transposed)
        #pragma unroll
        for (int j = 0; j < VECTOR_SIZE; j++) {
            const int b_col = tile_col + thread_col + j;
            const int b_row_start = t + thread_row;
            if (b_col < N) {
                #pragma unroll
                for (int i = 0; i < VECTOR_SIZE; i++) {
                    const int row_idx = b_row_start + i;
                    Bs[thread_row+i][thread_col+j] = 
                        (row_idx < K) ? B[row_idx*N + b_col] : 0.0f;
                }
            } else {
                #pragma unroll
                for (int i = 0; i < VECTOR_SIZE; i++) {
                    Bs[thread_row+i][thread_col+j] = 0.0f;
                }
            }
        }

        __syncthreads();

        // Compute partial sums with loop unrolling
        #pragma unroll
        for (int k = 0; k < TILE_SIZE; k++) {
            #pragma unroll
            for (int i = 0; i < VECTOR_SIZE; i++) {
                #pragma unroll
                for (int j = 0; j < VECTOR_SIZE; j++) {
                    sum[i][j] += As[thread_row+i][k] * Bs[k][thread_col+j];
                }
            }
        }

        __syncthreads();
    }

    // Write final results with bounds checking
    #pragma unroll
    for (int i = 0; i < VECTOR_SIZE; i++) {
        #pragma unroll
        for (int j = 0; j < VECTOR_SIZE; j++) {
            const int row = tile_row + thread_row + i;
            const int col = tile_col + thread_col + j;
            if (row < M && col < N) {
                C[row*N + col] = sum[i][j];
            }
        }
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.is_cuda() && B.is_cuda(), "Inputs must be CUDA tensors");
    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Inputs must be 2D tensors");
    
    const int M = A.size(0);
    const int K = A.size(1);
    const int N = B.size(1);
    
    auto C = torch::zeros({M, N}, A.options());
    
    const dim3 block(TILE_SIZE/VECTOR_SIZE, TILE_SIZE/VECTOR_SIZE);
    const dim3 grid((N + TILE_SIZE - 1)/TILE_SIZE, (M + TILE_SIZE - 1)/TILE_SIZE);
    
    matmul_kernel<<<grid, block>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, N, K
    );
    
    return C;
}
"""

matmul_cpp_source = "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul_extension = load_inline(
    name="matmul_extension",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_cuda_source,
    functions=["matmul_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.matmul = matmul_extension.matmul_cuda

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.matmul(A, B)
```
