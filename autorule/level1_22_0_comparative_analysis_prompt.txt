You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 18.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom CUDA kernel for optimized Tanh activation
tanh_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void tanh_kernel(const float* input, float* output, int num_elements) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        output[idx] = tanhf(input[idx]);
    }
}

torch::Tensor custom_tanh_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    const int num_elements = input.numel();
    const int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;
    
    tanh_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    
    return output;
}
"""

tanh_cpp_source = "torch::Tensor custom_tanh_cuda(torch::Tensor input);"

# Compile the inline CUDA code
tanh_extension = load_inline(
    name="tanh_extension",
    cpp_sources=tanh_cpp_source,
    cuda_sources=tanh_cuda_source,
    functions=["custom_tanh_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.tanh_module = tanh_extension

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.tanh_module.custom_tanh_cuda(x)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()  # Moved to CUDA for compatibility
    return [x]

def get_init_inputs():
    return []
```

Kernel 2 (runtime: 18.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for Tanh activation
tanh_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void tanh_kernel(const float* input, float* output, int num_elements) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        output[idx] = tanhf(input[idx]);
    }
}

torch::Tensor fast_tanh_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int num_elements = input.numel();
    
    const int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;
    
    tanh_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    
    return output;
}
"""

tanh_cpp_source = "torch::Tensor fast_tanh_cuda(torch::Tensor input);"

# Compile the CUDA kernel
fast_tanh = load_inline(
    name='fast_tanh',
    cpp_sources=tanh_cpp_source,
    cuda_sources=tanh_kernel_source,
    functions=['fast_tanh_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.tanh_op = fast_tanh

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.tanh_op.fast_tanh_cuda(x)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()  # Move to GPU for CUDA kernel
    return [x]

def get_init_inputs():
    return []
```
