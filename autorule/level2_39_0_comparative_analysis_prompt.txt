You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 35.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused scale + batch norm CUDA kernel with warp reductions and shared memory broadcast
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__inline__ __device__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset /= 2) 
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

__global__ void fused_scale_bn_kernel(
    const float* __restrict__ input,
    const float* __restrict__ scale,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ running_mean,
    float* __restrict__ running_var,
    float* __restrict__ output,
    int batch_size,
    int num_features,
    float eps,
    float momentum,
    bool training) {
    
    const int feature_idx = blockIdx.x;
    if (feature_idx >= num_features) return;

    const int tid = threadIdx.x;
    const int stride = blockDim.x;
    
    const float s = scale[feature_idx];
    const float rm = running_mean[feature_idx];
    const float rv = running_var[feature_idx];
    
    extern __shared__ float shared_mem[];
    const int num_warps = blockDim.x / 32;
    float* shared_sum = shared_mem;
    float* shared_sqsum = shared_mem + num_warps;
    
    float sum = 0.0f, sum_sq = 0.0f;
    if (training) {
        for(int i = tid; i < batch_size; i += stride) {
            const float val = __ldg(&input[i * num_features + feature_idx]) * s;
            sum += val;
            sum_sq += val * val;
        }
    }
    
    // Warp-level reduction
    sum = warpReduceSum(sum);
    sum_sq = warpReduceSum(sum_sq);
    
    if (tid % 32 == 0) {
        const int warp_id = tid / 32;
        shared_sum[warp_id] = sum;
        shared_sqsum[warp_id] = sum_sq;
    }
    __syncthreads();
    
    // Sum across warps
    if (tid < num_warps) {
        sum = shared_sum[tid];
        sum_sq = shared_sqsum[tid];
    } else {
        sum = 0.0f;
        sum_sq = 0.0f;
    }
    
    sum = warpReduceSum(sum);
    sum_sq = warpReduceSum(sum_sq);
    
    float mean, var;
    if (training) {
        if (tid == 0) {
            mean = sum / batch_size;
            var = fmaxf((sum_sq / batch_size) - (mean * mean), 0.0f);
            running_mean[feature_idx] = (1 - momentum) * rm + momentum * mean;
            running_var[feature_idx] = (1 - momentum) * rv + momentum * var;
            // Store in shared memory for broadcast
            shared_sum[0] = mean;
            shared_sqsum[0] = var;
        }
        __syncthreads();  // Ensure all threads see updated shared mem
        mean = shared_sum[0];
        var = shared_sqsum[0];
    } else {
        mean = rm;
        var = rv;
    }
    __syncthreads();
    
    const float inv_std = rsqrtf(var + eps);
    const float gamma_val = gamma[feature_idx];
    const float beta_val = beta[feature_idx];
    
    for(int i = tid; i < batch_size; i += stride) {
        const int idx = i * num_features + feature_idx;
        const float val = __ldg(&input[idx]) * s;
        output[idx] = (val - mean) * inv_std * gamma_val + beta_val;
    }
}

torch::Tensor fused_scale_bn_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps,
    float momentum,
    bool training) {
    
    auto output = torch::empty_like(input);
    const int batch_size = input.size(0);
    const int num_features = input.size(1);
    
    const dim3 blocks(num_features);
    const dim3 threads(512);
    const size_t shared_mem_size = 2 * (threads.x / 32) * sizeof(float);
    
    fused_scale_bn_kernel<<<blocks, threads, shared_mem_size>>>(
        input.data_ptr<float>(),
        scale.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_features,
        eps,
        momentum,
        training
    );
    
    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_scale_bn_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps,
    float momentum,
    bool training);
"""

fused_scale_bn = load_inline(
    name='fused_scale_bn',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_scale_bn_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        self.register_buffer('running_mean', torch.zeros(out_features))
        self.register_buffer('running_var', torch.ones(out_features))
        self.eps = eps
        self.momentum = momentum

    def forward(self, x):
        x = self.gemm(x)
        x = fused_scale_bn.fused_scale_bn_cuda(
            x,
            self.scale,
            self.gamma,
            self.beta,
            self.running_mean,
            self.running_var,
            self.eps,
            self.momentum,
            self.training
        )
        return x
```

Kernel 2 (runtime: 40.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_scale_bn_kernel(
    const float* input,
    const float* scale,
    const float* bn_weight,
    const float* bn_bias,
    float* running_mean,
    float* running_var,
    float* output,
    int batch_size,
    int num_features,
    float eps,
    float momentum,
    bool training) {
    
    int feature_idx = blockIdx.x;
    int tid = threadIdx.x;
    int stride = blockDim.x;

    extern __shared__ float smem[];
    float* sum_buffer = smem;
    float* sq_sum_buffer = &smem[blockDim.x];

    float sum = 0.0f;
    float sq_sum = 0.0f;

    // First pass: compute sum and squared sum
    for(int i = tid; i < batch_size; i += stride) {
        float val = input[i * num_features + feature_idx] * scale[feature_idx];
        sum += val;
        sq_sum += val * val;
    }

    sum_buffer[tid] = sum;
    sq_sum_buffer[tid] = sq_sum;
    __syncthreads();

    // Block reduction for sum
    for(int s = blockDim.x/2; s>0; s>>=1) {
        if(tid < s) {
            sum_buffer[tid] += sum_buffer[tid + s];
            sq_sum_buffer[tid] += sq_sum_buffer[tid + s];
        }
        __syncthreads();
    }

    const float mean = sum_buffer[0] / batch_size;
    const float var = (sq_sum_buffer[0] / batch_size) - (mean * mean);

    // Update running stats if training
    if(training && tid == 0) {
        running_mean[feature_idx] = (1 - momentum) * running_mean[feature_idx] + momentum * mean;
        running_var[feature_idx] = (1 - momentum) * running_var[feature_idx] + momentum * var;
    }

    const float inv_std = 1.0f / sqrt(training ? var + eps : running_var[feature_idx] + eps);
    const float final_mean = training ? mean : running_mean[feature_idx];
    const float final_var = training ? var : running_var[feature_idx];

    // Second pass: normalize and transform
    for(int i = tid; i < batch_size; i += stride) {
        float val = input[i * num_features + feature_idx] * scale[feature_idx];
        float normalized = (val - final_mean) * inv_std;
        output[i * num_features + feature_idx] = normalized * bn_weight[feature_idx] + bn_bias[feature_idx];
    }
}

torch::Tensor fused_scale_bn_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor bn_weight,
    torch::Tensor bn_bias,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps,
    float momentum,
    bool training) {
    
    auto output = torch::empty_like(input);
    int batch_size = input.size(0);
    int num_features = input.size(1);

    const int block_size = 256;
    dim3 grid(num_features);
    dim3 block(block_size);
    
    size_t shared_mem = 2 * block_size * sizeof(float);

    fused_scale_bn_kernel<<<grid, block, shared_mem>>>(
        input.data_ptr<float>(),
        scale.data_ptr<float>(),
        bn_weight.data_ptr<float>(),
        bn_bias.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_features,
        eps,
        momentum,
        training
    );

    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_scale_bn_cuda(
    torch::Tensor input,
    torch::Tensor scale,
    torch::Tensor bn_weight,
    torch::Tensor bn_bias,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps,
    float momentum,
    bool training);
"""

fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_scale_bn_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.bn_weight = nn.Parameter(torch.ones(out_features))
        self.bn_bias = nn.Parameter(torch.zeros(out_features))
        self.register_buffer('running_mean', torch.zeros(out_features))
        self.register_buffer('running_var', torch.ones(out_features))
        self.eps = eps
        self.momentum = momentum

    def forward(self, x):
        x = self.gemm(x)
        x = fused_op.fused_scale_bn_cuda(
            x,
            self.scale,
            self.bn_weight,
            self.bn_bias,
            self.running_mean,
            self.running_var,
            self.eps,
            self.momentum,
            self.training
        )
        return x
```
