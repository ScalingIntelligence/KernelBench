You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 21.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused kernel using warp shuffles and fast math
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_softmax_sigmoid_kernel(float* input, float* output, 
                                            int batch_size, int channels,
                                            int D, int H, int W) {
    extern __shared__ float shared[];
    int tid = threadIdx.x;
    int warp_id = tid / 32;
    int lane_id = tid % 32;
    
    int total_spatial = D * H * W;
    int block_idx = blockIdx.x;
    int batch = block_idx / total_spatial;
    int spatial_idx = block_idx % total_spatial;
    
    int d = spatial_idx / (H * W);
    int hw = spatial_idx % (H * W);
    int h = hw / W;
    int w = hw % W;

    if (batch >= batch_size || tid >= channels) return;

    int input_idx = batch * channels * D * H * W 
                   + tid * D * H * W 
                   + d * H * W 
                   + h * W 
                   + w;
    float val = input[input_idx];

    // Warp-level max reduction
    float warp_max = val;
    for (int offset = 16; offset > 0; offset >>= 1) {
        warp_max = fmaxf(warp_max, __shfl_down_sync(0xffffffff, warp_max, offset));
    }

    if (lane_id == 0) {
        shared[warp_id] = warp_max;
    }
    __syncthreads();

    // Global max reduction across warps
    if (warp_id == 0) {
        float val = (lane_id < (blockDim.x + 31)/32) ? shared[lane_id] : -INFINITY;
        for (int offset = 16; offset > 0; offset >>= 1) {
            val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
        }
        if (lane_id == 0) {
            shared[0] = val;
        }
    }
    __syncthreads();
    float max_val = shared[0];
    __syncthreads();

    // Compute exp with fast math and warp-level sum reduction
    float exp_val = __expf(val - max_val);
    float warp_sum = exp_val;
    for (int offset = 16; offset > 0; offset >>= 1) {
        warp_sum += __shfl_down_sync(0xffffffff, warp_sum, offset);
    }

    if (lane_id == 0) {
        shared[warp_id] = warp_sum;
    }
    __syncthreads();

    // Global sum reduction across warps
    if (warp_id == 0) {
        float sum = (lane_id < (blockDim.x + 31)/32) ? shared[lane_id] : 0.0f;
        for (int offset = 16; offset > 0; offset >>= 1) {
            sum += __shfl_down_sync(0xffffffff, sum, offset);
        }
        if (lane_id == 0) {
            shared[0] = sum;
        }
    }
    __syncthreads();
    float sum_exp = shared[0];

    // Fast sigmoid approximation
    float result = exp_val / sum_exp;
    result = 1.0f / (1.0f + __expf(-result));
    output[input_idx] = result;
}

torch::Tensor fused_softmax_sigmoid_cuda(torch::Tensor input) {
    auto sizes = input.sizes();
    auto output = torch::empty_like(input);
    
    int batch_size = sizes[0];
    int channels = sizes[1];
    int D = sizes[2];
    int H = sizes[3];
    int W = sizes[4];
    
    dim3 blocks(batch_size * D * H * W);
    dim3 threads(channels);
    int num_warps = (channels + 31) / 32;
    size_t shared_size = num_warps * sizeof(float);
    
    fused_softmax_sigmoid_kernel<<<blocks, threads, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        D,
        H,
        W
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_softmax_sigmoid_cuda(torch::Tensor input);"

fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_softmax_sigmoid_cuda'],
    verbose=True,
    extra_cuda_cflags=['-use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, 
                 stride, padding, output_padding, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding, bias=bias
        )
        
    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_op.fused_softmax_sigmoid_cuda(x.contiguous())
        return x
```

Kernel 2 (runtime: 8.46 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused activation kernel with parallel reductions
fused_activation_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

template<typename scalar_t>
__global__ void fused_softmax_sigmoid_kernel(
    scalar_t* output,
    const scalar_t* input,
    int num_channels,
    int spatial_size
) {
    extern __shared__ __align__(sizeof(scalar_t)) unsigned char shared_mem_bytes[];
    scalar_t* shared_mem = reinterpret_cast<scalar_t*>(shared_mem_bytes);
    
    const int spatial_idx = blockIdx.x;
    const int channel_idx = threadIdx.x;

    if (spatial_idx >= spatial_size || channel_idx >= num_channels) return;

    const scalar_t* channel_slice = input + spatial_idx * num_channels;
    scalar_t* out_slice = output + spatial_idx * num_channels;

    // Load value and find max
    scalar_t val = channel_slice[channel_idx];
    shared_mem[channel_idx] = val;
    __syncthreads();

    // Parallel max reduction
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(channel_idx < stride && channel_idx + stride < num_channels) {
            shared_mem[channel_idx] = max(shared_mem[channel_idx], shared_mem[channel_idx + stride]);
        }
        __syncthreads();
    }
    const scalar_t max_val = shared_mem[0];
    __syncthreads();

    // Compute exponentials and sum
    scalar_t exp_val = exp(val - max_val);
    shared_mem[channel_idx] = exp_val;
    __syncthreads();

    // Parallel sum reduction
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(channel_idx < stride && channel_idx + stride < num_channels) {
            shared_mem[channel_idx] += shared_mem[channel_idx + stride];
        }
        __syncthreads();
    }
    const scalar_t sum_exp = shared_mem[0];
    __syncthreads();

    // Compute final value with fused activation
    const scalar_t sm_val = exp_val / sum_exp;
    out_slice[channel_idx] = scalar_t(1.0) / (scalar_t(1.0) + exp(-sm_val));
}

torch::Tensor fused_activation_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    const int num_channels = input.size(1);
    const int spatial_size = input.size(0) * input.size(2) * input.size(3) * input.size(4);

    TORCH_CHECK(num_channels <= 1024, "Channel dimension exceeds maximum block size");
    
    const dim3 grid(spatial_size);
    const dim3 block(num_channels);
    const size_t shared_mem = num_channels * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "fused_activation", ([&] {
        fused_softmax_sigmoid_kernel<scalar_t><<<grid, block, shared_mem>>>(
            output.data_ptr<scalar_t>(),
            input.data_ptr<scalar_t>(),
            num_channels,
            spatial_size
        );
    }));

    return output;
}
"""

fused_activation_cpp = "torch::Tensor fused_activation_cuda(torch::Tensor input);"

fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding, bias=bias
        )

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_activation.fused_activation_cuda(x)
        return x
```
