You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 34.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernels for memory-efficient Frobenius normalization
frobenius_kernel_code = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

// Vectorized sum of squares with two-stage hierarchical reduction
__global__ void sum_squares_partial_kernel(const float* input, float* block_sums, int num_elements) {
    extern __shared__ float shared[];
    const int tid = threadIdx.x;
    const int warp_id = tid / 32;
    const int lane_id = tid % 32;
    const int elements_per_block = blockDim.x * 4;
    
    float sum = 0.0f;
    int idx = blockIdx.x * elements_per_block + tid * 4;
    
    // Vectorized load with bounds checking
    if (idx + 3 < num_elements) {
        float4 vals = *reinterpret_cast<const float4*>(input + idx);
        sum += vals.x*vals.x + vals.y*vals.y + vals.z*vals.z + vals.w*vals.w;
    } else {
        for (int i = 0; i < 4 && (idx + i) < num_elements; i++) {
            sum += input[idx + i] * input[idx + i];
        }
    }

    // Warp-level reduction using shuffle instructions
    for (int offset = 16; offset > 0; offset >>= 1) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
    }
    
    // Store warp sums to shared memory
    if (lane_id == 0) {
        shared[warp_id] = sum;
    }
    __syncthreads();

    // Final block reduction
    if (warp_id == 0 && lane_id < blockDim.x / 32) {
        sum = shared[lane_id];
        for (int offset = 8; offset > 0; offset >>= 1) {
            sum += __shfl_down_sync(0xffffffff, sum, offset);
        }
        if (tid == 0) {
            block_sums[blockIdx.x] = sum;
        }
    }
}

// Final reduction kernel using warp shuffles
__global__ void sum_partial_kernel(const float* partial_sums, float* output, int num_partials) {
    const int tid = threadIdx.x;
    float sum = 0.0f;
    
    // Process multiple elements per thread
    for (int i = tid; i < num_partials; i += blockDim.x) {
        sum += partial_sums[i];
    }
    
    // Warp-level reduction
    for (int offset = 16; offset > 0; offset >>= 1) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
    }
    
    if (tid == 0) {
        output[0] = sum;
    }
}

torch::Tensor sum_squares_cuda(torch::Tensor input) {
    const int num_elements = input.numel();
    const int block_size = 256;
    const int elements_per_block = block_size * 4;
    int grid_size = (num_elements + elements_per_block - 1) / elements_per_block;

    // Clamp grid size to device limits
    int max_grid_size;
    cudaDeviceGetAttribute(&max_grid_size, cudaDevAttrMaxGridDimX, 0);
    grid_size = min(grid_size, max_grid_size);

    auto block_sums = torch::zeros(grid_size, input.options().dtype(torch::kFloat32));
    const size_t shared_mem = (block_size / 32) * sizeof(float);

    sum_squares_partial_kernel<<<grid_size, block_size, shared_mem>>>(
        input.data_ptr<float>(),
        block_sums.data_ptr<float>(),
        num_elements
    );

    auto output = torch::zeros(1, input.options());
    sum_partial_kernel<<<1, 256>>>(block_sums.data_ptr<float>(), output.data_ptr<float>(), grid_size);

    return output;
}

// Optimized element-wise multiplication with vectorized in-place operation
__global__ void mul_scalar_kernel(float* input, const float* scalar, int num_elements) {
    const float scale = *scalar;
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int elements_per_thread = 4;
    const int stride = blockDim.x * gridDim.x * elements_per_thread;
    int idx = tid * elements_per_thread;

    // Process 4 elements per thread with vectorized store
    if (idx + 3 < num_elements) {
        float4 vals = *reinterpret_cast<float4*>(input + idx);
        vals.x *= scale;
        vals.y *= scale;
        vals.z *= scale;
        vals.w *= scale;
        *reinterpret_cast<float4*>(input + idx) = vals;
    } else {
        for (int i = 0; i < elements_per_thread && (idx + i) < num_elements; i++) {
            input[idx + i] *= scale;
        }
    }
}

torch::Tensor mul_scalar_cuda(torch::Tensor input, torch::Tensor scalar) {
    const int num_elements = input.numel();
    const int block_size = 256;
    const int elements_per_block = block_size * 4;
    int grid_size = (num_elements + elements_per_block - 1) / elements_per_block;

    // Clamp grid size to device limits
    int max_grid_size;
    cudaDeviceGetAttribute(&max_grid_size, cudaDevAttrMaxGridDimX, 0);
    grid_size = min(grid_size, max_grid_size);

    mul_scalar_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        scalar.data_ptr<float>(),
        num_elements
    );

    return input;
}
"""

frobenius_cpp_code = """
torch::Tensor sum_squares_cuda(torch::Tensor input);
torch::Tensor mul_scalar_cuda(torch::Tensor input, torch::Tensor scalar);
"""

# Load custom operations
frobenius_ops = load_inline(
    name='frobenius_ops',
    cpp_sources=frobenius_cpp_code,
    cuda_sources=frobenius_kernel_code,
    functions=['sum_squares_cuda', 'mul_scalar_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.ops = frobenius_ops

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        sum_sq = self.ops.sum_squares_cuda(x)
        inv_norm = 1.0 / torch.sqrt(sum_sq)
        return self.ops.mul_scalar_cuda(x, inv_norm)
```

Kernel 2 (runtime: 79.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void compute_frobenius_norm(float* input, float* output, int num_elements) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int bid = blockIdx.x;
    int stride = blockDim.x * gridDim.x;
    double sum_sq = 0.0;

    for (int i = tid; i < num_elements; i += stride) {
        float val = input[i];
        sum_sq += static_cast<double>(val) * val;
    }

    sdata[tid] = static_cast<float>(sum_sq);
    __syncthreads();

    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(output, sdata[0]);
    }
}

__global__ void apply_division(float* input, float norm_inv, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        input[idx] *= norm_inv;
    }
}

torch::Tensor fused_forward_cuda(torch::Tensor x) {
    auto device = x.device().type();
    TORCH_CHECK(device == torch::kCUDA, "Input must be CUDA tensor");

    auto ones = torch::ones({}, x.options());
    auto output = torch::empty({}, x.options());

    int num_elements = x.numel();
    const int threads_per_block = 1024;
    const int blocks_per_grid = (threads_per_block * ((num_elements + threads_per_block - 1) / threads_per_block)) / threads_per_block;

    compute_frobenius_norm<<<blocks_per_grid, threads_per_block, threads_per_block * sizeof(float)>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    cudaDeviceSynchronize();

    float norm = output.item<float>();
    float norm_inv = 1.0f / norm;

    const int blocks_div = (num_elements + threads_per_block - 1) / threads_per_block;
    apply_division<<<blocks_div, threads_per_block>>>(
        x.data_ptr<float>(),
        norm_inv,
        num_elements
    );

    return x;
}
"""

cpp_wrapper = "torch::Tensor fused_forward_cuda(torch::Tensor x);"

fused_op = load_inline(
    name='fused_op',
    cpp_sources=cpp_wrapper,
    cuda_sources=fused_kernel_code,
    functions=['fused_forward_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.fused_forward = fused_op.fused_forward_cuda

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.fused_forward(x.contiguous())
```
