You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 18.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom CUDA kernel for LeakyReLU
leaky_relu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void leaky_relu_kernel(const float* input, float* output, float negative_slope, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float val = input[idx];
        output[idx] = val > 0.0f ? val : val * negative_slope;
    }
}

torch::Tensor leaky_relu_cuda(torch::Tensor input, float negative_slope) {
    auto output = torch::empty_like(input);
    int num_elements = input.numel();
    
    const int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;
    
    leaky_relu_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        negative_slope,
        num_elements
    );
    
    return output;
}
"""

leaky_relu_cpp_source = "torch::Tensor leaky_relu_cuda(torch::Tensor input, float negative_slope);"

# Compile the inline CUDA code
leaky_relu_ext = load_inline(
    name='leaky_relu_ext',
    cpp_sources=leaky_relu_cpp_source,
    cuda_sources=leaky_relu_source,
    functions=['leaky_relu_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, negative_slope: float = 0.01):
        super().__init__()
        self.negative_slope = negative_slope
        self.leaky_relu_cuda = leaky_relu_ext.leaky_relu_cuda
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.leaky_relu_cuda(x, self.negative_slope)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()
    return [x]

def get_init_inputs():
    return []
```

Kernel 2 (runtime: 18.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom LeakyReLU CUDA kernel
leaky_relu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void leaky_relu_kernel(const float* x, float* out, int size, float negative_slope) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = x[idx] > 0 ? x[idx] : x[idx] * negative_slope;
    }
}

torch::Tensor leaky_relu_cuda(torch::Tensor x, float negative_slope) {
    auto out = torch::empty_like(x);
    int size = x.numel();
    
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    
    leaky_relu_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        size,
        negative_slope
    );
    
    return out;
}
"""

leaky_relu_cpp_source = "torch::Tensor leaky_relu_cuda(torch::Tensor x, float negative_slope);"

# Compile the inline CUDA code
leaky_relu_module = load_inline(
    name="leaky_relu",
    cpp_sources=leaky_relu_cpp_source,
    cuda_sources=leaky_relu_source,
    functions=["leaky_relu_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, negative_slope: float = 0.01):
        super().__init__()
        self.negative_slope = negative_slope
        self.leaky_relu_cuda = leaky_relu_module.leaky_relu_cuda

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.leaky_relu_cuda(x, self.negative_slope)
```
