You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.39 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom fused kernel implementation
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename T>
__global__ void fused_groupnorm_swish_mul_swish_kernel(
    const T* input,
    const T* gamma,
    const T* beta,
    const T* mul_weight,
    T* output,
    int batch_size,
    int num_features,
    int num_groups,
    float eps) {
    
    const int group_id = blockIdx.x;
    const int sample_id = blockIdx.y;
    const int features_per_group = num_features / num_groups;
    const int tid = threadIdx.x;

    extern __shared__ __align__(sizeof(T)) unsigned char shared_mem[];
    T* shared_sum = reinterpret_cast<T*>(shared_mem);
    T* shared_sqsum = &shared_sum[features_per_group];

    const int feature_idx = group_id * features_per_group + tid;
    const int input_idx = sample_id * num_features + feature_idx;
    
    // Load input and initialize shared memory
    T val = input[input_idx];
    shared_sum[tid] = val;
    shared_sqsum[tid] = val * val;
    __syncthreads();

    // Parallel reduction for sum and sum of squares
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(tid < stride) {
            shared_sum[tid] += shared_sum[tid + stride];
            shared_sqsum[tid] += shared_sqsum[tid + stride];
        }
        __syncthreads();
    }

    if(tid == 0) {
        T mean = shared_sum[0] / features_per_group;
        T var = (shared_sqsum[0] / features_per_group) - (mean * mean);
        var += static_cast<T>(eps);
        T inv_std = rsqrt(var);
        
        shared_sum[0] = mean;
        shared_sqsum[0] = inv_std;
    }
    __syncthreads();

    // Normalization and activation fusion
    T norm = (val - shared_sum[0]) * shared_sqsum[0];
    T scaled = norm * gamma[feature_idx] + beta[feature_idx];
    
    // First swish
    T sigmoid1 = static_cast<T>(1.0) / (static_cast<T>(1.0) + exp(-scaled));
    T swish1 = scaled * sigmoid1;
    
    // Multiply and second swish
    T multiplied = swish1 * mul_weight[feature_idx];
    T sigmoid2 = static_cast<T>(1.0) / (static_cast<T>(1.0) + exp(-multiplied));
    output[input_idx] = multiplied * sigmoid2;
}

torch::Tensor fused_operation_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor mul_weight,
    int num_groups,
    float eps=1e-5) {
    
    auto output = torch::empty_like(input);
    const int batch_size = input.size(0);
    const int num_features = input.size(1);
    const int features_per_group = num_features / num_groups;

    dim3 grid(num_groups, batch_size);
    dim3 block(features_per_group);
    size_t shared_size = 2 * features_per_group * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "fused_op", ([&] {
        fused_groupnorm_swish_mul_swish_kernel<scalar_t><<<grid, block, shared_size>>>(
            input.data_ptr<scalar_t>(),
            gamma.data_ptr<scalar_t>(),
            beta.data_ptr<scalar_t>(),
            mul_weight.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            num_features,
            num_groups,
            eps);
    }));
    
    return output;
}
"""

fused_kernel_cpp = """
torch::Tensor fused_operation_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor mul_weight,
    int num_groups,
    float eps=1e-5);
"""

# Load the custom CUDA kernel
fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_operation_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)  # Retain for params
        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))
        self.eps = 1e-5

    def forward(self, x):
        x = self.gemm(x)
        x = fused_op.fused_operation_cuda(
            x,
            self.group_norm.weight,
            self.group_norm.bias,
            self.multiply_weight,
            self.group_norm.num_groups,
            self.eps
        )
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
num_groups = 256
multiply_weight_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features, num_groups, multiply_weight_shape]
```

Kernel 2 (runtime: 7.44 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_op_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_swish_multiply_swish_kernel(
    const float* input,
    const float* weight,
    float* output,
    int num_elements,
    int out_features
) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= num_elements) return;

    const float x = input[idx];
    const float s = 1.0f / (1.0f + expf(-x));
    float y = x * s;
    
    const int feature_idx = idx % out_features;
    y *= weight[feature_idx];
    
    const float s2 = 1.0f / (1.0f + expf(-y));
    output[idx] = y * s2;
}

torch::Tensor fused_swish_multiply_swish_cuda(
    torch::Tensor input,
    torch::Tensor weight
) {
    TORCH_CHECK(input.dim() == 2, "Input must be 2D");
    TORCH_CHECK(weight.dim() == 1, "Weight must be 1D");
    TORCH_CHECK(input.size(1) == weight.size(0), "Feature dimension mismatch");

    auto output = torch::empty_like(input);
    const int num_elements = input.numel();
    const int out_features = input.size(1);
    
    const int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;
    
    fused_swish_multiply_swish_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        out_features
    );
    
    return output;
}
"""

fused_op_cpp = "torch::Tensor fused_swish_multiply_swish_cuda(torch::Tensor input, torch::Tensor weight);"

fused_op = load_inline(
    name='fused_op',
    cpp_sources=fused_op_cpp,
    cuda_sources=fused_op_source,
    functions=['fused_swish_multiply_swish_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, multiply_weight_shape):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.multiply_weight = nn.Parameter(torch.randn(multiply_weight_shape))

    def forward(self, x):
        x = self.gemm(x)
        x = self.group_norm(x)
        x = fused_op.fused_swish_multiply_swish_cuda(x, self.multiply_weight)
        return x
```
