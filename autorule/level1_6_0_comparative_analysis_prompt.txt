You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 30.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for blocked matrix multiplication
blocked_matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 32

__global__ void blocked_matmul_kernel(
    const float* A, const float* B, float* C,
    int M, int N, int K
) {
    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    int row = by * BLOCK_SIZE + ty;
    int col = bx * BLOCK_SIZE + tx;

    float Cvalue = 0.0f;

    for (int m = 0; m < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; ++m) {
        if (row < M && m * BLOCK_SIZE + tx < K) {
            As[ty][tx] = A[row * K + m * BLOCK_SIZE + tx];
        } else {
            As[ty][tx] = 0.0f;
        }

        if (col < N && m * BLOCK_SIZE + ty < K) {
            Bs[ty][tx] = B[(m * BLOCK_SIZE + ty) * N + col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        for (int k = 0; k < BLOCK_SIZE; ++k) {
            Cvalue += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    if (row < M && col < N) {
        C[row * N + col] = Cvalue;
    }
}

torch::Tensor blocked_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    auto M = A.size(0);
    auto N = B.size(1);
    auto K = A.size(1);

    auto C = torch::zeros({M, N}, A.options());

    dim3 blocks((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (M + BLOCK_SIZE - 1) / BLOCK_SIZE);
    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);

    blocked_matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, N, K);

    return C;
}
"""

blocked_matmul_cpp_source = (
    "torch::Tensor blocked_matmul_cuda(torch::Tensor A, torch::Tensor B);"
)

# Compile the inline CUDA code for blocked matrix multiplication
blocked_matmul = load_inline(
    name="blocked_matmul",
    cpp_sources=blocked_matmul_cpp_source,
    cuda_sources=blocked_matmul_source,
    functions=["blocked_matmul_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.blocked_matmul = blocked_matmul

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.blocked_matmul.blocked_matmul_cuda(A, B)


M = 256
N = 256
K = 131072 * 4

def get_inputs():
    A = torch.rand(M, K)
    B = torch.rand(K, N)
    return [A, B]

def get_init_inputs():
    return []
```

Kernel 2 (runtime: 25.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the optimized CUDA kernel for matrix multiplication
matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

#define TILE_SIZE 32
#define VECTOR_SIZE 4

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int N, int K) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    int tx = threadIdx.x;    // 0-7 (handles 4 elements per thread)
    int ty = threadIdx.y;    // 0-31
    int bx = blockIdx.x;
    int by = blockIdx.y;

    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx * VECTOR_SIZE;

    float sum[VECTOR_SIZE] = {0.0f};

    for (int k_step = 0; k_step < K; k_step += TILE_SIZE) {
        // Load A tile with vectorized access
        int a_col = k_step + tx * VECTOR_SIZE;
        if (row < M && a_col + (VECTOR_SIZE-1) < K) {
            float4 a_val = *reinterpret_cast<const float4*>(&A[row*K + a_col]);
            As[ty][tx*VECTOR_SIZE] = a_val.x;
            As[ty][tx*VECTOR_SIZE+1] = a_val.y;
            As[ty][tx*VECTOR_SIZE+2] = a_val.z;
            As[ty][tx*VECTOR_SIZE+3] = a_val.w;
        } else {
            for (int i = 0; i < VECTOR_SIZE; ++i) {
                As[ty][tx*VECTOR_SIZE+i] = (a_col+i < K && row < M) ? A[row*K + a_col+i] : 0.0f;
            }
        }

        // Load B tile with vectorized access
        int b_row = k_step + ty;
        int b_col = col;
        if (b_row < K && b_col + (VECTOR_SIZE-1) < N) {
            float4 b_val = *reinterpret_cast<const float4*>(&B[b_row*N + b_col]);
            Bs[ty][tx*VECTOR_SIZE] = b_val.x;
            Bs[ty][tx*VECTOR_SIZE+1] = b_val.y;
            Bs[ty][tx*VECTOR_SIZE+2] = b_val.z;
            Bs[ty][tx*VECTOR_SIZE+3] = b_val.w;
        } else {
            for (int i = 0; i < VECTOR_SIZE; ++i) {
                Bs[ty][tx*VECTOR_SIZE+i] = (b_row < K && (b_col+i) < N) ? B[b_row*N + b_col+i] : 0.0f;
            }
        }

        __syncthreads();

        // Compute partial sums using register blocking
        for (int i = 0; i < TILE_SIZE; ++i) {
            float a = As[ty][i];
            #pragma unroll
            for (int j = 0; j < VECTOR_SIZE; ++j) {
                sum[j] += a * Bs[i][tx*VECTOR_SIZE + j];
            }
        }

        __syncthreads();
    }

    // Store results with vectorized write
    if (row < M && col < N) {
        for (int j = 0; j < VECTOR_SIZE; ++j) {
            if (col + j < N) {
                C[row*N + col + j] = sum[j];
            }
        }
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Inputs must be 2D tensors");
    int M = A.size(0);
    int K = A.size(1);
    int N = B.size(1);
    TORCH_CHECK(K == B.size(0), "Matrix dimensions mismatch");

    auto C = torch::zeros({M, N}, A.options());

    dim3 block(8, 32);  // 256 threads per block
    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);

    matmul_kernel<<<grid, block>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, N, K
    );

    return C;
}
"""

matmul_cpp_source = "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"

# Compile the optimized CUDA extension
matmul_cuda_extension = load_inline(
    name="matmul_cuda",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_cuda_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cuda_cflags=["-O3", "--use_fast_math"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return matmul_cuda_extension.matmul_cuda(A, B)

# Input generation remains the same but ensures GPU placement
M = 256
N = 256
K = 131072 * 4

def get_inputs():
    A = torch.rand(M, K).cuda()
    B = torch.rand(K, N).cuda()
    return [A, B]

def get_init_inputs():
    return []
```
