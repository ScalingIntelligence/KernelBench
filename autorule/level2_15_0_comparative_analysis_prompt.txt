You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 5.97 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused mean subtraction
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_mean_subtract_kernel(const float* input, float* output, 
                                           int N, int C, int D, int H, int W) {
    // Each block handles one (n, c)
    int n = blockIdx.x / C;
    int c = blockIdx.x % C;
    
    int num_elements = D * H * W;
    float sum = 0.0f;

    // Grid-stride loop to compute sum
    for (int i = threadIdx.x; i < num_elements; i += blockDim.x) {
        int d = i / (H * W);
        int hw = i % (H * W);
        int h = hw / W;
        int w = hw % W;
        int idx = n * C * D * H * W + c * D * H * W + d * H * W + h * W + w;
        sum += __ldg(input + idx);
    }

    // Sum reduction within the block
    __shared__ float shared_sum[256];
    shared_sum[threadIdx.x] = sum;
    __syncthreads();

    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
        }
        __syncthreads();
    }

    float mean = shared_sum[0] / num_elements;

    // Subtract mean from each element
    for (int i = threadIdx.x; i < num_elements; i += blockDim.x) {
        int d = i / (H * W);
        int hw = i % (H * W);
        int h = hw / W;
        int w = hw % W;
        int idx = n * C * D * H * W + c * D * H * W + d * H * W + h * W + w;
        output[idx] = __ldg(input + idx) - mean;
    }
}

torch::Tensor fused_mean_subtract_cuda(torch::Tensor input) {
    auto N = input.size(0);
    auto C = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);
    auto output = torch::empty_like(input);

    int num_blocks = N * C;
    int block_size = 256;

    fused_mean_subtract_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, D, H, W
    );

    return output;
}
"""

cpp_wrapper = "torch::Tensor fused_mean_subtract_cuda(torch::Tensor input);"

# Load the custom CUDA kernel
fused_op = load_inline(
    name="fused_mean_subtract",
    cpp_sources=cpp_wrapper,
    cuda_sources=fused_kernel_code,
    functions=["fused_mean_subtract_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding, bias=bias
        )
        self.batch_norm = nn.BatchNorm3d(out_channels)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = fused_op.fused_mean_subtract_cuda(x)
        return x

batch_size = 16
in_channels = 16
out_channels = 32
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]
```

Kernel 2 (runtime: 5.84 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused mean subtraction
cuda_code = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void subtract_spatial_mean_kernel(float* x, int batch_size, int channels, int spatial_size) {
    int batch_channel_idx = blockIdx.x;
    int batch = batch_channel_idx / channels;
    int channel = batch_channel_idx % channels;
    
    if (batch >= batch_size || channel >= channels) return;
    
    int num_elements = spatial_size;
    int tid = threadIdx.x;
    int stride = blockDim.x;
    
    // Parallel reduction for sum
    float sum = 0.0f;
    for (int i = tid; i < num_elements; i += stride) {
        sum += x[batch * channels * spatial_size + channel * spatial_size + i];
    }
    
    __shared__ float shared_sum[256];
    shared_sum[tid] = sum;
    __syncthreads();
    
    // Block-wise reduction
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
        }
        __syncthreads();
    }
    
    // Compute and apply mean
    if (tid == 0) {
        float mean = shared_sum[0] / num_elements;
        shared_sum[0] = mean;
    }
    __syncthreads();
    
    float mean = shared_sum[0];
    
    // Subtract mean from elements
    for (int i = tid; i < num_elements; i += stride) {
        int idx = batch * channels * spatial_size + channel * spatial_size + i;
        x[idx] -= mean;
    }
}

torch::Tensor subtract_spatial_mean_cuda(torch::Tensor x) {
    TORCH_CHECK(x.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(x.dtype() == torch::kFloat32, "Input tensor must be float32");
    
    int batch_size = x.size(0);
    int channels = x.size(1);
    int spatial_size = x.size(2) * x.size(3) * x.size(4);
    
    int num_blocks = batch_size * channels;
    int block_size = 256;
    
    subtract_spatial_mean_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), batch_size, channels, spatial_size);
    
    return x;
}
"""

cpp_code = "torch::Tensor subtract_spatial_mean_cuda(torch::Tensor x);"

# Load the custom CUDA extension
subtract_mean_extension = load_inline(
    name="subtract_mean",
    cpp_sources=cpp_code,
    cuda_sources=cuda_code,
    functions=["subtract_spatial_mean_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, bias=bias
        )
        self.batch_norm = nn.BatchNorm3d(out_channels)
        self.subtract_mean = subtract_mean_extension.subtract_spatial_mean_cuda

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = self.subtract_mean(x)  # Fused mean subtraction
        return x

# Keep the same input generation functions
batch_size = 16
in_channels = 16
out_channels = 32
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]
```
