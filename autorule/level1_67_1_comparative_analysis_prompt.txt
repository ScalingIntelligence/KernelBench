You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 165.0 ms):
```
import torch
import torch.nn as nn
import math
from torch.utils.cpp_extension import load_inline

conv1d_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void conv1d_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int batch_size,
    int in_channels,
    int out_channels,
    int kernel_size,
    int input_length,
    int output_length,
    int stride,
    int padding,
    int dilation,
    int groups,
    int in_channels_per_group,
    int out_channels_per_group
) {
    extern __shared__ float s_weight[];
    
    const int oc = blockIdx.x;
    const int index = blockIdx.y * blockDim.x + threadIdx.x;
    const int total_elements = batch_size * output_length;
    if (index >= total_elements) return;

    const int b = index / output_length;
    const int out_pos = index % output_length;

    // Load weights into shared memory
    const int weight_size = in_channels_per_group * kernel_size;
    for (int i = threadIdx.x; i < weight_size; i += blockDim.x) {
        s_weight[i] = __ldg(&weight[oc * weight_size + i]);
    }
    __syncthreads();

    const int group_id = oc / out_channels_per_group;
    const int in_start = group_id * in_channels_per_group;
    float sum = 0.0f;

    for (int ic = in_start; ic < in_start + in_channels_per_group; ++ic) {
        for (int k = 0; k < kernel_size; ++k) {
            const int input_pos = out_pos * stride + k * dilation - padding;
            if (input_pos >= 0 && input_pos < input_length) {
                const float input_val = __ldg(&input[b * in_channels * input_length + ic * input_length + input_pos]);
                const float weight_val = s_weight[(ic - in_start) * kernel_size + k];
                sum += input_val * weight_val;
            }
        }
    }

    if (bias) sum += __ldg(&bias[oc]);
    output[b * out_channels * output_length + oc * output_length + out_pos] = sum;
}

torch::Tensor conv1d_forward(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int stride,
    int padding,
    int dilation,
    int groups
) {
    const int batch_size = input.size(0);
    const int in_channels = input.size(1);
    const int input_length = input.size(2);
    const int out_channels = weight.size(0);
    const int kernel_size = weight.size(2);

    const int output_length = ((input_length + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1;
    auto output = torch::zeros({batch_size, out_channels, output_length}, input.options());

    const int total_elements = batch_size * output_length;
    const int block_size = 256;
    const int weight_size = (in_channels / groups) * kernel_size;
    const dim3 grid(out_channels, (total_elements + block_size - 1) / block_size);

    conv1d_kernel<<<grid, block_size, weight_size * sizeof(float)>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.defined() ? bias.data_ptr<float>() : nullptr,
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        out_channels,
        kernel_size,
        input_length,
        output_length,
        stride,
        padding,
        dilation,
        groups,
        in_channels / groups,
        out_channels / groups
    );

    return output;
}
"""

conv1d_cpp_source = "torch::Tensor conv1d_forward(torch::Tensor, torch::Tensor, torch::Tensor, int, int, int, int);"

conv1d_ext = load_inline(
    name='conv1d_ext',
    cpp_sources=conv1d_cpp_source,
    cuda_sources=conv1d_cuda_source,
    functions=['conv1d_forward'],
    verbose=True,
    extra_cuda_cflags=['-use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups

        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.contiguous()  # Ensure contiguous memory layout
        bias = self.bias if self.bias is not None else torch.tensor([], dtype=x.dtype, device=x.device)
        return conv1d_ext.conv1d_forward(
            x,
            self.weight,
            bias,
            self.stride,
            self.padding,
            self.dilation,
            self.groups
        )
```

Kernel 2 (runtime: 166.0 ms):
```
import torch
import torch.nn as nn
import math
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel implementation
conv1d_cuda_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void conv1d_forward_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int input_length, int kernel_size, int output_length,
    int stride, int padding, int dilation, int groups) {

    extern __shared__ float s_weight[];

    int n = blockIdx.x;
    int c_out = blockIdx.y;
    int l_out = blockIdx.z * blockDim.x + threadIdx.x;

    if (n >= batch_size || c_out >= out_channels || l_out >= output_length) return;

    const int out_channels_per_group = out_channels / groups;
    const int group = c_out / out_channels_per_group;
    const int in_channels_per_group = in_channels / groups;
    const int start_c_in = group * in_channels_per_group;
    const int weight_size = in_channels_per_group * kernel_size;

    // Load group weights into shared memory
    for (int i = threadIdx.x; i < weight_size; i += blockDim.x) {
        int c_in_group = i / kernel_size;
        int k = i % kernel_size;
        s_weight[i] = weight[c_out * weight_size + c_in_group * kernel_size + k];
    }
    __syncthreads();

    float sum = 0.0f;
    for (int c_in_group = 0; c_in_group < in_channels_per_group; c_in_group++) {
        const int c_in = start_c_in + c_in_group;
        for (int k = 0; k < kernel_size; k++) {
            const int l_in = l_out * stride - padding + k * dilation;
            if (l_in >= 0 && l_in < input_length) {
                sum += input[n * in_channels * input_length + c_in * input_length + l_in] 
                     * s_weight[c_in_group * kernel_size + k];
            }
        }
    }

    if (bias) {
        sum += bias[c_out];
    }

    output[n * out_channels * output_length + c_out * output_length + l_out] = sum;
}

torch::Tensor conv1d_forward_cuda(
    torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
    int stride, int padding, int dilation, int groups) {

    const int batch_size = input.size(0);
    const int in_channels = input.size(1);
    const int input_length = input.size(2);
    const int out_channels = weight.size(0);
    const int kernel_size = weight.size(2);

    const int output_length = ((input_length + 2 * padding - dilation * (kernel_size - 1) - 1) / stride) + 1;
    auto output = torch::zeros({batch_size, out_channels, output_length}, input.options());

    const dim3 grid(batch_size, out_channels, (output_length + 255) / 256);
    const int block_size = 256;
    const size_t shared_mem_size = (in_channels / groups) * kernel_size * sizeof(float);

    conv1d_forward_kernel<<<grid, block_size, shared_mem_size>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.defined() ? bias.data_ptr<float>() : nullptr,
        output.data_ptr<float>(),
        batch_size, in_channels, out_channels,
        input_length, kernel_size, output_length,
        stride, padding, dilation, groups
    );

    return output;
}
"""

conv1d_cpp_source = "torch::Tensor conv1d_forward_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int stride, int padding, int dilation, int groups);"

# Load the CUDA kernel as a PyTorch extension
conv1d_cuda = load_inline(
    name='conv1d_cuda',
    cpp_sources=conv1d_cpp_source,
    cuda_sources=conv1d_cuda_source,
    functions=['conv1d_forward_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):
        super(ModelNew, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups

        # Weight parameters
        self.weight = nn.Parameter(
            torch.Tensor(out_channels, in_channels // groups, kernel_size)
        )
        
        # Bias parameter
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)

        # Initialize parameters
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return conv1d_cuda.conv1d_forward_cuda(
            x, 
            self.weight, 
            self.bias if self.bias is not None else torch.Tensor().to(x.device),
            self.stride,
            self.padding,
            self.dilation,
            self.groups
        )
```
