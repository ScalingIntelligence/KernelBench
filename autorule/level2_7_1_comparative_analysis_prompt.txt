You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 11.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_activations_bias_kernel(
    const float* __restrict__ conv_output,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int num_elements,
    int C, int D, int H, int W) {
    
    extern __shared__ float s_bias[];
    
    // Load bias into shared memory with coalesced access pattern
    for(int i = threadIdx.x; i < C; i += blockDim.x) {
        s_bias[i] = __ldg(&bias[i]);
    }
    __syncthreads();
    
    const int elements_per_channel = D * H * W;
    const int elements_per_sample = C * elements_per_channel;
    
    // Grid-strided loop for optimal load balancing
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;
    
    for(int idx = tid; idx < num_elements; idx += stride) {
        // Calculate channel index using optimized modular arithmetic
        const int sample_rem = idx % elements_per_sample;
        const int c = sample_rem / elements_per_channel;
        
        float val = __ldg(conv_output + idx);
        
        // Fused activation sequence with exact PyTorch matching
        // ReLU (clamps negatives to 0)
        val = fmaxf(0.0f, val);
        
        // LeakyReLU (preserve original logic despite potential redundancy)
        val = fmaxf(0.01f * val, val);
        
        // GELU (exact implementation matching PyTorch's default)
        val = 0.5f * val * (1.0f + erff(val * 0.7071067811865475f));
        
        // Sigmoid with stable computation
        val = 1.0f / (1.0f + __expf(-val));
        
        // Add per-channel bias from shared memory
        output[idx] = val + s_bias[c];
    }
}

torch::Tensor fused_activations_bias_cuda(torch::Tensor conv_output, torch::Tensor bias) {
    auto output = torch::empty_like(conv_output);
    const int num_elements = conv_output.numel();
    
    const int C = conv_output.size(1);
    const int D = conv_output.size(2);
    const int H = conv_output.size(3);
    const int W = conv_output.size(4);

    // Optimized kernel configuration
    const int block_size = 256;  // Optimal for modern GPU architectures
    const int grid_size = (num_elements + block_size - 1) / block_size;
    const size_t shared_mem = C * sizeof(float);

    fused_activations_bias_kernel<<<grid_size, block_size, shared_mem>>>(
        conv_output.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        C, D, H, W
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_activations_bias_cuda(torch::Tensor conv_output, torch::Tensor bias);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_activations_bias_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_ops.fused_activations_bias_cuda(x, self.bias)
        return x
```

Kernel 2 (runtime: 11.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_activations_bias_kernel(
    const float* __restrict__ conv_output,
    const float* __restrict__ bias,
    float* __restrict__ output,
    int num_elements,
    int C, int D, int H, int W) {
    
    extern __shared__ float s_bias[];
    
    // Coalesced shared memory loading
    for(int i = threadIdx.x; i < C; i += blockDim.x) {
        s_bias[i] = __ldg(&bias[i]);
    }
    __syncthreads();
    
    const int elements_per_channel = D * H * W;
    
    // Optimized grid-strided loop
    const int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int stride = blockDim.x * gridDim.x;
    
    for(int idx = tid; idx < num_elements; idx += stride) {
        // Efficient channel calculation
        const int c = (idx / elements_per_channel) % C;
        
        float val = __ldg(conv_output + idx);
        
        // Fused activation sequence
        val = fmaxf(0.0f, val);  // ReLU
        val = 0.5f * val * (1.0f + erff(val * 0.7071067811865475f));  // GELU
        val = 1.0f / (1.0f + __expf(-val));  // Fast sigmoid
        
        output[idx] = val + s_bias[c];
    }
}

torch::Tensor fused_activations_bias_cuda(torch::Tensor conv_output, torch::Tensor bias) {
    auto output = torch::empty_like(conv_output);
    const int num_elements = conv_output.numel();
    
    const int C = conv_output.size(1);
    const int D = conv_output.size(2);
    const int H = conv_output.size(3);
    const int W = conv_output.size(4);

    // Optimized kernel configuration
    const int block_size = 256;
    const int max_grid_size = 65535;
    int grid_size = (num_elements + block_size - 1) / block_size;
    grid_size = grid_size < max_grid_size ? grid_size : max_grid_size;
    const size_t shared_mem = C * sizeof(float);

    fused_activations_bias_kernel<<<grid_size, block_size, shared_mem>>>(
        conv_output.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        C, D, H, W
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_activations_bias_cuda(torch::Tensor conv_output, torch::Tensor bias);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_activations_bias_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_ops.fused_activations_bias_cuda(x, self.bias)
        return x
```
