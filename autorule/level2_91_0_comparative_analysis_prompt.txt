You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 28.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__inline__ __device__ float warpReduceMax(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    }
    return val;
}

__inline__ __device__ float blockReduceMax(float val, float* shared) {
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;
    
    val = warpReduceMax(val);
    
    if (lane == 0) {
        shared[wid] = val;
    }
    __syncthreads();
    
    if (wid == 0) {
        val = (lane < (blockDim.x / 32)) ? shared[lane] : -INFINITY;
        val = warpReduceMax(val);
    }
    
    // Broadcast max value to all threads
    __syncthreads();
    if (threadIdx.x == 0) shared[0] = val;
    __syncthreads();
    val = shared[0];
    
    return val;
}

__inline__ __device__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}

__inline__ __device__ float blockReduceSum(float val, float* shared) {
    int lane = threadIdx.x % 32;
    int wid = threadIdx.x / 32;
    
    val = warpReduceSum(val);
    
    if (lane == 0) {
        shared[wid] = val;
    }
    __syncthreads();
    
    if (wid == 0) {
        val = (lane < (blockDim.x / 32)) ? shared[lane] : 0.0f;
        val = warpReduceSum(val);
    }
    
    // Broadcast sum to all threads
    __syncthreads();
    if (threadIdx.x == 0) shared[0] = val;
    __syncthreads();
    val = shared[0];
    
    return val;
}

__global__ void fused_softmax_bias_scale_sigmoid_kernel(
    const float* input,
    const float* bias,
    float scaling_factor,
    float* output,
    int batch_size,
    int num_channels,
    int height,
    int width
) {
    extern __shared__ float shared[];
    float* max_shared = shared;
    float* sum_shared = &shared[blockDim.x / 32];

    int b = blockIdx.x / (height * width);
    int hw = blockIdx.x % (height * width);
    int h = hw / width;
    int w = hw % width;

    if (b >= batch_size || h >= height || w >= width) return;

    // First pass: compute max
    float thread_max = -INFINITY;
    for (int c = threadIdx.x; c < num_channels; c += blockDim.x) {
        int idx = ((b * num_channels + c) * height + h) * width + w;
        thread_max = fmaxf(thread_max, input[idx]);
    }
    float max_val = blockReduceMax(thread_max, max_shared);

    // Second pass: compute sum
    float thread_sum = 0.0f;
    for (int c = threadIdx.x; c < num_channels; c += blockDim.x) {
        int idx = ((b * num_channels + c) * height + h) * width + w;
        thread_sum += expf(input[idx] - max_val);
    }
    float sum = blockReduceSum(thread_sum, sum_shared);

    // Final pass: compute values
    for (int c = threadIdx.x; c < num_channels; c += blockDim.x) {
        int idx = ((b * num_channels + c) * height + h) * width + w;
        float exp_val = expf(input[idx] - max_val);
        float result = exp_val / sum;
        result += bias[c];
        result *= scaling_factor;
        result = 1.0f / (1.0f + expf(-result));
        output[idx] = result;
    }
}

torch::Tensor fused_operations_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling_factor
) {
    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    TORCH_CHECK(bias.dim() == 3, "Bias must be 3D");
    
    auto output = torch::empty_like(input);
    int batch_size = input.size(0);
    int num_channels = input.size(1);
    int height = input.size(2);
    int width = input.size(3);

    int num_blocks = batch_size * height * width;
    int block_size = 256;
    size_t shared_mem = 2 * (block_size / 32) * sizeof(float);

    fused_softmax_bias_scale_sigmoid_kernel<<<num_blocks, block_size, shared_mem>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        batch_size,
        num_channels,
        height,
        width
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor);"

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=["fused_operations_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_operations_cuda(x, self.bias, self.scaling_factor)
        return x
```

Kernel 2 (runtime: 24.4 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fused_softmax_bias_scale_sigmoid_kernel(
    const float* input, 
    const float* bias, 
    float scaling_factor,
    float* output, 
    int N, int C, int H, int W) {

    int spatial_idx = blockIdx.x;
    int n = spatial_idx / (H * W);
    int hw = spatial_idx % (H * W);
    int h = hw / W;
    int w = hw % W;

    int c = threadIdx.x;
    if (c >= C) return;

    int input_idx = n*C*H*W + c*H*W + h*W + w;
    float val = input[input_idx];
    
    extern __shared__ float shared[];
    shared[c] = val;
    __syncthreads();

    // Parallel max reduction
    float max_val = val;
    for(int stride = blockDim.x/2; stride>0; stride>>=1) {
        if(c < stride && c + stride < C) {
            if(shared[c+stride] > max_val)
                max_val = shared[c+stride];
        }
        __syncthreads();
        if(c < stride) shared[c] = max_val;
        __syncthreads();
    }
    
    // Broadcast max value
    max_val = shared[0];
    __syncthreads();

    // Compute exp and sum
    float exp_val = expf(val - max_val);
    shared[c] = exp_val;
    __syncthreads();

    // Parallel sum reduction
    float sum_exp = exp_val;
    for(int stride = blockDim.x/2; stride>0; stride>>=1) {
        if(c < stride && c + stride < C) {
            sum_exp += shared[c+stride];
        }
        __syncthreads();
        if(c < stride) shared[c] = sum_exp;
        __syncthreads();
    }
    
    // Compute final values
    float result = exp_val / shared[0];
    result += bias[c];
    result *= scaling_factor;
    result = 1.0f / (1.0f + expf(-result));
    
    output[input_idx] = result;
}

torch::Tensor fused_op_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling_factor) {
    
    auto N = input.size(0);
    auto C = input.size(1);
    auto H = input.size(2);
    auto W = input.size(3);
    
    auto output = torch::empty_like(input);
    
    dim3 grid(N * H * W);
    dim3 block(C);
    size_t shared_size = C * sizeof(float);
    
    fused_softmax_bias_scale_sigmoid_kernel<<<grid, block, shared_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        N, C, H, W
    );
    
    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_op_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor);"

fused_extension = load_inline(
    name='fused_ops',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_code,
    functions=['fused_op_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O2']
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding,
            output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_extension.fused_op_cuda(x, self.bias, self.scaling_factor)
        return x
```
