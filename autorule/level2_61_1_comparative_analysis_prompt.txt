You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 8.49 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for fused ReLU + GroupNorm
fused_gn_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void group_norm_relu_sum_kernel(
    const float* input,
    float* sum,
    float* sum_sq,
    int batch_size,
    int num_channels,
    int groups,
    int D, int H, int W) {
    
    int batch = blockIdx.x;
    int group = blockIdx.y;
    int channels_per_group = num_channels / groups;
    int num_elements = channels_per_group * D * H * W;

    float thread_sum = 0.0f;
    float thread_sum_sq = 0.0f;

    for (int idx = threadIdx.x; idx < num_elements; idx += blockDim.x) {
        int c = idx / (D * H * W);
        int spatial_idx = idx % (D * H * W);
        int d = spatial_idx / (H * W);
        int h = (spatial_idx % (H * W)) / W;
        int w = spatial_idx % W;
        
        int channel = group * channels_per_group + c;
        int input_idx = batch * num_channels * D * H * W +
                       channel * D * H * W +
                       d * H * W +
                       h * W +
                       w;

        float val = fmaxf(0.0f, input[input_idx]);
        thread_sum += val;
        thread_sum_sq += val * val;
    }

    // Block reduction
    __shared__ float shared_sum[256];
    __shared__ float shared_sum_sq[256];
    shared_sum[threadIdx.x] = thread_sum;
    shared_sum_sq[threadIdx.x] = thread_sum_sq;
    __syncthreads();

    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
            shared_sum_sq[threadIdx.x] += shared_sum_sq[threadIdx.x + stride];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        sum[batch*groups + group] = shared_sum[0];
        sum_sq[batch*groups + group] = shared_sum_sq[0];
    }
}

__global__ void group_norm_relu_forward_kernel(
    const float* input,
    const float* gamma,
    const float* beta,
    const float* sum,
    const float* sum_sq,
    float* output,
    int batch_size,
    int num_channels,
    int groups,
    int D, int H, int W,
    float eps) {
    
    int batch = blockIdx.x;
    int group = blockIdx.y;
    int channels_per_group = num_channels / groups;
    int num_elements = channels_per_group * D * H * W;

    float mean = sum[batch*groups + group] / num_elements;
    float var = (sum_sq[batch*groups + group]/num_elements) - (mean*mean);
    float inv_std = rsqrtf(var + eps);

    for (int idx = threadIdx.x; idx < num_elements; idx += blockDim.x) {
        int c = idx / (D * H * W);
        int spatial_idx = idx % (D * H * W);
        int d = spatial_idx / (H * W);
        int h = (spatial_idx % (H * W)) / W;
        int w = spatial_idx % W;
        
        int channel = group * channels_per_group + c;
        int input_idx = batch * num_channels * D * H * W +
                       channel * D * H * W +
                       d * H * W +
                       h * W +
                       w;

        float val = fmaxf(0.0f, input[input_idx]);
        float norm = (val - mean) * inv_std;
        output[input_idx] = norm * gamma[channel] + beta[channel];
    }
}

torch::Tensor fused_relu_group_norm_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int groups,
    float eps) {
    
    auto batch_size = input.size(0);
    auto num_channels = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);

    auto options = torch::TensorOptions().dtype(input.dtype()).device(input.device());
    auto sum = torch::zeros({batch_size, groups}, options);
    auto sum_sq = torch::zeros({batch_size, groups}, options);
    auto output = torch::empty_like(input);

    dim3 blocks(batch_size, groups);
    int threads = 256;

    group_norm_relu_sum_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        sum.data_ptr<float>(),
        sum_sq.data_ptr<float>(),
        batch_size,
        num_channels,
        groups,
        D, H, W
    );

    group_norm_relu_forward_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        sum.data_ptr<float>(),
        sum_sq.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_channels,
        groups,
        D, H, W,
        eps
    );

    return output;
}
"""

fused_gn_cpp = """
torch::Tensor fused_relu_group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int groups, float eps);
"""

fused_gn = load_inline(
    name='fused_gn',
    cpp_sources=fused_gn_cpp,
    cuda_sources=fused_gn_source,
    functions=['fused_relu_group_norm_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, groups, bias=False):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=bias)
        self.groups = groups
        self.eps = 1e-5
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x.contiguous()
        return fused_gn.fused_relu_group_norm_cuda(x, self.gamma, self.beta, self.groups, self.eps)
```

Kernel 2 (runtime: 8.26 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom fused ReLU + GroupNorm kernel
fused_kernel_code = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>

#define CUDA_CHECK(err) do {                         \\
    cudaError_t err_ = (err);                        \\
    if (err_ != cudaSuccess) {                       \\
        printf("CUDA error %d: %s\\n",               \\
               err_,                                 \\
               cudaGetErrorString(err_));            \\
        throw std::runtime_error("CUDA error");       \\
    }                                                \\
} while (0)

__global__ void fused_relu_groupnorm_kernel(
    const float* input,
    const float* gamma,
    const float* beta,
    float* output,
    int batch_size,
    int num_channels,
    int D, int H, int W,
    int groups,
    float epsilon
) {
    const int group_id = blockIdx.x;
    const int sample_id = blockIdx.y;
    const int channels_per_group = num_channels / groups;
    const int elements_per_channel = D * H * W;
    const int elements_per_group = channels_per_group * elements_per_channel;

    extern __shared__ float smem[];
    float* shared_sum = smem;
    float* shared_sum_sq = &smem[blockDim.x];

    float sum = 0.0f;
    float sum_sq = 0.0f;

    // First pass: compute sum and sum_sq after ReLU
    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        const int c = (i / elements_per_channel) % channels_per_group;
        const int spatial_idx = i % elements_per_channel;
        const int input_idx = sample_id * num_channels * elements_per_channel +
                            (group_id * channels_per_group + c) * elements_per_channel +
                            spatial_idx;
        
        float val = input[input_idx];
        val = fmaxf(val, 0.0f);  // ReLU
        sum += val;
        sum_sq += val * val;
    }

    // Block reduction for sum and sum_sq
    shared_sum[threadIdx.x] = sum;
    shared_sum_sq[threadIdx.x] = sum_sq;
    __syncthreads();

    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
            shared_sum_sq[threadIdx.x] += shared_sum_sq[threadIdx.x + stride];
        }
        __syncthreads();
    }

    const float group_sum = shared_sum[0];
    const float group_sum_sq = shared_sum_sq[0];
    const float mean = group_sum / elements_per_group;
    const float var = (group_sum_sq / elements_per_group) - (mean * mean) + epsilon;
    const float inv_std = rsqrtf(var);

    // Second pass: normalize and apply gamma/beta
    for (int i = threadIdx.x; i < elements_per_group; i += blockDim.x) {
        const int c = (i / elements_per_channel) % channels_per_group;
        const int spatial_idx = i % elements_per_channel;
        const int input_idx = sample_id * num_channels * elements_per_channel +
                            (group_id * channels_per_group + c) * elements_per_channel +
                            spatial_idx;
        
        float val = input[input_idx];
        val = fmaxf(val, 0.0f);  // ReLU again
        
        // Normalize and transform
        val = (val - mean) * inv_std;
        val = val * gamma[group_id * channels_per_group + c] + 
              beta[group_id * channels_per_group + c];
        
        output[input_idx] = val;
    }
}

torch::Tensor fused_relu_groupnorm_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int groups,
    float epsilon
) {
    const int batch_size = input.size(0);
    const int num_channels = input.size(1);
    const int D = input.size(2);
    const int H = input.size(3);
    const int W = input.size(4);
    
    auto output = torch::empty_like(input);
    
    const int threads = 256;
    const dim3 blocks(groups, batch_size);
    const size_t smem_size = 2 * threads * sizeof(float);
    
    fused_relu_groupnorm_kernel<<<blocks, threads, smem_size>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_channels,
        D, H, W,
        groups,
        epsilon
    );
    
    CUDA_CHECK(cudaGetLastError());
    return output;
}
"""

cpp_src = "torch::Tensor fused_relu_groupnorm_cuda(torch::Tensor, torch::Tensor, torch::Tensor, int, float);"

fused_op = load_inline(
    name="fused_relu_groupnorm",
    cpp_sources=cpp_src,
    cuda_sources=fused_kernel_code,
    functions=["fused_relu_groupnorm_cuda"],
    verbose=True,
    extra_cuda_cflags=["-O3"]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, groups, bias=False):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, bias=bias
        )
        self.groups = groups
        self.epsilon = 1e-5
        
        # Initialize group norm parameters
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_op.fused_relu_groupnorm_cuda(
            x, self.gamma, self.beta, self.groups, self.epsilon
        )
        return x
```
