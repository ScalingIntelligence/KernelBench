You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 7.31 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define CUDA sources for fused operations
fused_ops_source = '''
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_bias_hardtanh_mish_kernel(const float* input, const float* bias, float* output, int batch_size, int out_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * out_features;
    if (idx < total) {
        int j = idx % out_features;  // Feature index
        float x = input[idx] + bias[j];
        
        // Apply Hardtanh (clamp between -1 and 1)
        x = fmaxf(-1.0f, fminf(1.0f, x));
        
        // Compute Mish: x * tanh(softplus(x))
        float sp = log1pf(expf(x));  // Numerically stable softplus
        float tanh_sp = tanhf(sp);
        output[idx] = x * tanh_sp;
    }
}

torch::Tensor fused_bias_hardtanh_mish_cuda(torch::Tensor input, torch::Tensor bias) {
    auto batch_size = input.size(0);
    auto out_features = input.size(1);
    auto output = torch::empty_like(input);

    int total_elements = batch_size * out_features;
    const int block_size = 256;
    int num_blocks = (total_elements + block_size - 1) / block_size;

    fused_bias_hardtanh_mish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features
    );

    return output;
}

__global__ void group_norm_kernel(const float* input, const float* gamma, const float* beta, float* output, 
                                  int batch_size, int num_groups, int channels_per_group, float epsilon) {
    int sample = blockIdx.x;
    int group = blockIdx.y;
    int tid = threadIdx.x;

    int channel = group * channels_per_group + tid;
    if (channel >= num_groups * channels_per_group) return;

    int input_idx = sample * (num_groups * channels_per_group) + channel;
    float x = input[input_idx];

    // Warp-wide reduction for sum and sum of squares
    float sum = x;
    float sum_sq = x * x;
    for (int offset = 16; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
        sum_sq += __shfl_down_sync(0xffffffff, sum_sq, offset);
    }

    // Broadcast mean and variance
    float mean = __shfl_sync(0xffffffff, sum, 0) / channels_per_group;
    float var = __shfl_sync(0xffffffff, sum_sq, 0) / channels_per_group - mean * mean;

    // Apply normalization
    float normalized = (x - mean) / sqrtf(var + epsilon);
    output[input_idx] = gamma[channel] * normalized + beta[channel];
}

torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, 
                              int num_groups, float epsilon) {
    auto batch_size = input.size(0);
    int channels = input.size(1);
    int channels_per_group = channels / num_groups;

    auto output = torch::empty_like(input);

    dim3 blocks(batch_size, num_groups);
    group_norm_kernel<<<blocks, channels_per_group>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_groups,
        channels_per_group,
        epsilon
    );

    return output;
}
'''

fused_ops_cpp_source = '''
torch::Tensor fused_bias_hardtanh_mish_cuda(torch::Tensor input, torch::Tensor bias);
torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int num_groups, float epsilon);
'''

# Load the CUDA kernels
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=['fused_bias_hardtanh_mish_cuda', 'group_norm_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.num_groups = num_groups
        self.epsilon = 1e-5
        
        # Initialize group norm parameters
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))

    def forward(self, x):
        x = self.gemm(x)
        x = fused_ops.fused_bias_hardtanh_mish_cuda(x, self.bias)
        x = fused_ops.group_norm_cuda(x, self.gamma, self.beta, self.num_groups, self.epsilon)
        return x
```

Kernel 2 (runtime: 7.36 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = '''
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_bias_hardtanh_mish_kernel(const float* input, const float* bias, float* output, int batch_size, int out_features) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    const int elements_per_thread = 4;
    int idx = tid * elements_per_thread;
    int total = batch_size * out_features;
    
    if (idx < total) {
        float4 in_data = *reinterpret_cast<const float4*>(input + idx);
        float4 bias_data = *reinterpret_cast<const float4*>(bias + (idx % out_features));
        float4 out_data;

        #pragma unroll
        for (int i = 0; i < 4; ++i) {
            float val = (&in_data.x)[i] + (&bias_data.x)[i];
            val = fmaxf(-1.0f, fminf(1.0f, val));
            float sp = log1pf(expf(val));
            float tanh_sp = tanhf(sp);
            (&out_data.x)[i] = val * tanh_sp;
        }

        // Handle partial vector at the end
        int remaining = total - idx;
        if (remaining >= 4) {
            *reinterpret_cast<float4*>(output + idx) = out_data;
        } else {
            for (int i = 0; i < remaining; ++i) {
                output[idx + i] = (&out_data.x)[i];
            }
        }
    }
}

torch::Tensor fused_bias_hardtanh_mish_cuda(torch::Tensor input, torch::Tensor bias) {
    auto batch_size = input.size(0);
    auto out_features = input.size(1);
    auto output = torch::empty_like(input);

    int total_elements = batch_size * out_features;
    const int block_size = 512;
    int num_blocks = (total_elements + block_size * 4 - 1) / (block_size * 4);

    fused_bias_hardtanh_mish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features
    );

    return output;
}

__global__ void group_norm_kernel(const float* input, const float* gamma, const float* beta, float* output, 
                                  int batch_size, int num_groups, int channels_per_group, float epsilon) {
    int sample = blockIdx.x;
    int group = blockIdx.y;
    int tid = threadIdx.x;

    int channel = group * channels_per_group + tid;
    if (channel >= num_groups * channels_per_group) return;

    int input_idx = sample * (num_groups * channels_per_group) + channel;
    float x = input[input_idx];

    // Warp-wide reduction using shuffle instructions
    float sum = x;
    float sum_sq = x * x;
    for (int offset = 16; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
        sum_sq += __shfl_down_sync(0xffffffff, sum_sq, offset);
    }

    float mean = __shfl_sync(0xffffffff, sum, 0) / channels_per_group;
    float var = __shfl_sync(0xffffffff, sum_sq, 0) / channels_per_group - mean * mean;
    float inv_std = rsqrtf(var + epsilon);

    output[input_idx] = gamma[channel] * (x - mean) * inv_std + beta[channel];
}

torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, 
                              int num_groups, float epsilon) {
    auto batch_size = input.size(0);
    int channels = input.size(1);
    int channels_per_group = channels / num_groups;

    auto output = torch::empty_like(input);

    dim3 blocks(batch_size, num_groups);
    group_norm_kernel<<<blocks, channels_per_group>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_groups,
        channels_per_group,
        epsilon
    );

    return output;
}
'''

fused_ops_cpp_source = '''
torch::Tensor fused_bias_hardtanh_mish_cuda(torch::Tensor input, torch::Tensor bias);
torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int num_groups, float epsilon);
'''

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=['fused_bias_hardtanh_mish_cuda', 'group_norm_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.num_groups = num_groups
        self.epsilon = 1e-5
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))

    def forward(self, x):
        x = self.gemm(x)
        x = fused_ops.fused_bias_hardtanh_mish_cuda(x, self.bias)
        x = fused_ops.group_norm_cuda(x, self.gamma, self.beta, self.num_groups, self.epsilon)
        return x
```
