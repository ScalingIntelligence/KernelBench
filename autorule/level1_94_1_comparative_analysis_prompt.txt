You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized CUDA kernel with vectorized loads and hierarchical reduction
mse_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mse_kernel(const float* __restrict__ pred,
                          const float* __restrict__ target,
                          float* __restrict__ sum,
                          int n) {
    extern __shared__ float warp_sums[];
    const int tid = threadIdx.x;
    const int warp_id = tid / 32;
    const int lane_id = tid % 32;
    const int vec_size = 4;
    
    float local_sum = 0.0f;
    const int vec_end = n - (n % vec_size);

    // Vectorized processing (4 elements per thread)
    #pragma unroll
    for(int i = blockIdx.x * blockDim.x * vec_size + tid * vec_size; 
        i < vec_end; 
        i += blockDim.x * gridDim.x * vec_size) {
        const float4 pred_vec = *reinterpret_cast<const float4*>(pred + i);
        const float4 target_vec = *reinterpret_cast<const float4*>(target + i);
        
        local_sum += (pred_vec.x - target_vec.x) * (pred_vec.x - target_vec.x);
        local_sum += (pred_vec.y - target_vec.y) * (pred_vec.y - target_vec.y);
        local_sum += (pred_vec.z - target_vec.z) * (pred_vec.z - target_vec.z);
        local_sum += (pred_vec.w - target_vec.w) * (pred_vec.w - target_vec.w);
    }

    // Process remaining elements with grid-stride loop
    for(int i = vec_end + blockIdx.x * blockDim.x + tid; 
        i < n; 
        i += blockDim.x * gridDim.x) {
        const float diff = pred[i] - target[i];
        local_sum += diff * diff;
    }

    // Warp-level reduction using shuffle
    #pragma unroll
    for(int offset = 16; offset > 0; offset >>= 1) {
        local_sum += __shfl_down_sync(0xffffffff, local_sum, offset);
    }

    // Store warp sum to shared memory
    if(lane_id == 0) {
        warp_sums[warp_id] = local_sum;
    }
    __syncthreads();

    // Block-level reduction using first warp
    if(warp_id == 0) {
        float block_sum = lane_id < (blockDim.x/32) ? warp_sums[lane_id] : 0.0f;
        
        #pragma unroll
        for(int offset = 16; offset > 0; offset >>= 1) {
            block_sum += __shfl_down_sync(0xffffffff, block_sum, offset);
        }

        // Atomic add to global sum
        if(tid == 0) {
            atomicAdd(sum, block_sum);
        }
    }
}

torch::Tensor mse_cuda(torch::Tensor predictions, torch::Tensor targets) {
    TORCH_CHECK(predictions.is_cuda() && targets.is_cuda(), 
               "Inputs must be CUDA tensors");
    TORCH_CHECK(predictions.sizes() == targets.sizes(), 
               "Shape mismatch between predictions and targets");
    
    const int64_t n = predictions.numel();
    auto sum_tensor = torch::zeros({1}, predictions.options());
    
    const int block_size = 256;
    const int vec_size = 4;
    const int grid_size = (n + block_size * vec_size - 1) / (block_size * vec_size);
    
    // Calculate shared memory size (1 float per warp)
    const size_t shared_mem = (block_size / 32) * sizeof(float);
    
    // Launch kernel with optimal configuration
    mse_kernel<<<grid_size, block_size, shared_mem>>>(
        predictions.contiguous().data_ptr<float>(),
        targets.contiguous().data_ptr<float>(),
        sum_tensor.data_ptr<float>(),
        n
    );

    return (sum_tensor / n).squeeze();
}
"""

mse_cpp_source = "torch::Tensor mse_cuda(torch::Tensor predictions, torch::Tensor targets);"

# Compile with architecture-aware optimizations
mse_cuda = load_inline(
    name='mse_cuda',
    cpp_sources=mse_cpp_source,
    cuda_sources=mse_source,
    functions=['mse_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math', '-Xcompiler', '-Wall']
)

class ModelNew(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.mse_cuda = mse_cuda

    def forward(self, predictions, targets):
        return self.mse_cuda.mse_cuda(predictions.contiguous(), targets.contiguous())
```

Kernel 2 (runtime: 12.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

mse_cuda_source = """
#include <torch/extension.h>
#include <cuda_fp16.h>

constexpr int WARP_SIZE = 32;
constexpr int VEC_SIZE = 4;

__global__ void mse_kernel(const float* __restrict__ a,
                           const float* __restrict__ b,
                           float* __restrict__ out,
                           int num_elements) {
    extern __shared__ float warp_sums[];
    const int tid = threadIdx.x;
    const int warp_id = tid / WARP_SIZE;
    const int lane_id = tid % WARP_SIZE;
    
    float thread_sum = 0.0f;
    int i = (blockIdx.x * blockDim.x + tid) * VEC_SIZE;
    
    // Process vectorized elements
    while (i + VEC_SIZE <= num_elements) {
        const float4 a_vec = *reinterpret_cast<const float4*>(a + i);
        const float4 b_vec = *reinterpret_cast<const float4*>(b + i);
        
        thread_sum += (a_vec.x - b_vec.x) * (a_vec.x - b_vec.x);
        thread_sum += (a_vec.y - b_vec.y) * (a_vec.y - b_vec.y);
        thread_sum += (a_vec.z - b_vec.z) * (a_vec.z - b_vec.z);
        thread_sum += (a_vec.w - b_vec.w) * (a_vec.w - b_vec.w);
        
        i += blockDim.x * gridDim.x * VEC_SIZE;
    }

    // Process remaining elements
    while (i < num_elements) {
        float diff = a[i] - b[i];
        thread_sum += diff * diff;
        i += blockDim.x * gridDim.x;
    }

    // Warp-level reduction
    for (int offset = WARP_SIZE/2; offset > 0; offset >>= 1) {
        thread_sum += __shfl_down_sync(0xffffffff, thread_sum, offset);
    }

    // Store warp sum to shared memory
    if (lane_id == 0) {
        warp_sums[warp_id] = thread_sum;
    }
    __syncthreads();

    // First warp reduces per-warp sums
    if (warp_id == 0 && lane_id < blockDim.x/WARP_SIZE) {
        float sum = warp_sums[lane_id];
        for (int offset = WARP_SIZE/2; offset > 0; offset >>= 1) {
            sum += __shfl_down_sync(0xffffffff, sum, offset);
        }
        if (lane_id == 0) {
            atomicAdd(out, sum);
        }
    }
}

torch::Tensor mse_cuda(torch::Tensor a, torch::Tensor b) {
    TORCH_CHECK(a.sizes() == b.sizes(), "Input tensors must have the same shape");
    TORCH_CHECK(a.is_cuda() && b.is_cuda(), "Inputs must be CUDA tensors");
    TORCH_CHECK(a.is_contiguous() && b.is_contiguous(), "Inputs must be contiguous");
    
    const int num_elements = a.numel();
    auto out = torch::zeros({}, a.options());  // Scalar tensor
    
    if (num_elements == 0) return out;
    
    const int block_size = 256;
    const int warps_per_block = block_size / WARP_SIZE;
    int grid_size = (num_elements + block_size * VEC_SIZE - 1) / (block_size * VEC_SIZE);
    grid_size = std::min(grid_size, 65535);
    
    size_t shared_mem_size = warps_per_block * sizeof(float);
    
    mse_kernel<<<grid_size, block_size, shared_mem_size>>>(
        a.data_ptr<float>(),
        b.data_ptr<float>(),
        out.data_ptr<float>(),
        num_elements
    );
    
    // Final division on GPU
    out.div_(num_elements);
    return out;
}
"""

mse_cpp_source = "torch::Tensor mse_cuda(torch::Tensor a, torch::Tensor b);"

mse_extension = load_inline(
    name='mse_extension',
    cpp_sources=mse_cpp_source,
    cuda_sources=mse_cuda_source,
    functions=['mse_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O3', '--use_fast_math']
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, predictions, targets):
        return mse_extension.mse_cuda(predictions, targets)
```
