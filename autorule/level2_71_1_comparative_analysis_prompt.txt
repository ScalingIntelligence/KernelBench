You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 4.23 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template<typename scalar_t>
__global__ void fused_div_leaky_relu_kernel(
    const scalar_t* input,
    scalar_t* output,
    scalar_t divisor,
    int num_elements) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= num_elements) return;

    scalar_t val = input[idx] / divisor;
    output[idx] = val > 0 ? val : val * 0.01f;
}

torch::Tensor fused_div_leaky_relu_cuda(
    torch::Tensor input,
    float divisor) {
    
    auto output = torch::empty_like(input);
    int numel = input.numel();

    const int threads_per_block = 256;
    int num_blocks = (numel + threads_per_block - 1) / threads_per_block;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "fused_div_leaky_relu", ([&] {
        fused_div_leaky_relu_kernel<scalar_t><<<num_blocks, threads_per_block>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            static_cast<scalar_t>(divisor),
            numel);
    }));

    return output;
}
"""

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources="""
torch::Tensor fused_div_leaky_relu_cuda(
    torch::Tensor input,
    float divisor);
""",
    cuda_sources=fused_ops_source,
    functions=['fused_div_leaky_relu_cuda'],
    verbose=False
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, divisor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.divisor = divisor
        self.fused_op = fused_ops

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_op.fused_div_leaky_relu_cuda(x, self.divisor)
        return x
```

Kernel 2 (runtime: 4.21 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused division and LeakyReLU CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_div_relu_kernel(const float* input, float* output, float inv_divisor, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx] * inv_divisor;
        output[idx] = fmaxf(val, 0.01f * val);
    }
}

torch::Tensor fused_div_relu_cuda(torch::Tensor input, float divisor) {
    auto inv_divisor = 1.0f / divisor;
    auto output = torch::empty_like(input);
    int size = input.numel();
    
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    
    fused_div_relu_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        inv_divisor,
        size
    );
    
    return output;
}
"""

fused_cpp_source = "torch::Tensor fused_div_relu_cuda(torch::Tensor input, float divisor);"

# Load the custom CUDA operation
fused_op = load_inline(
    name='fused_div_relu',
    cpp_sources=fused_cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_div_relu_cuda'],
    verbose=True,
    extra_cflags=[],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    """
    Optimized model with fused division+LeakyReLU CUDA kernel
    """
    def __init__(self, in_channels, out_channels, kernel_size, divisor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.divisor = divisor
        self.fused_op = fused_op

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_op.fused_div_relu_cuda(x, self.divisor)
        return x
```
