You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 14.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for optimized argmin along dim=1
argmin_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

__global__ void argmin_kernel(const float* input, int64_t* output, int batch_size, int dim1, int dim2) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * dim2) return;

    int batch = idx / dim2;
    int dim2_pos = idx % dim2;

    float min_val = INFINITY;
    int min_idx = 0;

    for(int dim1_idx = 0; dim1_idx < dim1; dim1_idx++) {
        float val = input[batch * dim1 * dim2 + dim1_idx * dim2 + dim2_pos];
        if(val < min_val) {
            min_val = val;
            min_idx = dim1_idx;
        }
    }
    
    output[idx] = min_idx;
}

torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    TORCH_CHECK(dim == 1, "This kernel only supports dim=1");
    auto sizes = input.sizes();
    auto output = torch::empty({sizes[0], sizes[2]}, 
                             torch::TensorOptions().dtype(torch::kLong).device(input.device()));

    int total_threads = sizes[0] * sizes[2];
    const int block_size = 256;
    dim3 grid((total_threads + block_size - 1) / block_size);

    argmin_kernel<<<grid, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<int64_t>(),
        sizes[0], sizes[1], sizes[2]
    );

    return output;
}
"""

argmin_cpp = "torch::Tensor argmin_cuda(torch::Tensor input, int dim);"

# Load the custom CUDA extension
argmin_ext = load_inline(
    name='argmin_ext',
    cpp_sources=argmin_cpp,
    cuda_sources=argmin_source,
    functions=['argmin_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.dim == 1 and x.is_cuda:
            return argmin_ext.argmin_cuda(x, self.dim)
        return torch.argmin(x, dim=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095
dim = 1

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2).cuda()
    return [x]

def get_init_inputs():
    return [dim]
```

Kernel 2 (runtime: 14.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

argmin_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void argmin_dim1_kernel(const float* input, int64_t* output, int B, int D1, int D2) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B * D2) return;

    int b = idx / D2;
    int d2 = idx % D2;

    float min_val = INFINITY;
    int min_index = 0;

    for (int d1 = 0; d1 < D1; ++d1) {
        float val = input[b * D1 * D2 + d1 * D2 + d2];
        if (val < min_val) {
            min_val = val;
            min_index = d1;
        }
    }

    output[idx] = min_index;
}

torch::Tensor argmin_dim1_cuda(torch::Tensor input) {
    TORCH_CHECK(input.is_cuda(), "Input tensor must be on CUDA");
    TORCH_CHECK(input.dim() == 3, "Input tensor must be 3D");
    input = input.contiguous();

    int B = input.size(0);
    int D1 = input.size(1);
    int D2 = input.size(2);

    auto options = torch::TensorOptions().dtype(torch::kLong).device(input.device());
    torch::Tensor output = torch::empty({B, D2}, options);

    int total_threads = B * D2;
    int block_size = 256;
    int grid_size = (total_threads + block_size - 1) / block_size;

    argmin_dim1_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<int64_t>(),
        B,
        D1,
        D2
    );

    return output;
}
"""

argmin_cpp_source = "torch::Tensor argmin_dim1_cuda(torch::Tensor input);"

argmin_ext = load_inline(
    name="argmin_dim1",
    cpp_sources=argmin_cpp_source,
    cuda_sources=argmin_source,
    functions=["argmin_dim1_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.dim == 1:
            return argmin_ext.argmin_dim1_cuda(x)
        else:
            return torch.argmin(x, dim=self.dim)
```
