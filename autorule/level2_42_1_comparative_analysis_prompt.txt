You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 28.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

optimized_kernel_code = '''
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

#define BLOCK_SIZE 256
#define VEC_SIZE 4

__global__ void fused_pool_bias_kernel(
    const float* input,
    const float* bias,
    float* output,
    int B, int C, int H, int W
) {
    int bc = blockIdx.x;
    int b = bc / C;
    int c = bc % C;

    if (b >= B || c >= C) return;

    const int input_offset = b*C*H*W + c*H*W;
    const int num_elements = H * W;
    const int num_vecs = num_elements / VEC_SIZE;

    float sum = 0.0f;

    // Vectorized processing of 4 elements per thread
    for(int i = threadIdx.x; i < num_vecs; i += blockDim.x) {
        float4 vals = *reinterpret_cast<const float4*>(input + input_offset + i*VEC_SIZE);
        sum += vals.x + vals.y + vals.z + vals.w;
    }

    // Process remaining elements
    const int remainder_start = num_vecs * VEC_SIZE;
    for(int i = remainder_start + threadIdx.x; i < num_elements; i += blockDim.x) {
        sum += input[input_offset + i];
    }

    // Block reduction with shared memory
    __shared__ float shared[BLOCK_SIZE];
    shared[threadIdx.x] = sum;
    __syncthreads();

    for(int stride = blockDim.x/2; stride>0; stride>>=1) {
        if(threadIdx.x < stride) {
            shared[threadIdx.x] += shared[threadIdx.x + stride];
        }
        __syncthreads();
    }

    if(threadIdx.x == 0) {
        output[bc] = shared[0]/(H*W) + bias[c];
    }
}

__global__ void fused_lse_scale_kernel(
    const float* input,
    float* output,
    int B, int C
) {
    extern __shared__ float shared[];
    int b = blockIdx.x;
    int tid = threadIdx.x;

    // Find max value with grid-strided loop
    float max_val = -INFINITY;
    for(int i = tid; i < C; i += blockDim.x) {
        max_val = fmaxf(max_val, input[b*C + i]);
    }

    // Block max reduction
    shared[tid] = max_val;
    __syncthreads();
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(tid < stride) {
            shared[tid] = fmaxf(shared[tid], shared[tid + stride]);
        }
        __syncthreads();
    }
    float block_max = shared[0];
    __syncthreads();

    // Compute sum(exp(x - max)) with grid-strided loop
    float sum = 0.0f;
    for(int i = tid; i < C; i += blockDim.x) {
        sum += expf(input[b*C + i] - block_max);
    }

    // Block sum reduction
    shared[tid] = sum;
    __syncthreads();
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(tid < stride) {
            shared[tid] += shared[tid + stride];
        }
        __syncthreads();
    }

    if(tid == 0) {
        output[b] = (logf(shared[0]) + block_max) * 10.0f;
    }
}

torch::Tensor fused_pool_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    int B = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);
    
    auto output = torch::zeros({B, C, 1, 1}, input.options());
    
    dim3 blocks(B*C);
    fused_pool_bias_kernel<<<blocks, BLOCK_SIZE>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W
    );
    
    return output;
}

torch::Tensor fused_lse_scale_cuda(torch::Tensor input) {
    int B = input.size(0);
    int C = input.size(1);
    
    auto output = torch::zeros({B, 1}, input.options());
    
    dim3 blocks(B);
    size_t shared_mem = BLOCK_SIZE * sizeof(float);
    
    fused_lse_scale_kernel<<<blocks, BLOCK_SIZE, shared_mem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C
    );
    
    return output;
}
'''

# Load optimized operations
custom_ops = load_inline(
    name='custom_ops',
    cpp_sources=[
        "torch::Tensor fused_pool_bias_cuda(torch::Tensor input, torch::Tensor bias);\n"
        "torch::Tensor fused_lse_scale_cuda(torch::Tensor input);"
    ],
    cuda_sources=optimized_kernel_code,
    functions=['fused_pool_bias_cuda', 'fused_lse_scale_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = custom_ops.fused_pool_bias_cuda(x, self.bias)
        x = custom_ops.fused_lse_scale_cuda(x.contiguous().view(x.size(0), x.size(1)))
        return x
```

Kernel 2 (runtime: 28.9 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

optimized_kernel_code = '''
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

#define BLOCK_SIZE 256
#define VEC_SIZE 4

// Fused Pooling + Bias with vectorized loads
__global__ void fused_pool_bias_kernel(
    const float* input,
    const float* bias,
    float* output,
    int B, int C, int H, int W
) {
    int bc = blockIdx.x;
    int b = bc / C;
    int c = bc % C;

    if (b >= B || c >= C) return;

    const int input_offset = b*C*H*W + c*H*W;
    const int num_elements = H * W;
    const int num_vecs = num_elements / VEC_SIZE;
    const int remainder = num_elements % VEC_SIZE;

    float sum = 0.0f;

    // Vectorized processing
    for(int i = threadIdx.x; i < num_vecs; i += blockDim.x) {
        float4 vals = *reinterpret_cast<const float4*>(input + input_offset + i*VEC_SIZE);
        sum += vals.x + vals.y + vals.z + vals.w;
    }

    // Process remaining elements
    for(int i = num_vecs*VEC_SIZE + threadIdx.x; i < num_elements; i += blockDim.x) {
        sum += input[input_offset + i];
    }

    // Block reduction
    __shared__ float shared[BLOCK_SIZE];
    shared[threadIdx.x] = sum;
    __syncthreads();

    for(int stride = blockDim.x/2; stride>0; stride>>=1) {
        if(threadIdx.x < stride) {
            shared[threadIdx.x] += shared[threadIdx.x + stride];
        }
        __syncthreads();
    }

    if(threadIdx.x == 0) {
        output[bc] = shared[0]/(H*W) + bias[c];
    }
}

// Optimized LogSumExp + Scaling with robust reductions
__global__ void fused_lse_scale_kernel(
    const float* input,
    float* output,
    int B, int C
) {
    extern __shared__ float shared[];
    int b = blockIdx.x;
    int tid = threadIdx.x;

    // Phase 1: Find max value
    float max_val = -INFINITY;
    for(int i = tid; i < C; i += blockDim.x) {
        max_val = fmaxf(max_val, input[b*C + i]);
    }

    // Block max reduction
    shared[tid] = max_val;
    __syncthreads();
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(tid < stride) {
            shared[tid] = fmaxf(shared[tid], shared[tid + stride]);
        }
        __syncthreads();
    }
    float block_max = shared[0];
    __syncthreads();

    // Phase 2: Compute sum(exp(x - max))
    float sum = 0.0f;
    for(int i = tid; i < C; i += blockDim.x) {
        sum += expf(input[b*C + i] - block_max);
    }

    // Block sum reduction
    shared[tid] = sum;
    __syncthreads();
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(tid < stride) {
            shared[tid] += shared[tid + stride];
        }
        __syncthreads();
    }

    if(tid == 0) {
        output[b] = (logf(shared[0]) + block_max) * 10.0f;
    }
}

// Wrapper functions
torch::Tensor fused_pool_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    int B = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);
    
    auto output = torch::zeros({B, C, 1, 1}, input.options());
    
    dim3 blocks(B*C);
    fused_pool_bias_kernel<<<blocks, BLOCK_SIZE>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W
    );
    
    return output;
}

torch::Tensor fused_lse_scale_cuda(torch::Tensor input) {
    int B = input.size(0);
    int C = input.size(1);
    
    auto output = torch::zeros({B, 1}, input.options());
    
    dim3 blocks(B);
    size_t shared_mem = BLOCK_SIZE * sizeof(float);
    
    fused_lse_scale_kernel<<<blocks, BLOCK_SIZE, shared_mem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C
    );
    
    return output;
}
'''

# Load optimized operations
custom_ops = load_inline(
    name='custom_ops',
    cpp_sources=[
        "torch::Tensor fused_pool_bias_cuda(torch::Tensor input, torch::Tensor bias);\n"
        "torch::Tensor fused_lse_scale_cuda(torch::Tensor input);"
    ],
    cuda_sources=optimized_kernel_code,
    functions=['fused_pool_bias_cuda', 'fused_lse_scale_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = custom_ops.fused_pool_bias_cuda(x, self.bias)
        x = custom_ops.fused_lse_scale_cuda(x.contiguous().view(x.size(0), x.size(1)))
        return x
```
