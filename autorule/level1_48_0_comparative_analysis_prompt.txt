You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 14.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for mean reduction
mean_reduce_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_reduce_kernel(const float* input, float* output, int dim, int reduce_size, int B, int D1, int D2, int output_numel) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= output_numel) return;

    float sum = 0.0f;

    switch (dim) {
        case 0: {  // Reduce over batch dimension
            int d1 = idx / D2;
            int d2 = idx % D2;
            for (int b = 0; b < B; b++) {
                sum += input[b * D1 * D2 + d1 * D2 + d2];
            }
            break;
        }
        case 1: {  // Reduce over first feature dimension
            int b = idx / D2;
            int d2 = idx % D2;
            for (int d1 = 0; d1 < D1; d1++) {
                sum += input[b * D1 * D2 + d1 * D2 + d2];
            }
            break;
        }
        case 2: {  // Reduce over second feature dimension
            int b = idx / D1;
            int d1 = idx % D1;
            for (int d2 = 0; d2 < D2; d2++) {
                sum += input[b * D1 * D2 + d1 * D2 + d2];
            }
            break;
        }
    }

    output[idx] = sum / reduce_size;
}

torch::Tensor mean_reduce_cuda(torch::Tensor input, int dim) {
    auto sizes = input.sizes();
    int B = sizes[0];
    int D1 = sizes[1];
    int D2 = sizes[2];
    int reduce_size;
    torch::Tensor output;

    if (dim == 0) {
        reduce_size = B;
        output = torch::empty({D1, D2}, input.options());
    } else if (dim == 1) {
        reduce_size = D1;
        output = torch::empty({B, D2}, input.options());
    } else if (dim == 2) {
        reduce_size = D2;
        output = torch::empty({B, D1}, input.options());
    } else {
        throw std::invalid_argument("Invalid reduction dimension");
    }

    int output_numel = output.numel();
    const int block_size = 256;
    int num_blocks = (output_numel + block_size - 1) / block_size;

    mean_reduce_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim,
        reduce_size,
        B,
        D1,
        D2,
        output_numel
    );

    return output;
}
"""

mean_reduce_cpp_source = "torch::Tensor mean_reduce_cuda(torch::Tensor input, int dim);"

# Compile the inline CUDA code
mean_reduce = load_inline(
    name='mean_reduce',
    cpp_sources=mean_reduce_cpp_source,
    cuda_sources=mean_reduce_source,
    functions=['mean_reduce_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    """
    Optimized model using custom CUDA kernel for mean reduction
    """
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.mean_reduce = mean_reduce

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.mean_reduce.mean_reduce_cuda(x, self.dim)
```

Kernel 2 (runtime: 14.4 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for mean reduction along dim=1
mean_dim1_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_dim1_kernel(const float* input, float* output, int B, int D1, int D2) {
    int i = blockIdx.y;
    int j = blockIdx.x * blockDim.x + threadIdx.x;

    if (i >= B || j >= D2) return;

    const float* ptr = input + i * D1 * D2 + j;
    float sum = 0.0f;

    for (int k = 0; k < D1; ++k) {
        sum += ptr[k * D2];
    }

    output[i * D2 + j] = sum / D1;
}

torch::Tensor mean_dim1_cuda(torch::Tensor input) {
    int B = input.size(0);
    int D1 = input.size(1);
    int D2 = input.size(2);

    auto output = torch::zeros({B, D2}, input.options());

    dim3 block(256);
    dim3 grid((D2 + block.x - 1) / block.x, B);

    mean_dim1_kernel<<<grid, block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, D1, D2
    );

    return output;
}
"""

mean_dim1_cpp = "torch::Tensor mean_dim1_cuda(torch::Tensor input);"

# Custom CUDA kernel for mean reduction along dim=2
mean_dim2_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_dim2_kernel(const float* input, float* output, int B, int D1, int D2) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int i = idx / D1;
    int j = idx % D1;

    if (i >= B || j >= D1) return;

    const float* ptr = input + i * D1 * D2 + j * D2;
    float sum = 0.0f;

    for (int k = 0; k < D2; ++k) {
        sum += ptr[k];
    }

    output[i * D1 + j] = sum / D2;
}

torch::Tensor mean_dim2_cuda(torch::Tensor input) {
    int B = input.size(0);
    int D1 = input.size(1);
    int D2 = input.size(2);

    auto output = torch::zeros({B, D1}, input.options());

    int total = B * D1;
    dim3 block(256);
    dim3 grid((total + block.x - 1) / block.x);

    mean_dim2_kernel<<<grid, block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, D1, D2
    );

    return output;
}
"""

mean_dim2_cpp = "torch::Tensor mean_dim2_cuda(torch::Tensor input);"

# Load kernels
mean_dim1_ext = load_inline(
    name='mean_dim1',
    cpp_sources=mean_dim1_cpp,
    cuda_sources=mean_dim1_source,
    functions=['mean_dim1_cuda'],
    verbose=True
)

mean_dim2_ext = load_inline(
    name='mean_dim2',
    cpp_sources=mean_dim2_cpp,
    cuda_sources=mean_dim2_source,
    functions=['mean_dim2_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super(ModelNew, self).__init__()
        self.dim = dim
        
        if dim == 1:
            self.mean_op = mean_dim1_ext.mean_dim1_cuda
        elif dim == 2:
            self.mean_op = mean_dim2_ext.mean_dim2_cuda
        else:
            self.mean_op = None  # Fallback to PyTorch

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.mean_op:
            return self.mean_op(x)
        return torch.mean(x, dim=self.dim)
```
