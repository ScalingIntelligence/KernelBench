You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused ReLU + Bias Add CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_kernel(
    const float* input,
    const float* bias,
    float* output,
    int num_elements,
    int channels,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= num_elements) return;

    int ch = (idx / (height * width)) % channels;
    float val = input[idx];
    val = fmaxf(val, 0.0f) + bias[ch];
    output[idx] = val;
}

torch::Tensor fused_relu_bias_cuda(torch::Tensor x, torch::Tensor bias) {
    auto output = torch::empty_like(x);
    int num_elements = x.numel();
    auto sizes = x.sizes();
    
    const int channels = sizes[1];
    const int height = sizes[2];
    const int width = sizes[3];
    
    const int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;

    fused_relu_bias_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        channels,
        height,
        width
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_relu_bias_cuda(torch::Tensor x, torch::Tensor bias);"

fused_relu_bias = load_inline(
    name='fused_relu_bias',
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=['fused_relu_bias_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_relu_bias = fused_relu_bias

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_relu_bias.fused_relu_bias_cuda(x, self.bias)
        return x
```

Kernel 2 (runtime: 12.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused ReLU + bias add kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_kernel(const float* input, const float* bias, float* output, 
                                      int N, int C, int H, int W, int total_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < total_elements) {
        int c = (idx / (H * W)) % C;  // Calculate channel index
        float val = input[idx];
        val = fmaxf(val, 0.0f);      // ReLU
        output[idx] = val + bias[c]; // Add channel-specific bias
    }
}

torch::Tensor fused_relu_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    auto sizes = input.sizes();
    int N = sizes[0], C = sizes[1], H = sizes[2], W = sizes[3];
    auto output = torch::empty_like(input);
    
    int total = N * C * H * W;
    const int block_size = 256;
    int blocks = (total + block_size - 1) / block_size;
    
    fused_relu_bias_kernel<<<blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W, total
    );
    
    return output;
}
"""

cpp_source = "torch::Tensor fused_relu_bias_cuda(torch::Tensor input, torch::Tensor bias);"

# Load the custom CUDA kernel
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_relu_bias_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_relu_bias_cuda(x, self.bias)
        return x
```
