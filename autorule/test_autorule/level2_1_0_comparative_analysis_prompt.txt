You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 12.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused ReLU + bias add kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_kernel(const float* input, const float* bias, float* output, 
                                      int N, int C, int H, int W, int total_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < total_elements) {
        int c = (idx / (H * W)) % C;  // Calculate channel index
        float val = input[idx];
        val = fmaxf(val, 0.0f);      // ReLU
        output[idx] = val + bias[c]; // Add channel-specific bias
    }
}

torch::Tensor fused_relu_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    auto sizes = input.sizes();
    int N = sizes[0], C = sizes[1], H = sizes[2], W = sizes[3];
    auto output = torch::empty_like(input);
    
    int total = N * C * H * W;
    const int block_size = 256;
    int blocks = (total + block_size - 1) / block_size;
    
    fused_relu_bias_kernel<<<blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W, total
    );
    
    return output;
}
"""

cpp_source = "torch::Tensor fused_relu_bias_cuda(torch::Tensor input, torch::Tensor bias);"

# Load the custom CUDA kernel
fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=cpp_source,
    cuda_sources=fused_kernel_source,
    functions=['fused_relu_bias_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_relu_bias_cuda(x, self.bias)
        return x
```

Kernel 2 (runtime: 12.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define fused ReLU+Bias CUDA kernel
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_kernel(
    const float* conv_output,
    const float* bias,
    float* output,
    int num_elements,
    int channels,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= num_elements) return;

    int n = idx / (channels * height * width);
    int c = (idx / (height * width)) % channels;
    
    float val = conv_output[idx];
    output[idx] = fmaxf(0.0f, val) + bias[c];
}

torch::Tensor fused_relu_bias_cuda(torch::Tensor conv_output, torch::Tensor bias) {
    auto output = torch::empty_like(conv_output);
    int num_elements = conv_output.numel();
    int C = conv_output.size(1);
    int H = conv_output.size(2);
    int W = conv_output.size(3);

    const int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_relu_bias_kernel<<<num_blocks, block_size>>>(
        conv_output.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements,
        C,
        H,
        W
    );

    return output;
}
"""

fused_kernel_cpp = "torch::Tensor fused_relu_bias_cuda(torch::Tensor conv_output, torch::Tensor bias);"

# Load the fused kernel
fused_op = load_inline(
    name="fused_relu_bias",
    cpp_sources=fused_kernel_cpp,
    cuda_sources=fused_kernel_source,
    functions=["fused_relu_bias_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_op = fused_op

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_op.fused_relu_bias_cuda(x, self.bias)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]

# Constants from original code
batch_size = 128
in_channels  = 64  
out_channels = 128  
height = width = 128
kernel_size = 3
bias_shape = (out_channels, 1, 1)
```
