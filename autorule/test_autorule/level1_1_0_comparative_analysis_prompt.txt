You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 45.8 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template<typename scalar_t>
__global__ void fused_matmul_kernel(const scalar_t* A, const scalar_t* B, scalar_t* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if(row >= N || col >= N) return;
    
    scalar_t c = static_cast<scalar_t>(0);
    __shared__ scalar_t SAs[16][16], Sab[16][16];
    
    for(int m = 0; m < (N +15)/16; ++m) {
        // Load A tile
        int a_row = row;
        int a_col = m * 16 + threadIdx.x;
        SAs[threadIdx.y][threadIdx.x] = (a_col < N) ? A[a_row*N + a_col] : static_cast<scalar_t>(0);
        
        // Load B tile
        int b_row = m * 16 + threadIdx.y;
        int b_col = col;
        Sab[threadIdx.y][threadIdx.x] = (b_row < N) ? B[b_row*N + b_col] : static_cast<scalar_t>(0);
        
        __syncthreads();
        
        // Compute partial sums
        for(int k = 0; k < 16; ++k) {
            c += SAs[threadIdx.y][k] * Sab[k][threadIdx.x];
        }
        __syncthreads();
    }
    C[row*N + col] = c;
}

torch::Tensor fused_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Inputs must be 2D");
    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1), "Matrices must be square");
    int N = A.size(0);
    auto C = torch::empty({N, N}, A.options());
    
    dim3 threadsPerBlock(16, 16);
    dim3 blocksPerGrid((N+15)/16, (N+15)/16);
    
    AT_DISPATCH_FLOATING_TYPES(A.scalar_type(), "fused_matmul", ([&]{
        fused_matmul_kernel<scalar_t><<<blocksPerGrid, threadsPerBlock>>>(
            A.data_ptr<scalar_t>(),
            B.data_ptr<scalar_t>(),
            C.data_ptr<scalar_t>(),
            N
        );
    }));
    
    return C;
}
"""

matmul_cpp_source = "torch::Tensor fused_matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul_ext = load_inline(
    name='matmul_ext',
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=['fused_matmul_cuda'],
    verbose=False,
    extra_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.fused_matmul = matmul_ext

    def forward(self, A, B):
        return self.fused_matmul.fused_matmul_cuda(A.contiguous(), B.contiguous())
```

Kernel 2 (runtime: 45.5 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template<typename scalar_t>
__global__ void fused_matmul_kernel(const scalar_t* A, const scalar_t* B, scalar_t* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if(row >= N || col >= N) return;
    
    scalar_t c = static_cast<scalar_t>(0);
    __shared__ scalar_t SAs[32][32], Sab[32][32];
    
    for(int m = 0; m < (N +31)/32; ++m) {
        // Load A tile
        int a_col = m * 32 + threadIdx.x;
        SAs[threadIdx.y][threadIdx.x] = (a_col < N) ? A[row*N + a_col] : static_cast<scalar_t>(0);
        
        // Load B tile
        int b_row = m * 32 + threadIdx.y;
        Sab[threadIdx.y][threadIdx.x] = (b_row < N) ? B[b_row*N + col] : static_cast<scalar_t>(0);
        
        __syncthreads();
        
        // Compute partial sums
        for(int k = 0; k < 32; ++k) {
            c += SAs[threadIdx.y][k] * Sab[k][threadIdx.x];
        }
        // Removed __syncthreads() here
    }
    C[row*N + col] = c;
}

torch::Tensor fused_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, "Inputs must be 2D");
    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1), "Matrices must be square");
    int N = A.size(0);
    auto C = torch::empty({N, N}, A.options());
    
    dim3 threadsPerBlock(32, 32);
    dim3 blocksPerGrid((N+31)/32, (N+31)/32);
    
    AT_DISPATCH_FLOATING_TYPES(A.scalar_type(), "fused_matmul", ([&]{
        fused_matmul_kernel<scalar_t><<<blocksPerGrid, threadsPerBlock>>>(
            A.data_ptr<scalar_t>(),
            B.data_ptr<scalar_t>(),
            C.data_ptr<scalar_t>(),
            N
        );
    }));
    
    return C;
}
"""

matmul_cpp_source = "torch::Tensor fused_matmul_cuda(torch::Tensor A, torch::Tensor B);"

matmul_ext = load_inline(
    name='matmul_ext',
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=['fused_matmul_cuda'],
    verbose=False,
    extra_cflags=['-O3']
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.fused_matmul = matmul_ext

    def forward(self, A, B):
        return self.fused_matmul.fused_matmul_cuda(A.contiguous(), B.contiguous())
```
