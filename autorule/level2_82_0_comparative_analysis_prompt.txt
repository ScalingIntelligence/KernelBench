You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 20.6 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_tanh_scale_bias_kernel(
    const float* input,
    const float* bias,
    float scaling_factor,
    float* output,
    int N, int C, int H, int W
) {
    const int index = blockIdx.x * blockDim.x + threadIdx.x;
    const int total_elements = N * C * H * W;
    if (index >= total_elements) return;

    // Compute channel index
    const int c = (index / (H * W)) % C;
    const float val = input[index];

    const float tanh_val = tanhf(val);
    output[index] = tanh_val * scaling_factor + bias[c];
}

torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling_factor
) {
    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    TORCH_CHECK(bias.dim() == 3, "Bias must be 3D (C,1,1)");
    TORCH_CHECK(scaling_factor > 0, "Scaling factor must be positive");

    auto output = torch::empty_like(input);
    const int numel = input.numel();

    const int threads_per_block = 256;
    const int blocks_per_grid = (numel + threads_per_block - 1) / threads_per_block;

    fused_tanh_scale_bias_kernel<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        input.size(0), input.size(1), input.size(2), input.size(3)
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_tanh_scale_bias_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_tanh_scale_bias_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.max_pool = nn.MaxPool2d(pool_kernel_size)

    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_tanh_scale_bias_cuda(x, self.bias, self.scaling_factor)
        x = self.max_pool(x)
        return x
```

Kernel 2 (runtime: 14.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_tanh_scale_bias_max_pool_kernel(
    const float* input,
    const float* bias,
    float scaling_factor,
    float* output,
    int N, int C, int H_in, int W_in, int H_out, int W_out, int kernel_size
) {
    const int index = blockIdx.x * blockDim.x + threadIdx.x;
    const int total_elements = N * C * H_out * W_out;
    if (index >= total_elements) return;

    const int n = index / (C * H_out * W_out);
    const int rem = index % (C * H_out * W_out);
    const int c = rem / (H_out * W_out);
    const int hw = rem % (H_out * W_out);
    const int h_out = hw / W_out;
    const int w_out = hw % W_out;

    float max_val = -INFINITY;
    for (int i = 0; i < kernel_size; ++i) {
        for (int j = 0; j < kernel_size; ++j) {
            const int h_in = h_out * 4 + i;
            const int w_in = w_out * 4 + j;
            if (h_in < H_in && w_in < W_in) {
                const int input_idx = ((n * C + c) * H_in + h_in) * W_in + w_in;
                const float val = input[input_idx];
                
                // Apply tanh, scale, add bias
                const float tanh_val = tanhf(val);
                const float scaled_val = tanh_val * scaling_factor;
                const float biased_val = scaled_val + bias[c];
                
                if (biased_val > max_val) {
                    max_val = biased_val;
                }
            }
        }
    }

    output[index] = max_val;
}

torch::Tensor fused_tanh_scale_bias_max_pool_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling_factor,
    int kernel_size
) {
    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    TORCH_CHECK(bias.dim() == 3, "Bias must be 3D (C,1,1)");
    TORCH_CHECK(scaling_factor > 0, "Scaling factor must be positive");
    TORCH_CHECK(kernel_size > 0, "Kernel size must be positive");

    const int N = input.size(0);
    const int C = input.size(1);
    const int H_in = input.size(2);
    const int W_in = input.size(3);
    const int H_out = H_in / kernel_size;
    const int W_out = W_in / kernel_size;

    auto output = torch::empty({N, C, H_out, W_out}, input.options());

    const int total_elements = N * C * H_out * W_out;
    const int threads_per_block = 256;
    const int blocks_per_grid = (total_elements + threads_per_block - 1) / threads_per_block;

    fused_tanh_scale_bias_max_pool_kernel<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        N, C, H_in, W_in, H_out, W_out, kernel_size
    );

    return output;
}
"""

fused_ops_cpp = "torch::Tensor fused_tanh_scale_bias_max_pool_cuda(torch::Tensor input, torch::Tensor bias, float scaling_factor, int kernel_size);"

fused_ops = load_inline(
    name='fused_ops',
    cpp_sources=fused_ops_cpp,
    cuda_sources=fused_ops_source,
    functions=['fused_tanh_scale_bias_max_pool_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.pool_kernel_size = pool_kernel_size

    def forward(self, x):
        x = self.conv(x)
        x = fused_ops.fused_tanh_scale_bias_max_pool_cuda(
            x, self.bias, self.scaling_factor, self.pool_kernel_size
        )
        return x
```
