You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 14.0 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized fused GELU + Global Average Pooling kernel with vectorization
fused_gelu_pool_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__inline__ __device__ float gelu_fast(float x) {
    const float a = 0.7978845608f;  // sqrt(2/pi)
    const float b = 0.044715f;
    return 0.5f * x * (1.0f + tanhf(a * (x + b * x * x * x)));
}

__global__ void fused_gelu_pool_kernel(const float* __restrict__ conv_out, 
                                      float* __restrict__ output,
                                      int batch_size, int channels,
                                      int H, int W) {
    const int batch = blockIdx.x;
    const int channel = blockIdx.y;
    
    if (batch >= batch_size || channel >= channels) return;

    const int elements = H * W;
    const int tid = threadIdx.x;
    const int block_threads = blockDim.x;
    const int vec_size = 4;

    float thread_sum = 0.0f;

    // Vectorized processing with float4
    for (int i = tid * vec_size; i < elements; i += block_threads * vec_size) {
        if (i + vec_size <= elements) {
            const int h = i / W;
            const int w = i % W;
            const int idx = ((batch * channels + channel) * H + h) * W + w;
            
            // Load 4 elements at once
            const float4 vals = *reinterpret_cast<const float4*>(&conv_out[idx]);
            
            // Process vector elements
            thread_sum += gelu_fast(vals.x) 
                        + gelu_fast(vals.y) 
                        + gelu_fast(vals.z) 
                        + gelu_fast(vals.w);
        }
    }

    // Block reduction using warp shuffles
    __shared__ float shared_data[256];
    shared_data[tid] = thread_sum;
    __syncthreads();

    // Optimized reduction for 256 threads
    for (int stride = 128; stride > 0; stride >>= 1) {
        if (tid < stride) {
            shared_data[tid] += shared_data[tid + stride];
        }
        __syncthreads();
    }

    // Write final result
    if (tid == 0) {
        output[batch * channels + channel] = shared_data[0] / elements;
    }
}

torch::Tensor fused_gelu_pool_cuda(torch::Tensor conv_out) {
    const auto sizes = conv_out.sizes();
    const int batch_size = sizes[0];
    const int channels = sizes[1];
    const int H = sizes[2];
    const int W = sizes[3];

    auto output = torch::zeros({batch_size, channels}, 
                              torch::device(torch::kCUDA).dtype(torch::kFloat32));

    const dim3 grid(batch_size, channels);
    const int threads = 256;  // Better for occupancy

    fused_gelu_pool_kernel<<<grid, threads>>>(
        conv_out.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        channels,
        H,
        W
    );

    return output;
}
"""

fused_gelu_pool_cpp = "torch::Tensor fused_gelu_pool_cuda(torch::Tensor conv_out);"

# Compile the optimized CUDA extension
fused_gelu_pool = load_inline(
    name='fused_gelu_pool',
    cpp_sources=fused_gelu_pool_cpp,
    cuda_sources=fused_gelu_pool_source,
    functions=['fused_gelu_pool_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.fused_gelu_pool = fused_gelu_pool

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_gelu_pool.fused_gelu_pool_cuda(x)
        return x
```

Kernel 2 (runtime: 14.2 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused GELU + Global Average Pooling CUDA kernel
gelu_pool_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

#define BLOCK_SIZE 256

__global__ void gelu_pool_fused_kernel(const float* input, float* output, 
                                     int batch_size, int out_channels,
                                     int H, int W) {
    const int batch = blockIdx.x;
    const int channel = blockIdx.y;
    const int num_elements = H * W;
    
    extern __shared__ float shared_sum[];
    
    float thread_sum = 0.0f;
    const int start_idx = threadIdx.x;
    const int stride = blockDim.x;
    
    // Calculate base index for this batch and channel
    const int base_idx = (batch * out_channels + channel) * H * W;
    
    for(int i = start_idx; i < num_elements; i += stride) {
        const float val = input[base_idx + i];
        // GELU approximation matching PyTorch's implementation
        const float gelu = 0.5f * val * (1.0f + tanhf(0.7978845608028654f * 
                          (val + 0.044715f * val * val * val)));
        thread_sum += gelu;
    }
    
    shared_sum[start_idx] = thread_sum;
    __syncthreads();
    
    // Parallel reduction
    for(int s = blockDim.x/2; s>0; s>>=1) {
        if(start_idx < s) {
            shared_sum[start_idx] += shared_sum[start_idx + s];
        }
        __syncthreads();
    }
    
    if(start_idx == 0) {
        output[batch * out_channels + channel] = shared_sum[0] / num_elements;
    }
}

torch::Tensor gelu_pool_fused_cuda(torch::Tensor input) {
    const auto batch_size = input.size(0);
    const auto out_channels = input.size(1);
    const auto H = input.size(2);
    const auto W = input.size(3);
    
    auto output = torch::zeros({batch_size, out_channels}, input.options());
    
    const dim3 grid(batch_size, out_channels);
    const size_t shared_mem = BLOCK_SIZE * sizeof(float);
    
    gelu_pool_fused_kernel<<<grid, BLOCK_SIZE, shared_mem>>>(
        input.contiguous().data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_channels,
        H,
        W
    );
    
    return output;
}
"""

gelu_pool_cpp = "torch::Tensor gelu_pool_fused_cuda(torch::Tensor input);"

# Load the custom CUDA kernel
gelu_pool = load_inline(
    name="gelu_pool",
    cpp_sources=gelu_pool_cpp,
    cuda_sources=gelu_pool_source,
    functions=["gelu_pool_fused_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        
    def forward(self, x):
        x = self.conv(x)
        # Use fused GELU + pooling kernel
        x = gelu_pool.gelu_pool_fused_cuda(x)
        return x
```
