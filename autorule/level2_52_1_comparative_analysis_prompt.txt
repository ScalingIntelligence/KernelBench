You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 9.05 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom activation kernel implementation
activation_kernel_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_activation_kernel(const float* input, float* output, int size) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        const float x = input[idx];
        const float softplus = log1pf(expf(-fabsf(x))) + fmaxf(x, 0.0f);
        const float tanh_sp = tanhf(softplus);
        output[idx] = x * tanh_sp;
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    const int num_elements = input.numel();
    const int block_size = 256;
    const int grid_size = (num_elements + block_size - 1) / block_size;
    
    fused_activation_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    
    return output;
}
"""

activation_cpp_source = "torch::Tensor fused_activation_cuda(torch::Tensor input);"

# Load custom activation kernel
fused_activation = load_inline(
    name='fused_activation',
    cpp_sources=activation_cpp_source,
    cuda_sources=activation_kernel_source,
    functions=['fused_activation_cuda'],
    verbose=True
)

class ModelNew(nn.Module):
    """
    Optimized model with fused activation CUDA kernel
    """
    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)
        self.activation = fused_activation

    def forward(self, x):
        x = self.conv(x)
        x = self.activation.fused_activation_cuda(x)
        x = self.bn(x)
        return x
```

Kernel 2 (runtime: 9.08 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom fused activation kernel (softplus -> tanh -> multiply)
fused_activation_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_activation_kernel(const float* input, float* output, int num_elements) {
    const int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index < num_elements) {
        float x = input[index];
        float sp = log1p(expf(x));  // Numerically stable softplus
        float th = tanhf(sp);
        output[index] = th * x;
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor x) {
    torch::Tensor output = torch::empty_like(x);
    int num_elements = x.numel();
    
    const int threads_per_block = 256;
    const int num_blocks = (num_elements + threads_per_block - 1) / threads_per_block;
    
    fused_activation_kernel<<<num_blocks, threads_per_block>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        num_elements
    );
    
    return output;
}
"""

fused_activation_cpp = "torch::Tensor fused_activation_cuda(torch::Tensor x);"

# Load the custom CUDA extension
fused_activation = load_inline(
    name='fused_activation',
    cpp_sources=fused_activation_cpp,
    cuda_sources=fused_activation_source,
    functions=['fused_activation_cuda'],
    verbose=True,
    extra_cuda_cflags=['-O2']
)

class ModelNew(nn.Module):
    """
    Optimized model with fused activation CUDA kernel
    """
    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)
        self.fused_activation = fused_activation

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_activation.fused_activation_cuda(x)
        x = self.bn(x)
        return x
```
