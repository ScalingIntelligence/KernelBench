REASONING TRACE:
Okay, let's break down the reasoning provided and extract the rule-like statements. The user wants these rules to be general enough for various CUDA kernels and objectively verifiable.

First, the reasoning mentions that Kernel 1 uses 4 elements per thread with a strided loop. This leads to better memory coalescing. So a rule here could be about vectorized memory access with strided loops. The example given in the reasoning is "4x vectorized memory access," but to generalize, it's about processing multiple elements per thread with strides.

Next, shared memory usage: Kernel 1 loads the multiplier into shared memory once per block, reducing global memory access. The rule here is using shared memory for data reused across threads in a block. The example mentions this as a key point, so the rule would be about shared memory to reduce global memory access.

Then, avoiding warp divergence by using fmaxf instead of conditionals. The rule here is about using non-branching operations to prevent warp divergence. The reasoning specifically points out that Kernel 1 uses fmaxf instead of an if statement, which is a generalizable practice.

Grid configuration: Kernel 1 uses a 3D grid aligned with data dimensions (batch, channel, spatial), which improves data locality. The rule would be structuring the grid to match data dimensions for better locality. The example mentions 3D grid structure, so the rule should reflect that.

Processing multiple elements per thread: Kernel 1 does 4 elements per thread, increasing arithmetic intensity and occupancy. The general rule here is processing multiple elements per thread to improve occupancy and hide latency.

Explicit handling of batch dimension via grid structure: Kernel 1 uses blockIdx.z for batches, which is more cache-friendly. The rule would involve organizing the grid to explicitly handle batches for better cache usage.

Putting these together, the rules should be concise and applicable to other kernels. Checking the examples given (like shared memory tiling), these extracted rules fit the format. Each statement is a general principle that can be checked in code (e.g., presence of shared memory, grid structure, element per thread count, etc.).


ANSWER:


[
    "The kernel processes multiple elements per thread using strided loops to enable memory coalescing.",
    "The kernel uses shared memory to store frequently accessed data reused across threads within a block.",
    "The kernel avoids warp divergence by using non-branching operations instead of conditional statements for element-wise computations.",
    "The kernel employs a multi-dimensional grid structure aligned with data dimensions (e.g., batch/channel/spatial) to improve memory locality.",
    "The kernel processes multiple elements per thread to increase arithmetic intensity and hide memory latency.",
    "The kernel explicitly structures grid dimensions to maintain cache-friendly access patterns across batches."
]

Usage:
{'inputTokens': 2031, 'outputTokens': 566, 'totalTokens': 2597}