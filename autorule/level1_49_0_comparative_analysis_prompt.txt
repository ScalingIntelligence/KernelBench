You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 66.1 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

max_reduce_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void max_reduce_kernel(const float* input, float* output, 
                                  int dim, int B, int D1, int D2, 
                                  int input_s0, int input_s1, int input_s2,
                                  int output_s0, int output_s1) {
    int output_idx = blockIdx.x;
    int idx1, idx2;

    if (dim == 0) {
        int d1 = output_idx / D2;
        idx1 = d1;
        idx2 = output_idx % D2;
    } else if (dim == 1) {
        int b = output_idx / D2;
        idx1 = b;
        idx2 = output_idx % D2;
    } else {
        int b = output_idx / D1;
        idx1 = b;
        idx2 = output_idx % D1;
    }

    int tid = threadIdx.x;
    float local_max = -INFINITY;
    int elements_per_thread;
    int start, end;

    if (dim == 0) {
        elements_per_thread = (B + blockDim.x - 1) / blockDim.x;
        start = tid * elements_per_thread;
        end = min(start + elements_per_thread, B);
        for (int b = start; b < end; b++) {
            int input_idx = b * input_s0 + idx1 * input_s1 + idx2 * input_s2;
            local_max = fmaxf(local_max, input[input_idx]);
        }
    } else if (dim == 1) {
        elements_per_thread = (D1 + blockDim.x - 1) / blockDim.x;
        start = tid * elements_per_thread;
        end = min(start + elements_per_thread, D1);
        for (int d1 = start; d1 < end; d1++) {
            int input_idx = idx1 * input_s0 + d1 * input_s1 + idx2 * input_s2;
            local_max = fmaxf(local_max, input[input_idx]);
        }
    } else {
        elements_per_thread = (D2 + blockDim.x - 1) / blockDim.x;
        start = tid * elements_per_thread;
        end = min(start + elements_per_thread, D2);
        for (int d2 = start; d2 < end; d2++) {
            int input_idx = idx1 * input_s0 + idx2 * input_s1 + d2 * input_s2;
            local_max = fmaxf(local_max, input[input_idx]);
        }
    }

    // Warp-level reduction
    for (int offset = 16; offset > 0; offset /= 2) {
        float val = __shfl_down_sync(0xffffffff, local_max, offset);
        local_max = fmaxf(local_max, val);
    }

    if (tid % 32 == 0) {
        atomicMax(reinterpret_cast<int*>(&output[idx1 * output_s0 + idx2 * output_s1]), 
                  __float_as_int(local_max));
    }
}

torch::Tensor max_reduce_cuda(torch::Tensor input, int dim) {
    auto sizes = input.sizes().vec();
    int B = sizes[0], D1 = sizes[1], D2 = sizes[2];
    
    std::vector<int64_t> output_shape;
    for (int i = 0; i < 3; ++i) 
        if (i != dim) output_shape.push_back(sizes[i]);
    
    auto output = torch::empty(output_shape, input.options()).fill_(-INFINITY);
    
    int num_output_elements = output.numel();
    const int block_size = 256;
    
    max_reduce_kernel<<<num_output_elements, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim,
        B, D1, D2,
        input.stride(0), input.stride(1), input.stride(2),
        output.dim() > 0 ? output.stride(0) : 0,
        output.dim() > 1 ? output.stride(1) : 0
    );
    
    return output;
}
"""

max_reduce_cpp_source = "torch::Tensor max_reduce_cuda(torch::Tensor input, int dim);"

max_reduce = load_inline(
    name='max_reduce',
    cpp_sources=max_reduce_cpp_source,
    cuda_sources=max_reduce_source,
    functions=['max_reduce_cuda'],
    verbose=True,
    extra_cflags=['-O3'],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return max_reduce.max_reduce_cuda(x, self.dim)
```

Kernel 2 (runtime: 14.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

max_reduce_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <c10/cuda/CUDAGuard.h>

__global__ void max_reduce_kernel(const float* input, float* output, int reduce_size, int inner_dims, int total_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;

    int outer_idx = idx / inner_dims;
    int inner_idx = idx % inner_dims;

    int base = outer_idx * reduce_size * inner_dims + inner_idx;

    float max_val = -INFINITY;
    for (int i = 0; i < reduce_size; ++i) {
        float val = input[base + i * inner_dims];
        max_val = max(max_val, val);
    }

    output[idx] = max_val;
}

torch::Tensor max_reduce_cuda(torch::Tensor input, int dim) {
    input = input.contiguous();
    TORCH_CHECK(input.is_cuda(), "Input tensor must be on CUDA");
    TORCH_CHECK(dim >= 0 && dim < input.dim(), "dim is out of range");

    const at::cuda::OptionalCUDAGuard device_guard(input.device());

    auto original_input_shape = input.sizes().vec();
    int reduce_size = original_input_shape[dim];
    auto output_shape = original_input_shape;
    output_shape.erase(output_shape.begin() + dim);
    auto output = torch::empty(output_shape, input.options());

    int64_t inner_dims = 1;
    for (int i = dim + 1; i < original_input_shape.size(); ++i) {
        inner_dims *= original_input_shape[i];
    }

    int64_t total_elements = output.numel();

    if (total_elements > 0) {
        const int block_size = 256;
        const int num_blocks = (total_elements + block_size - 1) / block_size;

        const float* input_data = input.data_ptr<float>();
        float* output_data = output.data_ptr<float>();

        max_reduce_kernel<<<num_blocks, block_size>>>(input_data, output_data, reduce_size, inner_dims, total_elements);
    }

    return output;
}
"""

max_reduce_cpp_source = "torch::Tensor max_reduce_cuda(torch::Tensor input, int dim);"

max_reduce = load_inline(
    name="max_reduce",
    cpp_sources=max_reduce_cpp_source,
    cuda_sources=max_reduce_source,
    functions=["max_reduce_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super(ModelNew, self).__init__()
        self.dim = dim
        self.max_reduce = max_reduce

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.max_reduce.max_reduce_cuda(x, self.dim)
```
