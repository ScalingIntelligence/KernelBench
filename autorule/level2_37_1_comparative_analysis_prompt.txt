You are a kernel expert. You are given two CUDA kernels that solve the same problem. Both kernels are correct, but one is faster than the other. Analyze why one is faster than the other.
Kernel 1 (runtime: 17.3 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized Swish+Bias kernel with vectorized loads and fast math
swish_bias_source = """
#include <torch/extension.h>
#include <cuda_fp16.h>

__global__ void swish_bias_kernel(const float* x, const float* bias, float* out, int rows, int cols) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int row = tid / (cols / 4);
    int col = tid % (cols / 4);
    
    if (row >= rows) return;

    float4 x4 = reinterpret_cast<const float4*>(x + row * cols)[col];
    float4 b4 = reinterpret_cast<const float4*>(bias)[col];
    float4 out4;

    // Fast approximate sigmoid using __expf
    out4.x = x4.x * (1.0f / (1.0f + __expf(-x4.x))) + b4.x;
    out4.y = x4.y * (1.0f / (1.0f + __expf(-x4.y))) + b4.y;
    out4.z = x4.z * (1.0f / (1.0f + __expf(-x4.z))) + b4.z;
    out4.w = x4.w * (1.0f / (1.0f + __expf(-x4.w))) + b4.w;

    reinterpret_cast<float4*>(out + row * cols)[col] = out4;
}

torch::Tensor swish_bias_cuda(torch::Tensor x, torch::Tensor bias) {
    auto out = torch::empty_like(x);
    int rows = x.size(0);
    int cols = x.size(1);
    
    const int vec_size = 4;
    int num_threads = (rows * cols) / vec_size;
    const int block_size = 256;
    int num_blocks = (num_threads + block_size - 1) / block_size;

    swish_bias_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), 
                                                bias.data_ptr<float>(),
                                                out.data_ptr<float>(),
                                                rows, cols);
    return out;
}
"""

# Optimized GroupNorm kernel with warp-level reductions
group_norm_source = """
#include <torch/extension.h>
#include <cuda_fp16.h>

__global__ void group_norm_kernel(
    const float* input, const float* gamma, const float* beta,
    float* output, int num_samples, int num_features,
    int num_groups, float eps) {
    
    const int group_id = blockIdx.x;
    const int sample_id = blockIdx.y;
    const int group_size = num_features / num_groups;
    const int c_start = group_id * group_size;
    const int c = c_start + threadIdx.x;

    if (sample_id >= num_samples || group_id >= num_groups || c >= (group_id+1)*group_size) return;

    const int idx = sample_id * num_features + c;
    const float x = input[idx];
    
    // Warp-level reduction using shuffle instructions
    float sum = x;
    float sqsum = x * x;

    for (int offset = 16; offset > 0; offset >>= 1) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
        sqsum += __shfl_down_sync(0xffffffff, sqsum, offset);
    }

    __shared__ float shared_sum[2];
    __shared__ float shared_sqsum[2];
    __shared__ float mean, inv_std;

    if (threadIdx.x % 32 == 0) {
        int warp_id = threadIdx.x / 32;
        shared_sum[warp_id] = sum;
        shared_sqsum[warp_id] = sqsum;
    }
    __syncthreads();

    if (threadIdx.x < 2) {
        float total_sum = shared_sum[0] + shared_sum[1];
        float total_sqsum = shared_sqsum[0] + shared_sqsum[1];
        mean = total_sum / group_size;
        float var = total_sqsum / group_size - mean * mean;
        inv_std = rsqrtf(var + eps);
    }
    __syncthreads();

    output[idx] = (x - mean) * inv_std * gamma[c] + beta[c];
}

torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, 
                             int num_groups, float eps) {
    auto output = torch::empty_like(input);
    const int num_samples = input.size(0);
    
    dim3 grid(num_groups, num_samples);
    group_norm_kernel<<<grid, input.size(1)/num_groups>>>(
        input.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        num_samples,
        input.size(1),
        num_groups,
        eps
    );
    return output;
}
"""

# Load optimized CUDA extensions
swish_bias = load_inline(
    name='swish_bias',
    cpp_sources="torch::Tensor swish_bias_cuda(torch::Tensor x, torch::Tensor bias);",
    cuda_sources=swish_bias_source,
    functions=['swish_bias_cuda'],
    extra_cuda_cflags=['-O3', '--use_fast_math'],
    verbose=True
)

group_norm = load_inline(
    name='group_norm',
    cpp_sources="torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor gamma, torch::Tensor beta, int num_groups, float eps);",
    cuda_sources=group_norm_source,
    functions=['group_norm_cuda'],
    extra_cuda_cflags=['-O3', '--use_fast_math'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super().__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.swish_bias = swish_bias
        self.group_norm_op = group_norm

    def forward(self, x):
        x = self.matmul(x)
        x = self.swish_bias.swish_bias_cuda(x, self.bias)
        x = self.group_norm_op.group_norm_cuda(
            x, 
            self.group_norm.weight, 
            self.group_norm.bias, 
            self.group_norm.num_groups, 
            1e-5
        )
        return x

batch_size = 32768
in_features = 1024
out_features = 4096
num_groups = 64
bias_shape = (out_features,)

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features, num_groups, bias_shape]
```

Kernel 2 (runtime: 17.7 ms):
```
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Optimized SwishBias with vectorized float4 loads
swish_bias_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_fp16.h>

__global__ void swish_bias_kernel(const float* __restrict__ input, 
                                  const float* __restrict__ bias, 
                                  float* __restrict__ output, 
                                  int batch_size, int out_features) {
    const int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
    const int total = batch_size * out_features;
    if (idx >= total) return;

    const int col = idx % out_features;
    const int bias_idx = col / 4;

    float4 in4 = reinterpret_cast<const float4*>(input)[idx/4];
    float4 bias4 = reinterpret_cast<const float4*>(bias)[bias_idx];

    float4 out4;
    out4.x = in4.x * (1.0f / (1.0f + expf(-in4.x))) + bias4.x;
    out4.y = in4.y * (1.0f / (1.0f + expf(-in4.y))) + bias4.y;
    out4.z = in4.z * (1.0f / (1.0f + expf(-in4.z))) + bias4.z;
    out4.w = in4.w * (1.0f / (1.0f + expf(-in4.w))) + bias4.w;

    reinterpret_cast<float4*>(output)[idx/4] = out4;
}

torch::Tensor swish_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    auto batch_size = input.size(0);
    auto out_features = input.size(1);
    auto output = torch::empty_like(input);

    int total_elements = batch_size * out_features;
    int block_size = 256;
    int num_blocks = (total_elements + 4*block_size - 1) / (4*block_size);

    swish_bias_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features
    );

    return output;
}
"""

swish_bias_cpp = "torch::Tensor swish_bias_cuda(torch::Tensor input, torch::Tensor bias);"

# Optimized GroupNorm with warp-level reductions and minimal shared memory
group_norm_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void group_norm_kernel(const float* __restrict__ input,
                                  const float* __restrict__ weight,
                                  const float* __restrict__ bias,
                                  float* __restrict__ output,
                                  int batch_size, int out_features,
                                  int num_groups, float eps) {
    const int group_size = out_features / num_groups;
    const int sample_idx = blockIdx.y;
    const int group_idx = blockIdx.x;
    const int tid = threadIdx.x;

    if (sample_idx >= batch_size || group_idx >= num_groups || tid >= group_size) return;

    const int channel = group_idx * group_size + tid;
    const int idx = sample_idx * out_features + channel;

    const float val = input[idx];
    const float val_sq = val * val;

    // Warp reduction for sum and sum_sq
    float sum = val;
    float sum_sq = val_sq;
    for (int offset = 16; offset > 0; offset >>= 1) {
        sum += __shfl_down_sync(0xffffffff, sum, offset);
        sum_sq += __shfl_down_sync(0xffffffff, sum_sq, offset);
    }

    __shared__ float warp_sums[2][2];
    if (tid % 32 == 0) {
        int warp_id = tid / 32;
        warp_sums[warp_id][0] = sum;
        warp_sums[warp_id][1] = sum_sq;
    }
    __syncthreads();

    if (tid == 0) {
        float total_sum = 0.0f, total_sum_sq = 0.0f;
        const int num_warps = (group_size + 31) / 32;
        for (int w = 0; w < num_warps; ++w) {
            total_sum += warp_sums[w][0];
            total_sum_sq += warp_sums[w][1];
        }
        const float mean = total_sum / group_size;
        const float var = total_sum_sq / group_size - mean * mean;
        const float inv_std = rsqrtf(var + eps);

        warp_sums[0][0] = mean;
        warp_sums[0][1] = inv_std;
    }
    __syncthreads();

    output[idx] = (val - warp_sums[0][0]) * warp_sums[0][1] * weight[channel] + bias[channel];
}

torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor weight, 
                             torch::Tensor bias, int num_groups, float eps) {
    auto batch_size = input.size(0);
    auto out_features = input.size(1);
    const int group_size = out_features / num_groups;

    auto output = torch::empty_like(input);
    dim3 grid(num_groups, batch_size);
    dim3 block(group_size);

    group_norm_kernel<<<grid, block, sizeof(float)*2*2>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        out_features,
        num_groups,
        eps
    );

    return output;
}
"""

group_norm_cpp = "torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int num_groups, float eps);"

# Load extensions with fast math optimization
swish_bias_ext = load_inline(
    name='swish_bias',
    cpp_sources=swish_bias_cpp,
    cuda_sources=swish_bias_source,
    functions=['swish_bias_cuda'],
    extra_cuda_cflags=['-use_fast_math'],
    verbose=True
)

group_norm_ext = load_inline(
    name='group_norm',
    cpp_sources=group_norm_cpp,
    cuda_sources=group_norm_source,
    functions=['group_norm_cuda'],
    extra_cuda_cflags=['-use_fast_math'],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(ModelNew, self).__init__()
        self.matmul = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.num_groups = num_groups
        self.group_norm_weight = nn.Parameter(torch.ones(out_features))
        self.group_norm_bias = nn.Parameter(torch.zeros(out_features))
        self.eps = 1e-5

    def forward(self, x):
        x = self.matmul(x)
        x = swish_bias_ext.swish_bias_cuda(x, self.bias)
        x = group_norm_ext.group_norm_cuda(x, self.group_norm_weight, 
                                         self.group_norm_bias, self.num_groups, self.eps)
        return x
```
