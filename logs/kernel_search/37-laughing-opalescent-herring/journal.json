{"nodes":[{"code":"import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Define TILE_DIM for shared memory tiling. A common choice is 32.\n#define TILE_DIM 32\n\n__global__ void matmul_tiled_kernel(const float* A, const float* B, float* C, int N) {\n    // Shared memory for tiles of A and B\n    // Using volatile to prevent compiler from optimizing away redundant reads/writes\n    // which can happen across __syncthreads() in some older CUDA versions/compilers,\n    // though generally not strictly necessary for modern PyTorch/CUDA.\n    __shared__ float sA[TILE_DIM][TILE_DIM];\n    __shared__ float sB[TILE_DIM][TILE_DIM];\n\n    // Thread coordinates within the block\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n\n    // Global coordinates for the C element this thread computes\n    int row = blockIdx.y * TILE_DIM + ty;\n    int col = blockIdx.x * TILE_DIM + tx;\n\n    float Cvalue = 0.0f; // Accumulator for C[row][col], stored in a register\n\n    // Loop over the K dimension (inner product)\n    // Each iteration processes a TILE_DIM x TILE_DIM block of the K dimension\n    for (int k_block_offset = 0; k_block_offset < N; k_block_offset += TILE_DIM) {\n        // Load tiles of A and B from global memory into shared memory.\n        // Each thread loads one element.\n        // sA[ty][tx] loads A[row][k_block_offset + tx]\n        // sB[ty][tx] loads B[k_block_offset + ty][col]\n        \n        // Ensure global memory accesses are within bounds of A and B\n        // For A: row index `row`, column index `k_block_offset + tx`\n        if (row < N && (k_block_offset + tx) < N) {\n            sA[ty][tx] = A[row * N + (k_block_offset + tx)];\n        } else {\n            sA[ty][tx] = 0.0f; // Pad with zero if out of bounds (handles non-multiples of TILE_DIM)\n        }\n\n        // For B: row index `k_block_offset + ty`, column index `col`\n        if ((k_block_offset + ty) < N && col < N) {\n            sB[ty][tx] = B[(k_block_offset + ty) * N + col];\n        } else {\n            sB[ty][tx] = 0.0f; // Pad with zero if out of bounds\n        }\n\n        // Synchronize to ensure all shared memory loads are complete before computation begins\n        __syncthreads();\n\n        // Perform dot product using the loaded shared memory tiles\n        for (int k_inner = 0; k_inner < TILE_DIM; ++k_inner) {\n            Cvalue += sA[ty][k_inner] * sB[k_inner][tx];\n        }\n\n        // Synchronize before loading the next set of tiles to avoid race conditions\n        __syncthreads();\n    }\n\n    // Write the accumulated result back to global memory\n    if (row < N && col < N) {\n        C[row * N + col] = Cvalue;\n    }\n}\n\n// C++ wrapper function to launch the CUDA kernel\ntorch::Tensor matmul_tiled_cuda(torch::Tensor A, torch::Tensor B) {\n    // Input validation (optional, but good practice)\n    TORCH_CHECK(A.is_cuda() && B.is_cuda(), \"Inputs must be CUDA tensors.\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices.\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Matrix dimensions mismatch for multiplication.\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1) && A.size(0) == B.size(0), \n                \"Matrices must be square and of the same size.\");\n    TORCH_CHECK(A.scalar_type() == torch::kFloat && B.scalar_type() == torch::kFloat, \n                \"Inputs must be float32 (torch.float).\");\n\n    int N = A.size(0);\n    // Create output tensor C on the same device and with the same dtype as A\n    auto C = torch::zeros({N, N}, A.options());\n\n    // Define grid and block dimensions for the kernel launch\n    dim3 block_size(TILE_DIM, TILE_DIM);\n    dim3 grid_size((N + TILE_DIM - 1) / TILE_DIM, (N + TILE_DIM - 1) / TILE_DIM);\n\n    // Launch the CUDA kernel\n    matmul_tiled_kernel<<<grid_size, block_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N\n    );\n\n    return C;\n}\n\"\"\"\n\n# C++ function declaration for load_inline\ncpp_source = \"torch::Tensor matmul_tiled_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile the inline CUDA code\n# The 'name' argument should be unique if multiple extensions are loaded\ncustom_matmul_extension = load_inline(\n    name=\"custom_matmul_extension\",\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=[\"matmul_tiled_cuda\"],\n    verbose=False,  # Set to True for debugging compilation issues\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single square matrix multiplication (C = A * B)\n    using a custom tiled CUDA kernel.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the loaded custom CUDA function as an attribute\n        self.custom_matmul_op = custom_matmul_extension.matmul_tiled_cuda\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom tiled CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # The custom CUDA kernel expects CUDA tensors, ensure inputs are on GPU.\n        # This is typically handled by the calling context, but good to be aware.\n        return self.custom_matmul_op(A, B)\n","plan":"The optimization plan focuses on replacing the standard `torch.matmul` operation with a custom tiled matrix multiplication CUDA kernel. This approach leverages shared memory to cache portions of the input matrices, significantly reducing global memory traffic and improving data reuse. Each thread block will compute a tile of the output matrix `C`, with threads cooperatively loading corresponding tiles of `A` and `B` into shared memory, performing local dot products, and accumulating results. This design prioritizes correctness and basic performance gains through efficient memory access patterns and parallelism.","step":0,"id":"5fe8c90251ed4ece988d4f5a17565975","ctime":1762746291.093126,"parent":null,"children":[],"_term_out":["================================================================================","KERNEL EVALUATION SUCCESS","================================================================================","\u2713 Compilation: PASSED","\u2713 Correctness: PASSED","  Correctness trials: (5 / 5)","\n\u2713 Performance: 37.600 ms","  Mean: 37.600 ms","  Std:  0.452 ms","  Min:  37.400 ms","  Max:  40.700 ms","  Trials: 100","\nHardware: NVIDIA TITAN V","Runtime: 0.038 seconds"],"exec_time":0.0376,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"Kernel implementation successful! Runtime: 37.600 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled.","metric":{"value":37.6,"maximize":false},"is_buggy":false}],"node2parent":{},"__version":"2"}