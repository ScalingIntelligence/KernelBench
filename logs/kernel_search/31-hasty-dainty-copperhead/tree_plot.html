<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [], "layout": [[0.5, 0.0]], "plan": ["Optimization Plan: 1.  **Identify Bottleneck:** The primary performance\nbottleneck in the given architecture is the `torch.matmul` operation, especially\nfor large square matrices. 2.  **Custom Kernel Implementation:** Replace\n`torch.matmul` with a custom CUDA kernel to gain finer control over GPU\nexecution and memory access patterns. 3.  **Tiled Matrix Multiplication:**\nImplement a tiled matrix multiplication algorithm within the CUDA kernel. This\napproach divides the matrices into smaller blocks (tiles) and uses shared memory\nto cache these tiles. 4.  **Shared Memory Utilization:** By loading sub-tiles of\nmatrices A and B into shared memory, we maximize data reuse and reduce global\nmemory traffic, which is critical for performance on GPUs. 5.  **Coalesced\nMemory Access:** Ensure that global memory reads and writes, as well as shared\nmemory loads, are coalesced where possible to fully utilize memory bandwidth."], "code": ["import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the CUDA kernel code for tiled matrix multiplication\ncuda_kernel_code = \"\"\"\n#define BLOCK_SIZE 32 // Define tile size for shared memory and thread block dimensions\n\n__global__ void matmul_kernel(float* A, float* B, float* C, int N) {\n    // Determine the global row and column for the current thread's C element\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n\n    // Accumulator for the C[row][col] element\n    float C_value = 0;\n\n    // Shared memory for tiles of A and B\n    // Using BLOCK_SIZE x BLOCK_SIZE for simplicity and typical efficiency.\n    // Padding (e.g., BLOCK_SIZE + 1) could be used to prevent bank conflicts,\n    // but for BLOCK_SIZE = 32, it's often not strictly necessary as bank conflicts\n    // are less likely with this access pattern and modern hardware.\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    // Loop over the tiles in the K-dimension (inner product dimension)\n    // (N + BLOCK_SIZE - 1) / BLOCK_SIZE calculates ceil(N / BLOCK_SIZE)\n    int numTiles = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    for (int tile_idx = 0; tile_idx < numTiles; ++tile_idx) {\n        // Calculate global column index for A and global row index for B\n        int k_global = tile_idx * BLOCK_SIZE + threadIdx.x;\n\n        // Load sub-tile of A into shared memory\n        // Each thread loads one element. Threads in a row load elements for As[threadIdx.y][threadIdx.x]\n        // ensuring coalesced access to global memory A.\n        if (row < N && k_global < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k_global];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f; // Pad with zeros for out-of-bounds access\n        }\n\n        // Calculate global row index for B and global column index for A (same k_global logic)\n        k_global = tile_idx * BLOCK_SIZE + threadIdx.y; // Different k_global for B loading\n                                                        // B needs row-major access to its tile\n        // Load sub-tile of B into shared memory\n        // Each thread loads one element. Threads in a column load elements for Bs[threadIdx.y][threadIdx.x]\n        // ensuring coalesced access to global memory B.\n        if (k_global < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[k_global * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f; // Pad with zeros for out-of-bounds access\n        }\n\n        // Synchronize threads within the block after loading shared memory tiles\n        // This ensures all data is available before computation begins.\n        __syncthreads();\n\n        // Perform dot product for the current tile\n        // Each thread computes one element of the output tile.\n        // It iterates over the K-dimension (BLOCK_SIZE elements) of the shared memory tiles.\n        for (int k = 0; k < BLOCK_SIZE; ++k) {\n            C_value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        // Synchronize threads within the block before loading the next shared memory tiles\n        // This ensures all computation for the current tile is done before shared memory is overwritten.\n        __syncthreads();\n    }\n\n    // Write the accumulated result to global memory\n    if (row < N && col < N) {\n        C[row * N + col] = C_value;\n    }\n}\n\"\"\"\n\n# Define the C++ wrapper code to interface with PyTorch\ncpp_wrapper_code = \"\"\"\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n// Declare the CUDA kernel (must match the signature in cuda_kernel_code)\n__global__ void matmul_kernel(float* A, float* B, float* C, int N);\n\n// C++ function that will be exposed to Python\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    // Input validation\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Matrices must be 2D.\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1), \"Matrices must be square for this kernel.\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Inner dimensions must match for matrix multiplication.\");\n    TORCH_CHECK(A.is_cuda() && B.is_cuda(), \"Inputs must be CUDA tensors.\");\n    TORCH_CHECK(A.dtype() == torch::kFloat32 && B.dtype() == torch::kFloat32, \"Inputs must be float32.\");\n\n    int N = A.size(0); // Assuming square matrices N x N\n\n    // Allocate the output tensor C on the GPU with the same options (device, dtype) as A\n    torch::Tensor C = torch::empty({N, N}, A.options());\n\n    // Get raw pointers to the tensor data\n    float* A_ptr = A.data_ptr<float>();\n    float* B_ptr = B.data_ptr<float>();\n    float* C_ptr = C.data_ptr<float>();\n\n    // Define block and grid dimensions for the kernel launch\n    const int BLOCK_SIZE = 32; // Must match the #define in the CUDA kernel\n    dim3 dimGrid((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n\n    // Launch the CUDA kernel\n    matmul_kernel<<<dimGrid, dimBlock>>>(A_ptr, B_ptr, C_ptr, N);\n\n    // Optional: Check for CUDA errors. Good for debugging.\n    // cudaDeviceSynchronize();\n    // cudaError_t err = cudaGetLastError();\n    // if (err != cudaSuccess) {\n    //     std::cerr << \"CUDA error in matmul_forward: \" << cudaGetErrorString(err) << std::endl;\n    // }\n\n    return C;\n}\n\"\"\"\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single square matrix multiplication (C = A * B)\n    using a custom CUDA kernel with a tiled shared memory approach.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Compile the CUDA kernel and C++ wrapper only once during initialization.\n        # This creates a Python callable `custom_matmul_op` that executes our CUDA code.\n        self.custom_matmul_op = load_inline(\n            name=\"custom_matmul_module\",\n            cpp_sources=cpp_wrapper_code,\n            cuda_sources=cuda_kernel_code,\n            functions=[\"matmul_forward\"],  # Name of the C++ function to expose\n            with_cuda=True,\n            extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],  # Optimization flags for nvcc\n        ).matmul_forward\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # Ensure inputs are on the CUDA device and are contiguous.\n        # `contiguous()` is important for ensuring proper memory layout for raw pointer access.\n        # `torch.rand` typically produces contiguous tensors, but it's good practice to ensure.\n        A_cuda = A.contiguous().cuda()\n        B_cuda = B.contiguous().cuda()\n\n        # Call the compiled custom CUDA operation\n        return self.custom_matmul_op(A_cuda, B_cuda)"], "term_out": ["================================================================================CORRECTNESS CHECK FAILED================================================================================Runtime Error: builtins.RuntimeError\nError building extension 'custom_matmul_module': [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=custom_matmul_module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /scratch/sa6740/KernelBench/workspaces/kernel_search/31-hasty-dainty-copperhead/cuda_build/custom_matmul_module/main.cpp -o main.o \nFAILED: [code=1] main.o \nc++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=custom_matmul_module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /scratch/sa6740/KernelBench/workspaces/kernel_search/31-hasty-dainty-copperhead/cuda_build/custom_matmul_module/main.cpp -o main.o \n/scratch/sa6740/KernelBench/workspaces/kernel_search/31-hasty-dainty-copperhead/cuda_build/custom_matmul_module/main.cpp:4:10: fatal error: cuda.h: No such file or directory\n    4 | #include <cuda.h>\n      |          ^~~~~~~~\ncompilation terminated.\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=custom_matmul_module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /scratch/sa6740/miniconda3/envs/kernel-bench/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -c /scratch/sa6740/KernelBench/workspaces/kernel_search/31-hasty-dainty-copperhead/cuda_build/custom_matmul_module/cuda.cu -o cuda.cuda.o \nninja: build stopped: subcommand failed.\n"], "analysis": ["Runtime error during correctness testing. Error: builtins.RuntimeError. The kernel compiled but failed during execution. Check for memory access violations, synchronization issues, or invalid operations."], "exp_name": "31-hasty-dainty-copperhead", "metrics": [0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
