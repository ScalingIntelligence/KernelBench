{"nodes":[{"code":"import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// TILE_SIZE defines the dimensions of the shared memory tiles\n// and the number of threads per dimension in a block.\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    // Block index\n    int blockRow = blockIdx.y;\n    int blockCol = blockIdx.x;\n\n    // Thread index within the block\n    int threadRow = threadIdx.y;\n    int threadCol = threadIdx.x;\n\n    // Shared memory for A_sub and B_sub tiles\n    // These will store TILE_SIZE x TILE_SIZE sub-matrices from A and B\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    // C_value is used to accumulate the element C[row][col]\n    // that is computed by the current thread\n    float C_value = 0.0f;\n\n    // Calculate the global row and column of the C element computed by this thread\n    int row = blockRow * TILE_SIZE + threadRow;\n    int col = blockCol * TILE_SIZE + threadCol;\n\n    // Loop over the K dimension (inner product) in steps of TILE_SIZE\n    // Each iteration processes a pair of tiles from A and B\n    for (int k_tile = 0; k_tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++k_tile) {\n        // Load A_sub from global memory to shared memory sA\n        // Each thread loads one element.\n        // The element is from A[row][k_tile*TILE_SIZE + threadCol]\n        int globalA_row = row;\n        int globalA_col = k_tile * TILE_SIZE + threadCol;\n        if (globalA_row < N && globalA_col < N) {\n            sA[threadRow][threadCol] = A[globalA_row * N + globalA_col];\n        } else {\n            // Pad with zero if out of bounds (handles non-divisible N, though N=4096 is divisible by TILE_SIZE=32)\n            sA[threadRow][threadCol] = 0.0f; \n        }\n\n        // Load B_sub from global memory to shared memory sB\n        // Each thread loads one element.\n        // The element is from B[k_tile*TILE_SIZE + threadRow][col]\n        int globalB_row = k_tile * TILE_SIZE + threadRow;\n        int globalB_col = col;\n        if (globalB_row < N && globalB_col < N) {\n            sB[threadRow][threadCol] = B[globalB_row * N + globalB_col];\n        } else {\n            // Pad with zero if out of bounds\n            sB[threadRow][threadCol] = 0.0f;\n        }\n\n        // Synchronize to make sure all data is loaded into shared memory\n        __syncthreads();\n\n        // Perform the matrix multiplication for the current tiles\n        // Each thread computes a partial sum for its C_value\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            C_value += sA[threadRow][i] * sB[i][threadCol];\n        }\n\n        // Synchronize to make sure all computations are done\n        // before the next iteration (new data loaded into shared memory)\n        __syncthreads();\n    }\n\n    // Write the final result from C_value to global memory C\n    if (row < N && col < N) {\n        C[row * N + col] = C_value;\n    }\n}\n\n// C++ wrapper function to launch the CUDA kernel\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Ensure inputs are CUDA tensors and of float type\n    TORCH_CHECK(A.is_cuda(), \"Input A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"Input B must be a CUDA tensor\");\n    TORCH_CHECK(A.dtype() == torch::kFloat, \"Input A must be a float tensor\");\n    TORCH_CHECK(B.dtype() == torch::kFloat, \"Input B must be a float tensor\");\n\n    // Ensure inputs are contiguous for direct pointer access\n    A = A.contiguous();\n    B = B.contiguous();\n    \n    // Basic shape checks\n    TORCH_CHECK(A.dim() == 2, \"Input A must be 2D\");\n    TORCH_CHECK(B.dim() == 2, \"Input B must be 2D\");\n    TORCH_CHECK(A.size(0) == A.size(1), \"Input A must be square\");\n    TORCH_CHECK(B.size(0) == B.size(1), \"Input B must be square\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Inner dimensions of A and B must match for matrix multiplication\");\n\n    int N = A.size(0); // Assuming square matrices N x N\n\n    // Allocate output tensor C with the same options (device, dtype) as A\n    auto C = torch::empty({N, N}, A.options());\n\n    // Calculate grid and block dimensions for the tiled kernel\n    // Block dimensions are TILE_SIZE x TILE_SIZE threads\n    dim3 block_dim(TILE_SIZE, TILE_SIZE);\n    // Grid dimensions are (N/TILE_SIZE) x (N/TILE_SIZE) blocks\n    dim3 grid_dim((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    // Launch the kernel\n    matmul_kernel<<<grid_dim, block_dim>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N\n    );\n    \n    return C;\n}\n\"\"\"\n\n# C++ function declaration for load_inline\ncpp_source = \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile the inline CUDA code\ncustom_matmul_module = load_inline(\n    name=\"custom_matmul\",\n    cpp_sources=cpp_source,  # C++ function declaration\n    cuda_sources=cuda_source,  # CUDA kernel implementation and C++ wrapper\n    functions=[\"matmul_cuda\"],  # The C++ function name to expose to Python\n    verbose=True,  # Enable verbose output for compilation details\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single square matrix multiplication (C = A * B)\n    using a custom CUDA kernel with tiled shared memory approach.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the compiled CUDA function as an attribute\n        self.custom_matmul = custom_matmul_module.matmul_cuda\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return self.custom_matmul(A, B)\n","plan":"Optimization plan:\n1.  Identify the `torch.matmul` operation as the target for optimization.\n2.  Implement a custom CUDA kernel for square matrix multiplication using a tiled approach with shared memory. This strategy improves data reuse and memory access patterns by loading smaller blocks of input matrices into faster on-chip shared memory.\n3.  The kernel will divide the output matrix into tiles, with each thread block computing one tile. Threads within a block will cooperate to load input sub-tiles into shared memory, perform the partial matrix multiplication, and accumulate the results.\n4.  A C++ wrapper will handle argument parsing, output tensor allocation, and launching the CUDA kernel with appropriate grid and block dimensions based on the input matrix size and chosen tile size.","step":0,"id":"4c72ffacefaf4a688fc64d42ad4702e9","ctime":1762748950.5158205,"parent":null,"children":[],"_term_out":["================================================================================","KERNEL EVALUATION SUCCESS","================================================================================","\u2713 Compilation: PASSED","\u2713 Correctness: PASSED","  Correctness trials: (5 / 5)","\n\u2713 Performance: 37.600 ms","  Mean: 37.600 ms","  Std:  0.134 ms","  Min:  37.500 ms","  Max:  38.800 ms","  Trials: 100","\nHardware: NVIDIA TITAN V","Runtime: 0.038 seconds"],"exec_time":0.0376,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"Kernel implementation successful! Runtime: 37.600 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled.","metric":{"value":37.6,"maximize":false},"is_buggy":false}],"node2parent":{},"__version":"2"}