<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[0, 3], [3, 4]], "layout": [[0.0, 0.0], [0.5, 0.0], [1.0, 0.0], [0.0, 0.5], [0.0, 1.0]], "plan": ["My optimization strategy will focus on replacing PyTorch's `torch.matmul` with a\ncustom CUDA kernel. For this initial implementation, I will develop a basic\nmatrix multiplication kernel where each thread computes a single element of the\noutput matrix C. This involves iterating over the inner dimension for the dot\nproduct. The kernel will be launched with a 2D grid and block configuration to\nnaturally map to the output matrix dimensions, ensuring correctness and a\nfoundational custom operator.", "My optimization plan focuses on replacing the `torch.matmul` operation with a\ncustom CUDA kernel. For this initial implementation, I will develop a basic\nmatrix multiplication kernel where each CUDA thread computes a single element of\nthe output matrix C. This involves an inner loop within the kernel to perform\nthe dot product for that specific output element. The kernel will be launched\nwith a 2D grid and 2D block configuration to naturally map threads to the output\nmatrix dimensions, prioritizing correctness and a foundational custom operator.", "My optimization plan focuses on replacing the `torch.matmul` operation with a\ncustom CUDA kernel. For this initial implementation, I will develop a basic\nmatrix multiplication kernel where each CUDA thread computes a single element of\nthe output matrix C. This involves an inner loop within the kernel to perform\nthe dot product for that specific output element. The kernel will be launched\nwith a 2D grid and 2D block configuration to naturally map threads to the output\nmatrix dimensions, prioritizing correctness and a foundational custom operator.", "The previous kernel suffered from high global memory traffic due to repeated\nfetches of A's rows and B's columns for each output element. My optimization\nwill implement a tiled matrix multiplication using shared memory. Each thread\nblock will cooperatively load sub-tiles of matrices A and B into shared memory.\nThis allows for significant data reuse within the block, reducing global memory\naccesses and improving memory access patterns. The kernel will iterate over the\nK-dimension, loading new tiles into shared memory for partial computations until\nthe final result is accumulated.", "The previous tiled matrix multiplication kernel, while utilizing shared memory,\nsuffered from severe shared memory bank conflicts when threads accessed columns\nof the `Bs` tile during the accumulation phase. My optimization will eliminate\nthese conflicts by padding the `Bs` shared memory array. By declaring `Bs` with\nan extra element in its column dimension (e.g., `[BLOCK_SIZE][BLOCK_SIZE + 1]`),\nconsecutive column accesses by different threads will map to distinct shared\nmemory banks, significantly improving memory throughput and reducing\nserialization. This targeted change will enhance the efficiency of shared memory\nusage and improve overall kernel performance."], "code": ["import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel definition and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// CUDA kernel for matrix multiplication (C = A * B)\n// Each thread computes one element of the output matrix C.\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    // Calculate global row and column indices for the current thread\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check bounds to ensure the thread operates within the matrix dimensions\n    if (row < N && col < N) {\n        float sum = 0.0f;\n        // Compute the dot product for the element C[row][col]\n        for (int k = 0; k < N; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n// C++ wrapper function that prepares data and launches the CUDA kernel\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Ensure input tensors are on the CUDA device and are contiguous in memory.\n    // This is crucial for direct data_ptr access in the kernel.\n    A = A.to(torch::kCUDA).contiguous();\n    B = B.to(torch::kCUDA).contiguous();\n\n    // Get the matrix dimension N. We assume square matrices of size N x N.\n    int N = A.size(0);\n\n    // Create the output tensor C on the CUDA device, initialized to zeros.\n    // It will have the same dimensions, data type, and device as the input tensor A.\n    auto C = torch::zeros({N, N}, A.options());\n\n    // Define thread block dimensions. A 32x32 block is a common choice for 2D operations.\n    const int BLOCK_SIZE = 32;\n    dim3 block_dim(BLOCK_SIZE, BLOCK_SIZE);\n    \n    // Calculate grid dimensions. Each block processes a BLOCK_SIZE x BLOCK_SIZE tile.\n    // The ceiling division `(N + BLOCK_SIZE - 1) / BLOCK_SIZE` ensures enough blocks\n    // are launched to cover the entire matrix, even if N is not a multiple of BLOCK_SIZE.\n    dim3 grid_dim((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\n    // Launch the CUDA kernel with the specified grid and block dimensions.\n    // Pass raw pointers to the tensor data and the matrix dimension N.\n    matmul_kernel<<<grid_dim, block_dim>>>(\n        A.data_ptr<float>(), // Pointer to the start of matrix A's data\n        B.data_ptr<float>(), // Pointer to the start of matrix B's data\n        C.data_ptr<float>(), // Pointer to the start of matrix C's data\n        N                    // The dimension N of the square matrices\n    );\n\n    // Return the resulting output tensor.\n    return C;\n}\n\"\"\"\n\n# C++ function declaration. This is required by load_inline.\ncpp_source = \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile and load the custom CUDA kernel using torch.utils.cpp_extension.load_inline.\n# 'name' is the module name, 'cpp_sources' provides declarations, 'cuda_sources' provides implementations,\n# and 'functions' lists the C++ functions to expose in the Python module.\nmatmul_custom_op = load_inline(\n    name=\"matmul_custom\",\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=[\"matmul_cuda\"],\n    verbose=True,  # Set to True for detailed compilation output, useful for debugging.\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that replaces the standard torch.matmul with a custom CUDA kernel\n    for square matrix multiplication (C = A * B).\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the compiled custom operation module as a member of the nn.Module.\n        self.matmul_op = matmul_custom_op\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # Call the C++ wrapper function (matmul_cuda) from the loaded custom operation module.\n        return self.matmul_op.matmul_cuda(A, B)", "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// CUDA kernel for basic matrix multiplication C = A * B\n// Each thread computes one element of the output matrix C.\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    // Calculate global row and column indices for the current thread\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Perform bounds checking to ensure thread is within matrix dimensions\n    if (row < N && col < N) {\n        float sum = 0.0f;\n        // Compute the dot product for C[row][col]\n        // C[row][col] = sum(A[row][k] * B[k][col] for k in 0 to N-1)\n        for (int k = 0; k < N; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n// C++ wrapper function that prepares tensors and launches the CUDA kernel\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Basic input validation: ensure tensors are 2D, float, and compatible for multiplication\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Input tensors must be 2D.\");\n    TORCH_CHECK(A.is_floating_point() && B.is_floating_point(), \"Input tensors must be float (fp32).\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Inner dimensions must match for matrix multiplication.\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1), \"Input tensors must be square for this kernel.\");\n\n    int N = A.size(0); // Assuming A is N x N and B is N x N\n\n    // Allocate output tensor C on the same device and with the same dtype as input A\n    auto C = torch::zeros({N, N}, A.options());\n\n    // Define thread block dimensions (e.g., 16x16 is a common choice for 2D kernels)\n    const int BLOCK_SIZE_X = 16;\n    const int BLOCK_SIZE_Y = 16;\n    dim3 block_dim(BLOCK_SIZE_X, BLOCK_SIZE_Y);\n\n    // Calculate grid dimensions based on matrix size and block size\n    int num_blocks_x = (N + BLOCK_SIZE_X - 1) / BLOCK_SIZE_X;\n    int num_blocks_y = (N + BLOCK_SIZE_Y - 1) / BLOCK_SIZE_Y;\n    dim3 grid_dim(num_blocks_x, num_blocks_y);\n\n    // Launch the CUDA kernel\n    matmul_kernel<<<grid_dim, block_dim>>>(\n        A.data_ptr<float>(), // Pointer to A's data\n        B.data_ptr<float>(), // Pointer to B's data\n        C.data_ptr<float>(), // Pointer to C's data\n        N                    // Matrix dimension N\n    );\n\n    return C;\n}\n\"\"\"\n\n# C++ function declaration, required by load_inline for the C++ compiler to know the function signature\ncpp_source = \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile the inline CUDA code extension\n# 'name' is the module name, 'functions' lists the C++ wrapper functions to expose\ncustom_matmul_extension = load_inline(\n    name=\"custom_matmul_extension\",\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=[\"matmul_cuda\"],\n    verbose=False,  # Set to True for detailed compilation output\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that replaces torch.matmul with a custom CUDA kernel\n    for square matrix multiplication.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the loaded custom CUDA function as an attribute for easy access\n        self.custom_matmul = custom_matmul_extension.matmul_cuda\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the square matrix multiplication using the custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return self.custom_matmul(A, B)", "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Basic matrix multiplication kernel where each thread computes one element of C\n__global__ void basic_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    // Calculate global row and column indices for the current thread\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check bounds to ensure thread operates within matrix dimensions\n    if (row < N && col < N) {\n        float sum = 0.0f;\n        // Perform the dot product for C[row][col]\n        for (int k = 0; k < N; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n// C++ wrapper function to launch the CUDA kernel\ntorch::Tensor basic_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Ensure inputs are contiguous and on CUDA device\n    A = A.contiguous();\n    B = B.contiguous();\n\n    // Get matrix dimension N\n    int N = A.size(0);\n\n    // Create output tensor C with the same options (dtype, device) as A\n    auto C = torch::zeros({N, N}, A.options());\n\n    // Define block and grid dimensions\n    // Using 16x16 threads per block, which is a common starting point for 2D operations.\n    const int BLOCK_SIZE = 16; \n    dim3 block_dim(BLOCK_SIZE, BLOCK_SIZE);\n    \n    // Calculate grid dimensions to cover the entire N x N matrix\n    dim3 grid_dim((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\n    // Launch the kernel\n    basic_matmul_kernel<<<grid_dim, block_dim>>>(\n        A.data_ptr<float>(), \n        B.data_ptr<float>(), \n        C.data_ptr<float>(), \n        N\n    );\n\n    // No explicit cudaDeviceSynchronize() needed here, PyTorch's autograd engine\n    // handles stream synchronization implicitly when operations are chained or\n    // results are moved back to CPU.\n\n    return C;\n}\n\"\"\"\n\n# C++ function declaration\n# This string declares the C++ function signature that load_inline will expose.\ncpp_source = \"torch::Tensor basic_matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile the inline CUDA code\n# The 'name' argument should be unique for each compilation if multiple are used in the same session.\n# The 'functions' list should match the C++ wrapper function names to be exposed to Python.\nbasic_matmul_module = load_inline(\n    name=\"basic_matmul_kernel_extension\",\n    cpp_sources=cpp_source,  # REQUIRED: function declaration\n    cuda_sources=cuda_source,  # REQUIRED: kernel implementation\n    functions=[\"basic_matmul_cuda\"],\n    verbose=True,  # Set to False for less verbose output during compilation\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single square matrix multiplication (C = A * B)\n    using a custom CUDA kernel.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the compiled CUDA module as an attribute\n        self.basic_matmul_op = basic_matmul_module\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # Call the custom CUDA function via the compiled module\n        return self.basic_matmul_op.basic_matmul_cuda(A, B)", "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel definition and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Define BLOCK_SIZE for shared memory tiling. A common and effective size is 32.\n#define BLOCK_SIZE 32\n\n// CUDA kernel for tiled matrix multiplication (C = A * B)\n// Each thread block computes a BLOCK_SIZE x BLOCK_SIZE tile of the output matrix C.\n// Threads within a block cooperatively load sub-tiles of A and B into shared memory\n// to perform the partial matrix multiplication for their assigned C tile.\n__global__ void matmul_tiled_kernel(const float* A, const float* B, float* C, int N) {\n    // Shared memory for storing tiles of A and B.\n    // As stores a BLOCK_SIZE x BLOCK_SIZE tile of matrix A.\n    // Bs stores a BLOCK_SIZE x BLOCK_SIZE tile of matrix B.\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    // Calculate the global row and column indices for the current thread's element in C.\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n\n    float Cvalue = 0.0f; // Accumulator for the C[row][col] element.\n\n    // Loop over the tiles along the K dimension (the inner product dimension).\n    // This loop ensures all necessary intermediate products are calculated.\n    // (N + BLOCK_SIZE - 1) / BLOCK_SIZE gives the ceiling division for number of tiles.\n    for (int tile_idx = 0; tile_idx < (N + BLOCK_SIZE - 1) / BLOCK_SIZE; ++tile_idx) {\n        // Calculate global indices for loading data from global memory into shared memory.\n        // Each thread loads one element for As and one for Bs.\n        int A_global_row = row;\n        int A_global_col = tile_idx * BLOCK_SIZE + threadIdx.x; // K index for A\n        int B_global_row = tile_idx * BLOCK_SIZE + threadIdx.y; // K index for B\n        int B_global_col = col;\n\n        // Load data from global memory to shared memory.\n        // Perform boundary checks for inputs to avoid out-of-bounds access\n        // and pad with zeros if outside the matrix dimensions.\n        As[threadIdx.y][threadIdx.x] = (A_global_row < N && A_global_col < N) ? A[A_global_row * N + A_global_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (B_global_row < N && B_global_col < N) ? B[B_global_row * N + B_global_col] : 0.0f;\n\n        // Synchronize threads to ensure all shared memory data is loaded\n        // before any thread starts computation using that shared data.\n        __syncthreads();\n\n        // Perform the dot product for the current tile.\n        // Each thread calculates a partial sum for C[row][col] using the shared memory tiles.\n        for (int k = 0; k < BLOCK_SIZE; ++k) {\n            Cvalue += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        // Synchronize threads to ensure all shared memory reads are complete\n        // before the next iteration potentially loads new data into shared memory.\n        __syncthreads();\n    }\n\n    // Write the accumulated result to global memory, with a boundary check\n    // to ensure we only write within the valid matrix dimensions.\n    if (row < N && col < N) {\n        C[row * N + col] = Cvalue;\n    }\n}\n\n// C++ wrapper function that prepares data and launches the CUDA kernel\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Ensure input tensors are on the CUDA device and are contiguous in memory.\n    // This is crucial for direct data_ptr access in the kernel.\n    A = A.to(torch::kCUDA).contiguous();\n    B = B.to(torch::kCUDA).contiguous();\n\n    // Get the matrix dimension N. We assume square matrices of size N x N.\n    int N = A.size(0);\n\n    // Create the output tensor C on the CUDA device, initialized to zeros.\n    // It will have the same dimensions, data type, and device as the input tensor A.\n    auto C = torch::zeros({N, N}, A.options());\n\n    // Define thread block dimensions. Using BLOCK_SIZE from the kernel for consistency.\n    dim3 block_dim(BLOCK_SIZE, BLOCK_SIZE);\n    \n    // Calculate grid dimensions. Each block processes a BLOCK_SIZE x BLOCK_SIZE tile of C.\n    // The ceiling division `(N + BLOCK_SIZE - 1) / BLOCK_SIZE` ensures enough blocks\n    // are launched to cover the entire matrix, even if N is not a multiple of BLOCK_SIZE.\n    dim3 grid_dim((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\n    // Launch the CUDA kernel with the specified grid and block dimensions.\n    // Pass raw pointers to the tensor data and the matrix dimension N.\n    matmul_tiled_kernel<<<grid_dim, block_dim>>>(\n        A.data_ptr<float>(), // Pointer to the start of matrix A's data\n        B.data_ptr<float>(), // Pointer to the start of matrix B's data\n        C.data_ptr<float>(), // Pointer to the start of matrix C's data\n        N                    // The dimension N of the square matrices\n    );\n\n    // Return the resulting output tensor.\n    return C;\n}\n\"\"\"\n\n# C++ function declaration. This is required by load_inline.\ncpp_source = \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile and load the custom CUDA kernel using torch.utils.cpp_extension.load_inline.\nmatmul_custom_op = load_inline(\n    name=\"matmul_custom_tiled\",\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=[\"matmul_cuda\"],\n    verbose=False,  # Set to True for detailed compilation output, useful for debugging.\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that replaces the standard torch.matmul with a custom tiled CUDA kernel\n    for square matrix multiplication (C = A * B).\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the compiled custom operation module as a member of the nn.Module.\n        self.matmul_op = matmul_custom_op\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom tiled CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # Call the C++ wrapper function (matmul_cuda) from the loaded custom operation module.\n        return self.matmul_op.matmul_cuda(A, B)", "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel definition and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Define BLOCK_SIZE for shared memory tiling. A common and effective size is 32.\n#define BLOCK_SIZE 32\n// Define PADDED_BLOCK_SIZE to avoid shared memory bank conflicts for column access.\n// Shared memory on modern NVIDIA GPUs has 32 banks. Padding by one element\n// ensures that sequential accesses to elements in different rows but the same column\n// will hit different banks, avoiding 32-way conflicts when BLOCK_SIZE is 32.\n#define PADDED_BLOCK_SIZE (BLOCK_SIZE + 1)\n\n// CUDA kernel for tiled matrix multiplication (C = A * B)\n// Each thread block computes a BLOCK_SIZE x BLOCK_SIZE tile of the output matrix C.\n// Threads within a block cooperatively load sub-tiles of A and B into shared memory\n// to perform the partial matrix multiplication for their assigned C tile.\n__global__ void matmul_tiled_kernel(const float* A, const float* B, float* C, int N) {\n    // Shared memory for storing tiles of A and B.\n    // As stores a BLOCK_SIZE x BLOCK_SIZE tile of matrix A.\n    // Bs stores a BLOCK_SIZE x BLOCK_SIZE tile of matrix B, with padding to avoid bank conflicts.\n    __shared__ float As[BLOCK_SIZE][PADDED_BLOCK_SIZE]; // Pad As for consistency/future flexibility, though Bs is the primary target\n    __shared__ float Bs[BLOCK_SIZE][PADDED_BLOCK_SIZE];\n\n    // Calculate the global row and column indices for the current thread's element in C.\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n\n    float Cvalue = 0.0f; // Accumulator for the C[row][col] element.\n\n    // Loop over the tiles along the K dimension (the inner product dimension).\n    // This loop ensures all necessary intermediate products are calculated.\n    // (N + BLOCK_SIZE - 1) / BLOCK_SIZE gives the ceiling division for number of tiles.\n    for (int tile_idx = 0; tile_idx < (N + BLOCK_SIZE - 1) / BLOCK_SIZE; ++tile_idx) {\n        // Calculate global indices for loading data from global memory into shared memory.\n        // Each thread loads one element for As and one for Bs.\n        int A_global_row = row;\n        int A_global_col = tile_idx * BLOCK_SIZE + threadIdx.x; // K index for A\n        int B_global_row = tile_idx * BLOCK_SIZE + threadIdx.y; // K index for B\n        int B_global_col = col;\n\n        // Load data from global memory to shared memory.\n        // Perform boundary checks for inputs to avoid out-of-bounds access\n        // and pad with zeros if outside the matrix dimensions.\n        As[threadIdx.y][threadIdx.x] = (A_global_row < N && A_global_col < N) ? A[A_global_row * N + A_global_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (B_global_row < N && B_global_col < N) ? B[B_global_row * N + B_global_col] : 0.0f;\n\n        // Synchronize threads to ensure all shared memory data is loaded\n        // before any thread starts computation using that shared data.\n        __syncthreads();\n\n        // Perform the dot product for the current tile.\n        // Each thread calculates a partial sum for C[row][col] using the shared memory tiles.\n        // Accessing Bs[k][threadIdx.x] now benefits from padding, avoiding bank conflicts.\n        for (int k = 0; k < BLOCK_SIZE; ++k) {\n            Cvalue += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        // Synchronize threads to ensure all shared memory reads are complete\n        // before the next iteration potentially loads new data into shared memory.\n        __syncthreads();\n    }\n\n    // Write the accumulated result to global memory, with a boundary check\n    // to ensure we only write within the valid matrix dimensions.\n    if (row < N && col < N) {\n        C[row * N + col] = Cvalue;\n    }\n}\n\n// C++ wrapper function that prepares data and launches the CUDA kernel\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Ensure input tensors are on the CUDA device and are contiguous in memory.\n    // This is crucial for direct data_ptr access in the kernel.\n    A = A.to(torch::kCUDA).contiguous();\n    B = B.to(torch::kCUDA).contiguous();\n\n    // Get the matrix dimension N. We assume square matrices of size N x N.\n    int N = A.size(0);\n\n    // Create the output tensor C on the CUDA device, initialized to zeros.\n    // It will have the same dimensions, data type, and device as the input tensor A.\n    auto C = torch::zeros({N, N}, A.options());\n\n    // Define thread block dimensions. Using BLOCK_SIZE from the kernel for consistency.\n    dim3 block_dim(BLOCK_SIZE, BLOCK_SIZE);\n    \n    // Calculate grid dimensions. Each block processes a BLOCK_SIZE x BLOCK_SIZE tile of C.\n    // The ceiling division `(N + BLOCK_SIZE - 1) / BLOCK_SIZE` ensures enough blocks\n    // are launched to cover the entire matrix, even if N is not a multiple of BLOCK_SIZE.\n    dim3 grid_dim((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\n    // Launch the CUDA kernel with the specified grid and block dimensions.\n    // Pass raw pointers to the tensor data and the matrix dimension N.\n    matmul_tiled_kernel<<<grid_dim, block_dim>>>(\n        A.data_ptr<float>(), // Pointer to the start of matrix A's data\n        B.data_ptr<float>(), // Pointer to the start of matrix B's data\n        C.data_ptr<float>(), // Pointer to the start of matrix C's data\n        N                    // The dimension N of the square matrices\n    );\n\n    // Return the resulting output tensor.\n    return C;\n}\n\"\"\"\n\n# C++ function declaration. This is required by load_inline.\ncpp_source = \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile and load the custom CUDA kernel using torch.utils.cpp_extension.load_inline.\nmatmul_custom_op = load_inline(\n    name=\"matmul_custom_tiled_padded\",\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=[\"matmul_cuda\"],\n    verbose=False,  # Set to True for detailed compilation output, useful for debugging.\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that replaces the standard torch.matmul with a custom tiled CUDA kernel\n    for square matrix multiplication (C = A * B), incorporating shared memory padding\n    to mitigate bank conflicts.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the compiled custom operation module as a member of the nn.Module.\n        self.matmul_op = matmul_custom_op\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom tiled CUDA kernel\n        with shared memory padding.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # Call the C++ wrapper function (matmul_cuda) from the loaded custom operation module.\n        return self.matmul_op.matmul_cuda(A, B)"], "term_out": ["================================================================================KERNEL EVALUATION SUCCESS================================================================================\u2713 Compilation: PASSED\u2713 Correctness: PASSED  Correctness trials: (5 / 5)\n\u2713 Performance: 66.500 ms  Mean: 66.500 ms  Std:  0.038 ms  Min:  66.500 ms  Max:  66.700 ms  Trials: 100\nHardware: NVIDIA TITAN VRuntime: 0.067 seconds", "================================================================================KERNEL EVALUATION SUCCESS================================================================================\u2713 Compilation: PASSED\u2713 Correctness: PASSED  Correctness trials: (5 / 5)\n\u2713 Performance: 76.100 ms  Mean: 76.100 ms  Std:  0.059 ms  Min:  76.000 ms  Max:  76.400 ms  Trials: 100\nHardware: NVIDIA TITAN VRuntime: 0.076 seconds", "================================================================================KERNEL EVALUATION SUCCESS================================================================================\u2713 Compilation: PASSED\u2713 Correctness: PASSED  Correctness trials: (5 / 5)\n\u2713 Performance: 76.100 ms  Mean: 76.100 ms  Std:  0.056 ms  Min:  76.100 ms  Max:  76.400 ms  Trials: 100\nHardware: NVIDIA TITAN VRuntime: 0.076 seconds", "================================================================================KERNEL EVALUATION SUCCESS================================================================================\u2713 Compilation: PASSED\u2713 Correctness: PASSED  Correctness trials: (5 / 5)\n\u2713 Performance: 37.700 ms  Mean: 37.700 ms  Std:  0.046 ms  Min:  37.600 ms  Max:  37.900 ms  Trials: 100\nHardware: NVIDIA TITAN VRuntime: 0.038 seconds", "================================================================================KERNEL EVALUATION SUCCESS================================================================================\u2713 Compilation: PASSED\u2713 Correctness: PASSED  Correctness trials: (5 / 5)\n\u2713 Performance: 47.000 ms  Mean: 47.000 ms  Std:  0.042 ms  Min:  47.000 ms  Max:  47.200 ms  Trials: 100\nHardware: NVIDIA TITAN VRuntime: 0.047 seconds"], "analysis": ["Kernel implementation successful! Runtime: 66.500 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled.", "Kernel implementation successful! Runtime: 76.100 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled.", "Kernel implementation successful! Runtime: 76.100 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled.", "Kernel implementation successful! Runtime: 37.700 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled.", "Kernel implementation successful! Runtime: 47.000 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled."], "exp_name": "41-loutish-tourmaline-loon", "metrics": [0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
