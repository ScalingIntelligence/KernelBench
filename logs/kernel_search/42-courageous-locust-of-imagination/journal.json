{"nodes":[{"code":"import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Define the tile size for shared memory matrix multiplication\n#define TILE_SIZE 32\n\n// CUDA kernel for square matrix multiplication using shared memory tiling\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    // Shared memory for tiles of A and B\n    // TILE_SIZE x TILE_SIZE floats = 32 * 32 * 4 bytes = 4096 bytes per shared array.\n    // Total shared memory per block = 2 * 4096 = 8192 bytes. This is well within typical limits (e.g., 48KB).\n    __shared__ float s_A[TILE_SIZE][TILE_SIZE];\n    __shared__ float s_B[TILE_SIZE][TILE_SIZE];\n\n    // Global row and column index for the current thread\n    // These determine the element of C this thread is responsible for.\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float Cvalue = 0.0f; // Accumulator for C[row][col]\n\n    // Loop over the 'inner' dimension (k) in steps of TILE_SIZE\n    // Each iteration processes one pair of tiles from A and B\n    for (int k_idx = 0; k_idx < N; k_idx += TILE_SIZE) {\n        // Load A tile into shared memory\n        // Each thread loads one element of s_A.\n        // s_A[threadIdx.y][threadIdx.x] will hold A[row][k_idx + threadIdx.x]\n        // The condition (k_idx + threadIdx.x) < N handles potential padding if N is not a multiple of TILE_SIZE.\n        if (row < N && (k_idx + threadIdx.x) < N) {\n            s_A[threadIdx.y][threadIdx.x] = A[row * N + (k_idx + threadIdx.x)];\n        } else {\n            s_A[threadIdx.y][threadIdx.x] = 0.0f; // Pad with zeros for out-of-bounds reads\n        }\n\n        // Load B tile into shared memory\n        // Each thread loads one element of s_B.\n        // s_B[threadIdx.y][threadIdx.x] will hold B[k_idx + threadIdx.y][col]\n        // The condition (k_idx + threadIdx.y) < N handles potential padding if N is not a multiple of TILE_SIZE.\n        if (col < N && (k_idx + threadIdx.y) < N) {\n            s_B[threadIdx.y][threadIdx.x] = B[(k_idx + threadIdx.y) * N + col];\n        } else {\n            s_B[threadIdx.y][threadIdx.x] = 0.0f; // Pad with zeros for out-of-bounds reads\n        }\n\n        __syncthreads(); // Wait for all threads in the block to load their tile elements into shared memory\n\n        // Perform the dot product for the current tiles\n        // Each thread computes its part of Cvalue using the shared memory tiles.\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            Cvalue += s_A[threadIdx.y][i] * s_B[i][threadIdx.x];\n        }\n\n        __syncthreads(); // Wait for all threads to finish computing with shared memory before loading next tiles\n    }\n\n    // Store the computed value to global memory\n    // No boundary check needed for C[row][col] as grid_size ensures row < N and col < N\n    // for threads that actually write to C.\n    if (row < N && col < N) {\n        C[row * N + col] = Cvalue;\n    }\n}\n\n// C++ wrapper function to launch the CUDA kernel\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Input validation\n    TORCH_CHECK(A.is_cuda(), \"Input A must be a CUDA tensor.\");\n    TORCH_CHECK(B.is_cuda(), \"Input B must be a CUDA tensor.\");\n    TORCH_CHECK(A.dtype() == torch::kFloat32, \"Input A must be float32.\");\n    TORCH_CHECK(B.dtype() == torch::kFloat32, \"Input B must be float32.\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices.\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Matrix dimensions mismatch for multiplication.\");\n    TORCH_CHECK(A.size(0) == B.size(1), \"This kernel currently supports only square matrices of same size N x N.\");\n\n    int N = A.size(0); // Assuming square matrices N x N\n    TORCH_CHECK(A.size(1) == N && B.size(0) == N && B.size(1) == N, \"Input matrices must be square and of the same size N x N.\");\n\n    // Allocate output tensor C on the same device as A.\n    auto C = torch::empty({N, N}, A.options());\n\n    // Define grid and block dimensions for the kernel launch\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    // Calculate number of blocks needed to cover the N x N output matrix\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    // Launch the CUDA kernel\n    matmul_kernel<<<grid_size, block_size>>>(\n        A.data_ptr<float>(), // Pointer to A's data\n        B.data_ptr<float>(), // Pointer to B's data\n        C.data_ptr<float>(), // Pointer to C's data\n        N                    // Size of the matrix\n    );\n\n    // Check for any CUDA errors after kernel launch\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        fprintf(stderr, \"CUDA error in matmul_kernel: %s\\\\n\", cudaGetErrorString(err));\n        TORCH_CHECK(false, \"CUDA kernel launch failed.\");\n    }\n\n    return C;\n}\n\"\"\"\n\n# C++ function declaration for load_inline\ncpp_source = \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile the inline CUDA code extension\nmatmul_extension = load_inline(\n    name=\"matmul_extension\",  # Unique name for the compiled extension\n    cpp_sources=cpp_source,  # C++ function declarations\n    cuda_sources=cuda_source,  # CUDA kernel implementation and C++ wrapper\n    functions=[\"matmul_cuda\"],  # List of C++ functions to expose to Python\n    verbose=True,  # Enable verbose output during compilation\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that replaces torch.matmul with a custom tiled CUDA kernel.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the compiled CUDA extension's matmul_cuda function\n        self.matmul_op = matmul_extension.matmul_cuda\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # Call the custom CUDA kernel through the loaded extension\n        return self.matmul_op(A, B)\n","plan":"Optimization plan:\n1.  **Identify Bottleneck:** The primary bottleneck is the `torch.matmul` operation, especially for large square matrices.\n2.  **Custom Kernel Implementation:** Replace `torch.matmul` with a custom CUDA kernel using `torch.utils.cpp_extension.load_inline()`.\n3.  **Tiled Matrix Multiplication:** Implement a tiled matrix multiplication kernel that leverages shared memory. Each thread block will be responsible for computing a tile of the output matrix `C`.\n4.  **Shared Memory Usage:** Threads within a block will cooperatively load sub-matrices (tiles) of `A` and `B` into shared memory. This reduces global memory accesses and promotes data reuse, which is crucial for performance.\n5.  **Synchronization:** Use `__syncthreads()` to ensure all threads in a block have loaded their data into shared memory before computation and finished computation before loading the next tiles.","step":0,"id":"974b49a8a6d1429d9fbbcd213ec533f7","ctime":1762748344.3507602,"parent":null,"children":[],"_term_out":["================================================================================","KERNEL EVALUATION SUCCESS","================================================================================","\u2713 Compilation: PASSED","\u2713 Correctness: PASSED","  Correctness trials: (5 / 5)","\n\u2713 Performance: 37.700 ms","  Mean: 37.700 ms","  Std:  0.047 ms","  Min:  37.600 ms","  Max:  37.900 ms","  Trials: 100","\nHardware: NVIDIA TITAN V","Runtime: 0.038 seconds"],"exec_time":0.037700000000000004,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"Kernel implementation successful! Runtime: 37.700 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled.","metric":{"value":37.7,"maximize":false},"is_buggy":false},{"code":"import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# CUDA kernel source (includes both kernel and C++ wrapper)\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Define the tile size for shared memory matrix multiplication\n#define TILE_SIZE 32\n\n// CUDA kernel for square matrix multiplication using shared memory tiling\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    // Shared memory for tiles of A and B\n    __shared__ float s_A[TILE_SIZE][TILE_SIZE];\n    __shared__ float s_B[TILE_SIZE][TILE_SIZE];\n\n    // Global row and column index for the current thread\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float Cvalue = 0.0f; // Accumulator for C[row][col]\n\n    // Loop over the 'inner' dimension (k) in steps of TILE_SIZE\n    for (int k_idx = 0; k_idx < N; k_idx += TILE_SIZE) {\n        // Load A tile into shared memory\n        if (row < N && (k_idx + threadIdx.x) < N) {\n            s_A[threadIdx.y][threadIdx.x] = A[row * N + (k_idx + threadIdx.x)];\n        } else {\n            s_A[threadIdx.y][threadIdx.x] = 0.0f; // Pad with zeros for out-of-bounds reads\n        }\n\n        // Load B tile into shared memory\n        if (col < N && (k_idx + threadIdx.y) < N) {\n            s_B[threadIdx.y][threadIdx.x] = B[(k_idx + threadIdx.y) * N + col];\n        } else {\n            s_B[threadIdx.y][threadIdx.x] = 0.0f; // Pad with zeros for out-of-bounds reads\n        }\n\n        __syncthreads(); // Wait for all threads in the block to load their tile elements into shared memory\n\n        // Perform the dot product for the current tiles\n        // #pragma unroll directive to explicitly unroll this loop\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            Cvalue += s_A[threadIdx.y][i] * s_B[i][threadIdx.x];\n        }\n\n        __syncthreads(); // Wait for all threads to finish computing with shared memory before loading next tiles\n    }\n\n    // Store the computed value to global memory\n    if (row < N && col < N) {\n        C[row * N + col] = Cvalue;\n    }\n}\n\n// C++ wrapper function to launch the CUDA kernel\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Input validation\n    TORCH_CHECK(A.is_cuda(), \"Input A must be a CUDA tensor.\");\n    TORCH_CHECK(B.is_cuda(), \"Input B must be a CUDA tensor.\");\n    TORCH_CHECK(A.dtype() == torch::kFloat32, \"Input A must be float32.\");\n    TORCH_CHECK(B.dtype() == torch::kFloat32, \"Input B must be float32.\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices.\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Matrix dimensions mismatch for multiplication.\");\n    TORCH_CHECK(A.size(0) == B.size(1), \"This kernel currently supports only square matrices of same size N x N.\");\n\n    int N = A.size(0); // Assuming square matrices N x N\n    TORCH_CHECK(A.size(1) == N && B.size(0) == N && B.size(1) == N, \"Input matrices must be square and of the same size N x N.\");\n\n    // Allocate output tensor C on the same device as A.\n    auto C = torch::empty({N, N}, A.options());\n\n    // Define grid and block dimensions for the kernel launch\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    // Calculate number of blocks needed to cover the N x N output matrix\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    // Launch the CUDA kernel\n    matmul_kernel<<<grid_size, block_size>>>(\n        A.data_ptr<float>(), // Pointer to A's data\n        B.data_ptr<float>(), // Pointer to B's data\n        C.data_ptr<float>(), // Pointer to C's data\n        N                    // Size of the matrix\n    );\n\n    // Check for any CUDA errors after kernel launch\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        fprintf(stderr, \"CUDA error in matmul_kernel: %s\\\\n\", cudaGetErrorString(err));\n        TORCH_CHECK(false, \"CUDA kernel launch failed.\");\n    }\n\n    return C;\n}\n\"\"\"\n\n# C++ function declaration for load_inline\ncpp_source = \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile the inline CUDA code extension\nmatmul_extension = load_inline(\n    name=\"matmul_extension\",  # Unique name for the compiled extension\n    cpp_sources=cpp_source,  # C++ function declarations\n    cuda_sources=cuda_source,  # CUDA kernel implementation and C++ wrapper\n    functions=[\"matmul_cuda\"],  # List of C++ functions to expose to Python\n    verbose=True,  # Enable verbose output during compilation\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that replaces torch.matmul with a custom tiled CUDA kernel.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Store the compiled CUDA extension's matmul_cuda function\n        self.matmul_op = matmul_extension.matmul_cuda\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # Call the custom CUDA kernel through the loaded extension\n        return self.matmul_op(A, B)\n","plan":"The optimization plan focuses on enhancing the instruction-level parallelism and reducing loop overhead within the core computation phase of the tiled matrix multiplication. By applying `#pragma unroll` to the inner loop that accumulates `Cvalue`, we instruct the CUDA compiler to unroll this loop. This exposes more independent instructions to the GPU's scheduler, potentially leading to better utilization of execution units and improved throughput for the dot product calculation within each shared memory tile.","step":1,"id":"5dae7310d2dc42e0b965ddc558b335c7","ctime":1762748488.55277,"parent":null,"children":[],"_term_out":["================================================================================","KERNEL EVALUATION SUCCESS","================================================================================","\u2713 Compilation: PASSED","\u2713 Correctness: PASSED","  Correctness trials: (5 / 5)","\n\u2713 Performance: 37.700 ms","  Mean: 37.700 ms","  Std:  0.484 ms","  Min:  37.600 ms","  Max:  41.000 ms","  Trials: 100","\nHardware: NVIDIA TITAN V","Runtime: 0.038 seconds"],"exec_time":0.037700000000000004,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"Kernel implementation successful! Runtime: 37.700 ms. The kernel compiled correctly, passed all correctness checks, and was successfully profiled.","metric":{"value":37.7,"maximize":false},"is_buggy":false}],"node2parent":{"5dae7310d2dc42e0b965ddc558b335c7":"974b49a8a6d1429d9fbbcd213ec533f7"},"__version":"2"}