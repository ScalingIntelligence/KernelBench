{"nodes":[{"code":"import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the CUDA kernel for tiled matrix multiplication\ncuda_source = \"\"\"\n#define TILE_SIZE 32\n\n__global__ void matmul_tiled_kernel(float* C, const float* A, const float* B, int N) {\n    // Block index\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n\n    // Thread index within the block\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n\n    // Row and column of C that this thread is responsible for\n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n\n    // Accumulator for the C element\n    float Csub = 0;\n\n    // Declare shared memory for tiles of A and B\n    // As stores a TILE_SIZE x TILE_SIZE tile of matrix A\n    // Bs stores a TILE_SIZE x TILE_SIZE tile of matrix B\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    // Loop over the K dimension (inner product dimension)\n    // The loop iterates over \"blocks\" of K, each block being TILE_SIZE wide\n    for (int k_block = 0; k_block < (N + TILE_SIZE - 1) / TILE_SIZE; ++k_block) {\n        // Calculate the column index in A and row index in B for global memory access\n        int k_idx_A = k_block * TILE_SIZE + tx; // A's column index\n        int k_idx_B = k_block * TILE_SIZE + ty; // B's row index\n\n        // Load a tile of A into shared memory\n        // Each thread loads one element of A_tile\n        if (row < N && k_idx_A < N) {\n            As[ty][tx] = A[row * N + k_idx_A];\n        } else {\n            As[ty][tx] = 0.0f; // Pad with zeros if out of bounds\n        }\n\n        // Load a tile of B into shared memory\n        // Each thread loads one element of B_tile\n        if (k_idx_B < N && col < N) {\n            Bs[ty][tx] = B[k_idx_B * N + col];\n        } else {\n            Bs[ty][tx] = 0.0f; // Pad with zeros if out of bounds\n        }\n\n        // Synchronize threads to ensure all shared memory loads are complete\n        __syncthreads();\n\n        // Perform the dot product for the current tile\n        // Each thread computes one element of the Csub accumulator\n        for (int m = 0; m < TILE_SIZE; ++m) {\n            Csub += As[ty][m] * Bs[m][tx];\n        }\n\n        // Synchronize threads to ensure all computations using the current shared memory tiles are done\n        // before the next iteration loads new data into shared memory\n        __syncthreads();\n    }\n\n    // Write the accumulated Csub to global memory\n    if (row < N && col < N) {\n        C[row * N + col] = Csub;\n    }\n}\n\"\"\"\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single square matrix multiplication (C = A * B)\n    using a custom CUDA kernel.\n    \"\"\"\n\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # Compile the CUDA kernel\n        self.matmul_kernel = load_inline(\n            name=\"matmul_tiled_kernel_module\",\n            cpp_sources=\"\"\"\n            #include <torch/extension.h>\n            #include <vector>\n\n            // Forward declaration of the CUDA kernel\n            void matmul_tiled_kernel_launcher(\n                float* C, const float* A, const float* B, int N,\n                dim3 grid_dim, dim3 block_dim\n            );\n\n            torch::Tensor matmul_tiled_forward(\n                torch::Tensor A, torch::Tensor B\n            ) {\n                // Ensure inputs are contiguous and on CUDA device\n                A = A.contiguous();\n                B = B.contiguous();\n\n                int N = A.size(0);\n                torch::Tensor C = torch::zeros({N, N}, A.options());\n\n                const int TILE_SIZE = 32;\n                dim3 block_dim(TILE_SIZE, TILE_SIZE);\n                dim3 grid_dim((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n                matmul_tiled_kernel_launcher(\n                    C.data_ptr<float>(),\n                    A.data_ptr<float>(),\n                    B.data_ptr<float>(),\n                    N,\n                    grid_dim,\n                    block_dim\n                );\n\n                return C;\n            }\n            \"\"\",\n            cuda_sources=cuda_source,\n            functions=[\"matmul_tiled_forward\"],\n            with_cuda=True,\n            extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n            build_directory=\"/tmp/matmul_tiled_kernel_build\",  # Specify a build directory\n        )\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using the custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        if not (A.is_cuda and B.is_cuda):\n            raise ValueError(\"Inputs must be CUDA tensors.\")\n        if A.dtype != torch.float32 or B.dtype != torch.float32:\n            raise ValueError(\"Inputs must be float32 tensors.\")\n        if A.shape != B.shape or len(A.shape) != 2 or A.shape[0] != A.shape[1]:\n            raise ValueError(\"Inputs must be square matrices of the same size.\")\n\n        return self.matmul_kernel.matmul_tiled_forward(A, B)\n","plan":"My optimization plan focuses on replacing the `torch.matmul` operation with a custom CUDA kernel. I will implement a tiled matrix multiplication algorithm, which is a standard and highly effective optimization for GPUs. This approach leverages shared memory to improve data reuse, reduce global memory traffic, and achieve better memory coalescing, leading to significant performance gains over a naive element-wise kernel. I will ensure correct handling of grid and block dimensions and utilize `torch.utils.cpp_extension.load_inline` for seamless integration.","step":0,"id":"78ea859bef1b4b51b0e9ead626927a02","ctime":1762743672.7677054,"parent":null,"children":[],"_term_out":["Unexpected evaluation error: [Errno 2] No such file or directory: '/tmp/matmul_tiled_kernel_build/main.cpp'"],"exec_time":0.0,"exc_type":"EvaluationError","exc_info":{"error":"[Errno 2] No such file or directory: '/tmp/matmul_tiled_kernel_build/main.cpp'"},"exc_stack":null,"analysis":"Evaluation failed with error: EvaluationError","metric":{"value":null,"maximize":null},"is_buggy":true}],"node2parent":{},"__version":"2"}